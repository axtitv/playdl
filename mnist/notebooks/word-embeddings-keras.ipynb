{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "The key idea for improvement is \"*You shall know a word by the company it keeps.*\" ([John Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth), 1957).  Here is a [nice paper on GloVe](https://nlp.stanford.edu/pubs/glove.pdf).  I think this was the [word2vec paper that got word vectors started](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "\n",
    "BTW, this also works for embedding of other things like movies/users (from the netflix challenge) and [airlines](https://djcordhose.github.io/ml-workshop/2019-embeddings.html). If we want to get dense vectors for airline names, we need some mechanism to indicate similarity between airlines so that, rather than random dense factors, we can get dense vectors where airlines are somehow close to each other in some appropriate dense space.\n",
    "\n",
    "### skip-gram\n",
    "\n",
    "If we see the word `New`, very likely `York` will follow. If we see the word `bear`, it's likely that `brown`, `black`, or `grizzly` would proceed `bear`. The words `network` is unlikely to precede or follow `bear` in close proximity. We can train a neural network to respond to a word with word probabilities that could occur in a neighborhood, before or after.\n",
    "\n",
    "The training for the network is x = one-hot word vector for a single word and y = vector of probabilities of appearing in the neighborhood for all words. If there are 1000 words in our vocabulary then each x and y will be vectors of length 1000.\n",
    "\n",
    "### CBOW\n",
    "\n",
    "Or, we can train a CBOW representation of the neighborhood (a few words on either side) to recognize the center word.\n",
    "\n",
    "In either case, the weights of the single hidden layer represent the word embeddings.\n",
    "\n",
    "In either case, we are using a sliding window that moves over the words in a document. Global information about co-occurrence of words is not used.  Apparently a huge amount of data is required to avoid overfitting and training is expensive, given the explosion of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "The [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) approach uses a word co-occurrence matrix and I think is a little easier to understand.  If there are 1000 words in the vocabulary, then the co-occurrence matrix is 1000 x 1000. The entry at i,j is how often words i and j co-occur in the corpus. They have a more complicated model to solve I think than word2vec, but creating the cooccurrence matrix seems easier and smaller than the sliding window training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and uncompress [movie review polarity data set v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) and put into `data` subdir.  The files contain positive and negative reviews:\n",
    "\n",
    "```\n",
    "data/review_polarity/txt_sentoken/\n",
    "├── neg\n",
    "│   ├── cv000_29416.txt\n",
    "│   ├── cv001_19502.txt\n",
    "│   ├── cv002_17424.txt\n",
    "...\n",
    "└── pos\n",
    "    ├── cv000_29590.txt\n",
    "    ├── cv001_18431.txt\n",
    "    ├── cv002_15918.txt\n",
    "    ├── cv003_11664.txt\n",
    "...\n",
    "```\n",
    "\n",
    "The idea will be to map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
