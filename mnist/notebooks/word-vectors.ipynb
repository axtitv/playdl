{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenization, word indexing, word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using keras to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tweets from my twitter inbox with added text for experimentation\n",
    "samples = [\n",
    "    \"\"\"Tesla Motors has nothing to do with this tweet.\n",
    "    On those rare occasions when I really, really need to reduce the\n",
    "    size of a file I use \"xz -9\". Today I found out about the \"extreme\" setting\n",
    "    and \"xz -e9\" squeezed files down another 15% or so. It is not exactly quick,\n",
    "    but that doesn't really matter in such cases!\"\"\",\n",
    "    \n",
    "    \"\"\"Securities and exchange commission has nothing to do with this tweet.\n",
    "    Do grad students get paid a lot? No. But do we at least have solid\n",
    "    job security? Also, no. But are we at least ensured a stress-free work\n",
    "    environment with a healthy work-life balance? Still, also no.\"\"\",\n",
    "\n",
    "    \"\"\"A design process hyperfocused on A/B testing can result in dark patterns even\n",
    "    if that’s not the intent. That’s because most A/B tests are based on metrics\n",
    "    that are relevant to the company’s bottom line, even if they result in harm to users.\"\"\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 14,  2,  3,  5, 15, 16,  6,  7,  8,  8,  2,  4,  1,  7, 17,  7,\n",
       "        4, 18, 17, 19,  9, 20,  8, 10])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "np.array(sequences[0]) # token sequence for first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 1., 2., 1., 1., 3., 3., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 2., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 3., 1., 3., 0., 2., 0., 0., 0., 2., 0., 3., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 2., 2., 2., 2., 2., 0., 0., 0., 0.],\n",
       "       [0., 3., 2., 0., 2., 0., 2., 0., 0., 0., 2., 0., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count in one-hot positions for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='count')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One-hot hash trick\"\n",
    "\n",
    "Instead of using unique word index into a dictionary of unique words from all samples, compute hash function on word as its code. Lets us limit size of word vector to fixed length rather than length of vocab or whatever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: {'a': 7, 'to': 9, 'do': 1, 'the': 7, 'with': 0, 'on': 8, 'i': 5, 'really': 9, 'but': 4, 'in': 0, 'no': 1, 'are': 1, 'has': 7, 'nothing': 5, 'this': 3, 'tweet': 2, 'xz': 2, 'and': 8, 'not': 4, 'that': 0, 'we': 3, 'at': 2, 'least': 4, 'also': 7, 'work': 1, 'b': 8, 'result': 6, 'even': 2, 'if': 2, 'that’s': 1, 'tesla': 9, 'motors': 1, 'those': 9, 'rare': 9, 'occasions': 5, 'when': 2, 'need': 2, 'reduce': 9, 'size': 7, 'of': 0, 'file': 9, 'use': 9, '9': 7, 'today': 5, 'found': 2, 'out': 6, 'about': 4, 'extreme': 5, 'setting': 9, 'e9': 5, 'squeezed': 2, 'files': 7, 'down': 6, 'another': 4, '15': 5, 'or': 2, 'so': 1, 'it': 6, 'is': 5, 'exactly': 3, 'quick': 1, \"doesn't\": 2, 'matter': 2, 'such': 4, 'cases': 1, 'securities': 9, 'exchange': 7, 'commission': 0, 'grad': 8, 'students': 5, 'get': 6, 'paid': 2, 'lot': 6, 'have': 1, 'solid': 4, 'job': 0, 'security': 5, 'ensured': 2, 'stress': 3, 'free': 9, 'environment': 4, 'healthy': 9, 'life': 3, 'balance': 3, 'still': 4, 'design': 0, 'process': 3, 'hyperfocused': 4, 'testing': 7, 'can': 2, 'dark': 7, 'patterns': 9, 'intent': 2, 'because': 7, 'most': 8, 'tests': 1, 'based': 0, 'metrics': 5, 'relevant': 2, 'company’s': 5, 'bottom': 3, 'line': 7, 'they': 7, 'harm': 7, 'users': 3}\n",
      "10/105=9.5% collisions using 10 unique codes\n"
     ]
    }
   ],
   "source": [
    "def hash(w):\n",
    "    h = 0\n",
    "    for c in w:\n",
    "        h = (h<<3) + ord(c)\n",
    "    return h\n",
    "\n",
    "def hashwords(dimensionality = 1000):\n",
    "    collisions = set()\n",
    "    codes = set()\n",
    "    index = {}\n",
    "    for w in words:\n",
    "        wcode = hash(w)%dimensionality\n",
    "        if wcode in codes:\n",
    "            collisions.add(wcode)\n",
    "        codes.add(wcode)\n",
    "        index[w] = wcode\n",
    "    return index, collisions\n",
    "\n",
    "words = tokenizer.word_index.keys() # get tokenized words\n",
    "dimensionality=10\n",
    "index, collisions = hashwords(dimensionality)\n",
    "print(\"INDEX:\", index)\n",
    "print(f\"{len(collisions)}/{len(words)}={(len(collisions)*100)/len(words):.1f}% collisions using {dimensionality} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/105= 9.5% collisions using     10 unique codes\n",
      "30/105=28.6% collisions using     50 unique codes\n",
      "29/105=27.6% collisions using    100 unique codes\n",
      " 7/105= 6.7% collisions using    500 unique codes\n",
      " 4/105= 3.8% collisions using   1000 unique codes\n",
      " 3/105= 2.9% collisions using   2000 unique codes\n",
      " 1/105= 1.0% collisions using   5000 unique codes\n",
      " 0/105= 0.0% collisions using  10000 unique codes\n"
     ]
    }
   ],
   "source": [
    "for dimensionality in [10,50,100,500,1000,2000,5000,10000]:\n",
    "    index, collisions = hashwords(dimensionality)\n",
    "    print(f\"{len(collisions):2d}/{len(words):2d}={(len(collisions)*100)/len(words):4.1f}% collisions using {dimensionality:6d} unique codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Using my typical hash function it takes a big dimensionality before we reduce collisions. In this case, there are only 96 unique words which means unique (perfect) hashing requires a dimensionality of only 96. The hashing trick requires thousands of hash buckets before we get 0 collisions. I suppose that if a simple hash function worked well, we would need wording embeddings. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy to tokenize, add meta data\n",
    "\n",
    "Before, assigning (hopefully unique) integers to each word,It's a good idea to processed the text more heavily and to insert special tags to represent information about the text.  Humans innately see a stream of words and construct a parse tree in their head, such as this is the subject and this is the verb. In order to help a model understand the text, giving hints about proper nouns, verbs, and sentence structure could be very useful.\n",
    "\n",
    "Spacy is great library for getting information about text. Check out [Spacy 101](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "#!python -m spacy download en_core_web_sm  # requires restart of jupyter kernal afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tesla, Motors, has, nothing, to, do, with, this, tweet, ., On, those, rare, occasions, when, I, really, ,, really, need]\n",
      "[Securities, and, exchange, commission, has, nothing, to, do, with, this, tweet, ., Do, grad, students, get, paid, a, lot, ?]\n",
      "[A, design, process, hyperfocused, on, A, /, B, testing, can, result, in, dark, patterns, even, if, that, ’s, not, the]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # When I use plain English() it doesn't seem to give POS info\n",
    "for sample in samples:    \n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = list(doc)\n",
    "    print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parts-of-speech meta data\n",
    "\n",
    "Adding some tricks from [Jeremy / Sylvain's awesome book](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) that indicate \"start of stream\" (`xxbos`) etc...  Use Spacy to add proper noun and verb tags.  Lowercase after Spacy has sniffed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxpropn tesla xxpropn xxsubj motors has nothing to do with this tweet . on those rare occasions when xxsubj i really , really xxverb need to xxverb reduce the size of a file xxsubj i xxverb use \" xxpropn xz xxpropn -9 \" . today xxsubj i xxverb found\n",
      "xxbos xxpropn securities and xxpropn exchange xxpropn xxsubj commission has nothing to do with this tweet . do grad students get xxverb paid a lot ? no . but do xxsubj we at least have solid job security ? also , no . but are xxsubj we at least xxverb\n",
      "xxbos a design xxsubj process xxverb hyperfocused on xxpropn a / b testing xxverb can xxverb result in dark patterns even if xxsubj that xxverb ’s not the intent . that xxverb ’s because most a / b tests are xxverb based on metrics xxsubj that are relevant to the\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for sample in samples:\n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = ['xxbos']\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            tokens.append('xxverb')\n",
    "        if token.pos_=='PROPN':\n",
    "            tokens.append('xxpropn')\n",
    "        if token.dep_=='nsubj':           # is token subject of a sentence or phrase?\n",
    "            tokens.append('xxsubj')\n",
    "        tokens.append(str(token).lower())\n",
    "    tokens.append('xxeos')\n",
    "    print(' '.join(tokens[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify sentences and phrase subject\n",
    "\n",
    "Add `xxbegin` tokens before start of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxpropn xxbegin tesla xxpropn xxsubj motors has nothing to do with this tweet . xxbegin on those rare occasions when xxsubj i really , really xxverb need to xxverb reduce the size of a file xxsubj i xxverb use \" xxpropn xz xxpropn -9 \" . xxbegin today xxsubj\n",
      "xxbos xxpropn xxbegin securities and xxpropn exchange xxpropn xxsubj commission has nothing to do with this tweet . xxbegin do grad students get xxverb paid a lot ? xxbegin no . xxbegin but do xxsubj we at least have solid job security ? xxbegin also , no . xxbegin but\n",
      "xxbos xxbegin a design xxsubj process xxverb hyperfocused on xxpropn a / b testing xxverb can xxverb result in dark patterns even if xxsubj that xxverb ’s not the intent . xxbegin that xxverb ’s because most a / b tests are xxverb based on metrics xxsubj that are relevant\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(Sentencizer())\n",
    "for sample in samples:\n",
    "    doc = nlp(compress_whitespace(sample))\n",
    "    tokens = ['xxbos']\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            tokens.append('xxverb')\n",
    "        if token.pos_=='PROPN':\n",
    "            tokens.append('xxpropn')\n",
    "        if token.dep_=='nsubj':           # is token subject of a sentence or phrase?\n",
    "            tokens.append('xxsubj')\n",
    "        if token.is_sent_start:           # beginning of sentence?\n",
    "            tokens.append('xxbegin')\n",
    "        tokens.append(str(token).lower())\n",
    "    tokens.append('xxeos')\n",
    "    print(' '.join(tokens[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have everything tokenized properly, we can then assign a unique number to the words and meta-tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
