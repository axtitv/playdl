{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenization, word indexing, word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tweets from my twitter inbox\n",
    "samples = [\n",
    "    \"\"\"On those rare occasions when I really, really need to reduce the\n",
    "    size of a file I use \"xz -9\". Today I found out about the \"extreme\" setting\n",
    "    and \"xz -e9\" squeezed files down another 15% or so. It is not exactly quick,\n",
    "    but that doesn't really matter in such cases!\"\"\",\n",
    "    \n",
    "    \"\"\"Do grad students get paid a lot? No. But do we at least have solid\n",
    "    job security? Also, no. But are we at least ensured a stress-free work\n",
    "    environment with a healthy work-life balance? Still, also no.\"\"\",\n",
    "\n",
    "    \"\"\"A design process hyperfocused on A/B testing can result in dark patterns even\n",
    "    if that’s not the intent. That’s because most A/B tests are based on metrics\n",
    "    that are relevant to the company’s bottom line, even if they result in harm to users.\"\"\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 25, 26, 27, 28,  4,  5,  5, 29,  6,  2,  1,  4, 11,  4,  2, 11,\n",
       "       12,  7, 13,  5,  8])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "np.array(sequences[0]) # token sequence for first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 1., 3., 3., 1., 1., 1., 0., 0., 2., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 3., 0., 0., 0., 0., 0., 2., 0., 3., 1., 0., 0., 0., 2., 2.,\n",
       "        2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 3., 2., 2., 0., 0., 2., 0., 2., 0., 2., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count in one-hot positions for samples\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='count')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One-hot hash trick\"\n",
    "\n",
    "Instead of using unique word index into a dictionary of unique words from all samples, compute hash function on word as its code. Lets us limit size of word vector to fixed length rather than length of vocab or whatever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: {'a': 7, 'the': 7, 'on': 8, 'i': 5, 'really': 9, 'to': 9, 'but': 4, 'in': 0, 'no': 1, 'are': 1, 'xz': 2, 'not': 4, 'that': 0, 'do': 1, 'we': 3, 'at': 2, 'least': 4, 'also': 7, 'work': 1, 'b': 8, 'result': 6, 'even': 2, 'if': 2, 'that’s': 1, 'those': 9, 'rare': 9, 'occasions': 5, 'when': 2, 'need': 2, 'reduce': 9, 'size': 7, 'of': 0, 'file': 9, 'use': 9, '9': 7, 'today': 5, 'found': 2, 'out': 6, 'about': 4, 'extreme': 5, 'setting': 9, 'and': 8, 'e9': 5, 'squeezed': 2, 'files': 7, 'down': 6, 'another': 4, '15': 5, 'or': 2, 'so': 1, 'it': 6, 'is': 5, 'exactly': 3, 'quick': 1, \"doesn't\": 2, 'matter': 2, 'such': 4, 'cases': 1, 'grad': 8, 'students': 5, 'get': 6, 'paid': 2, 'lot': 6, 'have': 1, 'solid': 4, 'job': 0, 'security': 5, 'ensured': 2, 'stress': 3, 'free': 9, 'environment': 4, 'with': 0, 'healthy': 9, 'life': 3, 'balance': 3, 'still': 4, 'design': 0, 'process': 3, 'hyperfocused': 4, 'testing': 7, 'can': 2, 'dark': 7, 'patterns': 9, 'intent': 2, 'because': 7, 'most': 8, 'tests': 1, 'based': 0, 'metrics': 5, 'relevant': 2, 'company’s': 5, 'bottom': 3, 'line': 7, 'they': 7, 'harm': 7, 'users': 3}\n",
      "10/96=10.4% collisions using 10 unique codes\n"
     ]
    }
   ],
   "source": [
    "def hash(w):\n",
    "    h = 0\n",
    "    for c in w:\n",
    "        h = (h<<3) + ord(c)\n",
    "    return h\n",
    "\n",
    "def hashwords(dimensionality = 1000):\n",
    "    collisions = set()\n",
    "    codes = set()\n",
    "    index = {}\n",
    "    for w in words:\n",
    "        wcode = hash(w)%dimensionality\n",
    "        if wcode in codes:\n",
    "            collisions.add(wcode)\n",
    "        codes.add(wcode)\n",
    "        index[w] = wcode\n",
    "    return index, collisions\n",
    "\n",
    "words = tokenizer.word_index.keys() # get tokenized words\n",
    "dimensionality=10\n",
    "index, collisions = hashwords(dimensionality)\n",
    "print(\"INDEX:\", index)\n",
    "print(f\"{len(collisions)}/{len(words)}={(len(collisions)*100)/len(words):.1f}% collisions using {dimensionality} unique codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/96=10.4% collisions using     10 unique codes\n",
      "28/96=29.2% collisions using     50 unique codes\n",
      "24/96=25.0% collisions using    100 unique codes\n",
      " 6/96= 6.2% collisions using    500 unique codes\n",
      " 4/96= 4.2% collisions using   1000 unique codes\n",
      " 3/96= 3.1% collisions using   2000 unique codes\n",
      " 1/96= 1.0% collisions using   5000 unique codes\n",
      " 0/96= 0.0% collisions using  10000 unique codes\n"
     ]
    }
   ],
   "source": [
    "for dimensionality in [10,50,100,500,1000,2000,5000,10000]:\n",
    "    index, collisions = hashwords(dimensionality)\n",
    "    print(f\"{len(collisions):2d}/{len(words):2d}={(len(collisions)*100)/len(words):4.1f}% collisions using {dimensionality:6d} unique codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Using my typical hash function it takes a big dimensionality before we reduce collisions. In this case, there are only 96 unique words which means unique (perfect) hashing requires a dimensionality of only 96. The hashing trick requires thousands of hash buckets before we get 0 collisions. I suppose that if a simple hash function worked well, we would need wording embeddings. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
