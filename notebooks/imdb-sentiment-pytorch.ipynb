{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment prediction with pytorch\n",
    "\n",
    "I'm reading the material in the keras book but implementing in pytorch.\n",
    "\n",
    "I'm using [polarity data set v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) to predict binary: positive or negative review. The data uncompressed it looks like:\n",
    "\n",
    "```\n",
    "data/review_polarity/txt_sentoken/\n",
    "├── neg\n",
    "│   ├── cv000_29416.txt\n",
    "│   ├── cv001_19502.txt\n",
    "│   ├── cv002_17424.txt\n",
    "│   ├── cv003_12683.txt\n",
    "│   ├── cv004_12641.txt\n",
    "...\n",
    "```\n",
    "\n",
    "Instead of trying to learn the ancillary package torchtext, I'm going to use SpaCy to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def filelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory; sort names alphabetically.\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return sorted(allfiles)\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "def load_docs(docs_dirname:str):\n",
    "    \"\"\"\n",
    "    Load all .txt files under docs_dirname and return a list of doc strings, one per doc.\n",
    "    Ignore empty and non \".txt\" files.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for filename in filelist(docs_dirname):#[0:600]:\n",
    "        if not filename.endswith(\".txt\"): continue\n",
    "        # print(f\"Process {filename}\")\n",
    "        filetext = get_text(filename)\n",
    "        if len(filetext)==0: continue\n",
    "        docs.append( compress_whitespace(filetext) )\n",
    "    return docs\n",
    "\n",
    "def words(text:str):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    ctrl_chars = '\\x00-\\x1f'\n",
    "    regex = re.compile(r'[' + ctrl_chars + string.punctuation + '0-9\\r\\t\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size:float):\n",
    "    n = len(X)\n",
    "    shuffle_idx = np.random.permutation(range(n))\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    n_valid = int(n*test_size)\n",
    "    n_train = n - n_valid\n",
    "    X_train, X_valid = X[0:n_train], X[n_train:]\n",
    "    y_train, y_valid = y[0:n_train], y[n_train:]\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def pad(a, w):\n",
    "    return torch.cat([a, a.new_zeros(w - a.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/parrt/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/parrt/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/parrt/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (7.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# paperspace needs this\n",
    "!pip -q install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_dirname = \"data/review_polarity/txt_sentoken/neg\"\n",
    "pos_dirname = \"data/review_polarity/txt_sentoken/pos\"\n",
    "negdocs = load_docs(neg_dirname)\n",
    "posdocs = load_docs(pos_dirname)\n",
    "n_negdocs = len(negdocs)\n",
    "n_posdocs = len(posdocs)\n",
    "n_negdocs, n_posdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# The following fails on paperspace gradient platform\n",
    "#nlp = spacy.load(\"en_core_web_sm\") # When I use plain English() it doesn't seem to give POS info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_tokenized = [list(nlp(d)) for d in negdocs] # super slow!\n",
    "#pos_tokenized = [list(nlp(d)) for d in posdocs]\n",
    "\n",
    "neg_tokenized = [words(d) for d in negdocs] # much faster!\n",
    "pos_tokenized = [words(d) for d in posdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "ndocprefix = 20 # use first 20 words\n",
    "vocab_size = 10_000\n",
    "all = []\n",
    "for tokens in neg_tokenized + pos_tokenized:\n",
    "    all.extend([w for w in tokens])\n",
    "c = Counter(all)\n",
    "V = c.most_common(vocab_size)\n",
    "word_to_idx = {w:i for i,w in enumerate(V)}\n",
    "V = sorted(word_to_idx.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only words in V, just first 20, convert to ints in word_to_idx\n",
    "neg = []\n",
    "pos = []\n",
    "for i in range(len(neg_tokenized)):\n",
    "    d = [word_to_idx[w] for w in neg_tokenized[i] if w in V][:ndocprefix]\n",
    "    neg.append( pad(torch.tensor(d), ndocprefix) )\n",
    "for i in range(len(pos_tokenized)):\n",
    "    d = [word_to_idx[w] for w in pos_tokenized[i] if w in V][:ndocprefix]\n",
    "    pos.append( pad(torch.tensor(d), ndocprefix) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 20]), torch.Size([2000, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.stack(neg+pos)\n",
    "X = X.long()\n",
    "y = torch.tensor([0]*n_negdocs + [1]*n_posdocs).reshape(-1,1)\n",
    "y = y.float()\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build simple sentiment learner / predictor\n",
    "\n",
    "The keras book uses 8 dimensional embedding vectors, 20 word prefix for each document,\n",
    "10,000 most common words. They concatenate the 20 8-vectors and run that into a\n",
    "single-neuron final layer. That surprises me. I tried adding a hidden layer but \n",
    "it didn't help very much. Actually, now that I play with it, epochs=150,                      learning_rate=0.03, weight_decay=0.00001, it looks like it starts to consistently get better to about 56% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Sentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, nfactors):\n",
    "        super(Sentiment, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, nfactors)\n",
    "        self.final = nn.Linear(ndocprefix*nfactors,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)      # [800, 20]\n",
    "        output = self.embedding(x)\n",
    "#         print(output.shape) # [800, 20, 64] = (samples, num word features, embedding dim)\n",
    "        # must cat the 20 64-vectors together\n",
    "        output = output.view((x.shape[0], -1))\n",
    "#         print(output.shape) # [800, 1280]\n",
    "        output = self.final(output)\n",
    "#         print(output.shape) # [800, 1]\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, X, y, epochs=350, test_size=0.20, learning_rate = 0.002, weight_decay=1.e-4, print_every=30, loss_fn=nn.BCELoss()):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, 0.20)\n",
    "    print(f\"{len(X_train)} training and {len(X_valid)} test records\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    history = []\n",
    "    for t in range(epochs):\n",
    "        y_prob = model(X_train)\n",
    "        loss = loss_fn(y_prob, y_train)\n",
    "        y_pred = torch.round(y_prob)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad and M.grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_valid = loss_fn(model(X_valid), y_valid)\n",
    "            accur_train = accuracy_score(torch.round(model(X_train)), y_train)\n",
    "            accur_valid = accuracy_score(torch.round(model(X_valid)), y_valid)\n",
    "\n",
    "        history.append( (loss, loss_valid) )\n",
    "        if t % print_every == 0:\n",
    "            print(f\"Epoch {t:3d} log loss {loss:7.3f}, {loss_valid:7.3f}   accuracy {accur_train:4.3f}, {accur_valid:4.3f}\")        \n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 training and 400 test records\n",
      "Epoch   0 log loss   0.715,   6.628   accuracy 0.502, 0.490\n",
      "Epoch  10 log loss   0.813,   0.779   accuracy 0.497, 0.510\n",
      "Epoch  20 log loss   0.755,   0.764   accuracy 0.502, 0.490\n",
      "Epoch  30 log loss   0.740,   0.732   accuracy 0.497, 0.510\n",
      "Epoch  40 log loss   0.701,   0.707   accuracy 0.502, 0.490\n",
      "Epoch  50 log loss   0.694,   0.693   accuracy 0.497, 0.510\n",
      "Epoch  60 log loss   0.693,   0.694   accuracy 0.502, 0.490\n",
      "Epoch  70 log loss   0.693,   0.694   accuracy 0.502, 0.490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-276b01285f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        print_every=10)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-1d8ac5aad8b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, y, epochs, test_size, learning_rate, weight_decay, print_every, loss_fn)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# autograd computes U.grad and M.grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nfactors = 8\n",
    "model = Sentiment(vocab_size, nfactors)\n",
    "model, history = train(model, torch.tensor(X), torch.tensor(y),\n",
    "                       epochs=100,\n",
    "                       learning_rate=0.1,\n",
    "                       weight_decay=0.00001,\n",
    "                       print_every=10)\n",
    "\n",
    "plot_history(history, yrange=(0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not very good. Depending on where it initializes, I've seen it get to 60% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add mini-batches\n",
    "\n",
    "It sounds like many batches of around 32 are sort of the general recommendation to get stochastic gradient descent. See [stackoverflow](https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data) and [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612).  It seems like training on the whole batch is not done for two reasons:\n",
    "\n",
    "1.  it has to all fit in GPU memory\n",
    "2.  the full batch does not add any bouncing around to the gradient descent process which gets us stuck in local minima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y,\n",
    "          epochs=350,\n",
    "          test_size=0.20,\n",
    "          learning_rate = 0.002,\n",
    "          batch_size=32,\n",
    "          weight_decay=1.e-4,\n",
    "          loss_fn=nn.BCELoss(),\n",
    "          print_every=30):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, 0.20)\n",
    "    print(f\"{len(X_train)} training and {len(X_valid)} test records\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for t in range(epochs): # epochs\n",
    "        permutation = torch.randperm(X_train.shape[0])\n",
    "\n",
    "        for k in range(0, X_train.shape[0], batch_size): # mini-batch\n",
    "            batch_indices = permutation[k:k+batch_size]\n",
    "            batch_x, batch_y = X_train[batch_indices], y_train[batch_indices]\n",
    "    \n",
    "            y_prob = model(X_train)\n",
    "            loss = loss_fn(y_prob, y_train)\n",
    "            y_pred = torch.round(y_prob)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad and M.grad\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_valid = loss_fn(model(X_valid), y_valid)\n",
    "            accur_train = accuracy_score(torch.round(model(X_train)), y_train)\n",
    "            accur_valid = accuracy_score(torch.round(model(X_valid)), y_valid)\n",
    "\n",
    "        history.append( (loss, loss_valid) )\n",
    "        if t % print_every == 0:\n",
    "            print(f\"Epoch {t:3d} log loss {loss:7.3f}, {loss_valid:7.3f}   accuracy {accur_train:4.3f}, {accur_valid:4.3f}\")        \n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 training and 400 test records\n",
      "Epoch   0 log loss   0.693,   0.698   accuracy 0.507, 0.470\n",
      "Epoch  10 log loss   0.693,   0.694   accuracy 0.507, 0.470\n",
      "Epoch  20 log loss   0.693,   0.694   accuracy 0.507, 0.470\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9c09ec3cc2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                        print_every=10)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bc31cc5da331>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, y, epochs, test_size, learning_rate, batch_size, weight_decay, loss_fn, print_every)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# autograd computes U.grad and M.grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nfactors = 8\n",
    "model = Sentiment(vocab_size, nfactors)\n",
    "model, history = train(model, torch.tensor(X), torch.tensor(y),\n",
    "                       epochs=100,\n",
    "                       learning_rate=0.1,\n",
    "                       batch_size=32,\n",
    "                       weight_decay=0.001,\n",
    "                       print_every=10)\n",
    "\n",
    "plot_history(history, yrange=(0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now I'm getting much closer to 61% just through mini batch training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more layers and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "layer1 = 300\n",
    "layer2 = 32\n",
    "class SentimentRegularized(nn.Module):\n",
    "    def __init__(self, vocab_size, nfactors):\n",
    "        super(SentimentRegularized, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, nfactors)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(ndocprefix*nfactors,layer1), # 300 neurons\n",
    "            nn.BatchNorm1d(layer1),\n",
    "#             nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=layer1, out_features=layer2),\n",
    "            nn.BatchNorm1d(layer2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.embedding(x)\n",
    "        output = output.view((x.shape[0], -1))\n",
    "        output = self.layers(output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfactors = 64\n",
    "model = SentimentRegularized(vocab_size, nfactors)\n",
    "model, history = train(model, torch.tensor(X), torch.tensor(y),\n",
    "                       epochs=100,\n",
    "                       learning_rate=0.01,\n",
    "                       weight_decay=0.0001,\n",
    "                       print_every=10)\n",
    "\n",
    "plot_history(history, yrange=(0,2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. unable to get above about 56% validation accuracy. Try playing with nfactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfactors = 128\n",
    "model = SentimentRegularized(vocab_size, nfactors)\n",
    "model, history = train(model, torch.tensor(X), torch.tensor(y),\n",
    "                       epochs=100,\n",
    "                       learning_rate=0.03,\n",
    "                       weight_decay=0.000001,\n",
    "                       print_every=10)\n",
    "\n",
    "plot_history(history, yrange=(0,2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.round(model(X))\n",
    "accur = (y_pred==y).sum().item() / len(y)\n",
    "accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try RF\n",
    "\n",
    "Can't get much above 50% accurate as expected from raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, 0.20)\n",
    "rf = RandomForestClassifier(n_estimators=200, oob_score=True, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.oob_score_)\n",
    "accuracy_score(rf.predict(X_valid), y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
