{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment prediction with pytorch\n",
    "\n",
    "I'm reading the material in the keras book but implementing in pytorch.\n",
    "\n",
    "I'm using [polarity data set v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) to predict binary: positive or negative review. The data uncompressed it looks like:\n",
    "\n",
    "```\n",
    "data/review_polarity/txt_sentoken/\n",
    "├── neg\n",
    "│   ├── cv000_29416.txt\n",
    "│   ├── cv001_19502.txt\n",
    "│   ├── cv002_17424.txt\n",
    "│   ├── cv003_12683.txt\n",
    "│   ├── cv004_12641.txt\n",
    "...\n",
    "```\n",
    "\n",
    "Instead of trying to learn the ancillary package torchtext, I'm going to use SpaCy to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def filelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory; sort names alphabetically.\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return sorted(allfiles)\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "def load_docs(docs_dirname:str):\n",
    "    \"\"\"\n",
    "    Load all .txt files under docs_dirname and return a list of doc strings, one per doc.\n",
    "    Ignore empty and non \".txt\" files.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for filename in filelist(docs_dirname):#[0:600]:\n",
    "        if not filename.endswith(\".txt\"): continue\n",
    "        # print(f\"Process {filename}\")\n",
    "        filetext = get_text(filename)\n",
    "        if len(filetext)==0: continue\n",
    "        docs.append( compress_whitespace(filetext) )\n",
    "    return docs\n",
    "\n",
    "def words(text:str):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    ctrl_chars = '\\x00-\\x1f'\n",
    "    regex = re.compile(r'[' + ctrl_chars + string.punctuation + '0-9\\r\\t\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_dirname = \"data/review_polarity/txt_sentoken/neg\"\n",
    "pos_dirname = \"data/review_polarity/txt_sentoken/pos\"\n",
    "negdocs = load_docs(neg_dirname)\n",
    "posdocs = load_docs(pos_dirname)\n",
    "n_negdocs = len(negdocs)\n",
    "n_posdocs = len(posdocs)\n",
    "n_negdocs, n_posdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # When I use plain English() it doesn't seem to give POS info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_tokenized = [list(nlp(d)) for d in negdocs] # super slow!\n",
    "#pos_tokenized = [list(nlp(d)) for d in posdocs]\n",
    "\n",
    "neg_tokenized = [words(d) for d in negdocs] # much faster!\n",
    "pos_tokenized = [words(d) for d in posdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 500\n",
    "all = []\n",
    "for tokens in neg_tokenized + pos_tokenized:\n",
    "    all.extend([w for w in tokens])\n",
    "c = Counter(all)\n",
    "V = c.most_common(vocab_size)\n",
    "word_to_idx = dict(V)\n",
    "V = sorted(word_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neg_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-923dca45f6fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mword_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneg_tokenized\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpos_tokenized\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_negdocs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_posdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "X = torch.tensor( [[word_to_idx[w] for w in doc_tokens if w in V] for doc_tokens in neg_tokenized+pos_tokenized] )\n",
    "y = np.array([0]*n_negdocs + [1]*n_posdocs).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build simple sentiment learner / predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Sentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, nfactors):\n",
    "        super(Sentiment, self).__init__()\n",
    "#         self.embedding = \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, nfactors),\n",
    "            nn.Linear(vocab_size*nfactors,128), # 128 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nfactors,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        output = self.layers(x)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, epochs=350, test_size=0.20, learning_rate = 0.002, print_every=30, loss_fn=nn.BCELoss()):\n",
    "    n = len(X)\n",
    "    shuffle_idx = np.random.permutation(range(n))\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    n_valid = int(n*test_size)\n",
    "    n_train = n - n_valid\n",
    "    X_train, X_valid = X[0:n_train], X[n_train:]\n",
    "    y_train, y_valid = y[0:n_train], y[n_train:]\n",
    "    print(f\"{len(X_train)} training and {len(X_valid)} test records\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00000002) # model.parameters is just [U,M]\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad and M.grad\n",
    "        optimizer.step()\n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(500, 64)\n",
      "    (1): Linear(in_features=32000, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 155 at dim 1 (got 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-416aa18fe0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 155 at dim 1 (got 62)"
     ]
    }
   ],
   "source": [
    "nfactors = 64\n",
    "model = Sentiment(vocab_size, nfactors)\n",
    "print(model)\n",
    "train(model, torch.tensor(X), torch.tensor(y), epochs=5, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
