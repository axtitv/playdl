{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate movie and user embedding into two layers\n",
    "\n",
    "Previously I manually concatenated one hot vectors for movies and users so that I could keep a simple DL pipeline architecture. This made it complicated to extract the embeddings for just the movies. I suspect I will have better luck with separate layers that I join after the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2641 movies that are Comedy or Drama\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>167</td>\n",
       "      <td>718</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Borat: Cultural Learnings of America for Make ...</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>430</td>\n",
       "      <td>718</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Borat: Cultural Learnings of America for Make ...</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>181</td>\n",
       "      <td>718</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Borat: Cultural Learnings of America for Make ...</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating  \\\n",
       "66     167      718     3.5   \n",
       "67     430      718     4.5   \n",
       "68     181      718     4.0   \n",
       "\n",
       "                                                title  genres  \n",
       "66  Borat: Cultural Learnings of America for Make ...  Comedy  \n",
       "67  Borat: Cultural Learnings of America for Make ...  Comedy  \n",
       "68  Borat: Cultural Learnings of America for Make ...  Comedy  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "\n",
    "def load(n = 10):\n",
    "    df_ratings = pd.read_csv('data/ml-latest-small/ratings.csv')\n",
    "    df_ratings = df_ratings.drop('timestamp', axis=1)\n",
    "    df_ratings = df_ratings.sample(n=n).reset_index(drop=True)\n",
    "    # Merge in the title and genres\n",
    "    df_movies = pd.read_csv('data/ml-latest-small/movies.csv')\n",
    "    df = df_ratings.merge(df_movies, on='movieId')\n",
    "    # Strip the \"(1999)\" dates from the titles\n",
    "    p = re.compile(r'[()0-9]+$')\n",
    "    df['title'] = df['title'].map(lambda x: p.sub('', x).strip())\n",
    "    return df\n",
    "\n",
    "def compress_cats(df, colname):\n",
    "    df[colname] = df[colname].astype('category').cat.as_ordered()\n",
    "    df[colname] = df[colname].cat.codes + 1 # encode 1..n\n",
    "\n",
    "df = load(n=20_000)\n",
    "df = df[(df['genres']=='Comedy')|(df['genres']=='Drama')]\n",
    "nmovies = len(df.groupby('movieId').count())\n",
    "compress_cats(df, 'movieId')\n",
    "compress_cats(df, 'userId')\n",
    "print(len(df), 'movies that are Comedy or Drama')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique movie ID and title and genres\n",
    "df_movies = df.sort_values('movieId')[['movieId','title','genres']]\n",
    "df_movies = df_movies.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a split then joined network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to map a sparse movie and user ID vector to a dense vector of say `dimensionality`=8, 10, or 20 dimensions. To do that, we use the first parallel layers of a network that have `dimensionality` neurons.  There is a parallel layer for movie and for user ID embedding, which are then joined and run through a pipeline for predicting ratings. Each neuron will contribute a single dimension to each dense vector. The input X has, say, 10,000 rows, one for each one hot movie ID. It has nmovies columns. If there are 10 movies, there are 10 possible positions in the one hot encoding. The first layer is a transformation from nmovies or nusers space to `dimensionality` space. The key is that we want to choke that first layers into just a few neurons and then have a big layer afterwards that tries to make sense of those new compressed features. We don't care about the prediction at the end, we are just going to take the weights out of the first parallel layers to get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, callbacks, optimizers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(df,\n",
    "          dimensionality = 8,\n",
    "          otherlayers = (100,),\n",
    "          batch_size = 10,\n",
    "          epochs = 20,\n",
    "          batchnorm = False,\n",
    "          dropout = 0):\n",
    "    # Everybody get one hot!\n",
    "    nusers = len(df.groupby('userId').count())\n",
    "    nmovies = len(df.groupby('movieId').count())\n",
    "    X_onehot = pd.concat([pd.get_dummies(df['movieId']),\n",
    "                          pd.get_dummies(df['userId'])], axis=1)\n",
    "    y = df['rating']\n",
    "\n",
    "    layer1 = dimensionality\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer1, input_dim=nmovies+nusers, activation='relu',\n",
    "                           name='embedding'))\n",
    "    for n_hidden in otherlayers:\n",
    "        model.add(layers.Dense(n_hidden, activation='relu'))\n",
    "        if batchnorm:\n",
    "            model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.RMSprop(),\n",
    "                  metrics=['mae'])\n",
    "    #print(model.summary())\n",
    "\n",
    "    history = model.fit(X_onehot, y,\n",
    "                        shuffle=True,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.15,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=[tfa.callbacks.TQDMProgressBar(show_epoch_progress=False)]\n",
    "                        )\n",
    "    return model, history\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(3.5,3))\n",
    "    plt.ylabel(\"Rating (0..5.0) MAE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    accur = history.history['mae']\n",
    "    plt.plot(accur, label='train_mae')\n",
    "    val_accur = history.history['val_mae']\n",
    "    plt.plot(val_accur, label='val_mae')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(0.0, 1.00)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a48a868f894b2f9e0dfa937c68643c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', layout=Layout(flex='2'), max=20.0, style=Progrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, history = train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
