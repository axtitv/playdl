{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the language of the last name via RNN\n",
    "\n",
    "The idea is to one hot encode characters and then create dense embeddings for them based upon some classification problem, such as predicting the next letter or predicting nationality of last name (a common example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float,\n",
    "          mean=0.0, std=0.01, requires_grad=False):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Let's download [training](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz) and [testing](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz) data for last names.   This data set is a bunch of last names and the nationality or language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/names_train.csv\", header=None)\n",
    "df_train.columns = ['name','language']\n",
    "df_test = pd.read_csv(\"data/names_test.csv\", header=None)\n",
    "df_test.columns = ['name','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train[badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma = df_train['name'].str.contains(',') # might as well keep\n",
    "df_train[comma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['name'].str.contains(\"'\")][:3] # there are ok so keep quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train = df_train[~badname]\n",
    "\n",
    "badname = df_test['name']=='To The First Page'\n",
    "df_test = df_test[~badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['name'] = df_train['name'].str.lower()\n",
    "df_test['name'] = df_test['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlen(strings:Sequence[str]) -> int:\n",
    "    return max([len(l) for l in strings])\n",
    "\n",
    "max_len = max(maxlen(df_train['name']), maxlen(df_test['name']))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train[['name']], df_train['language']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)\n",
    "X_test, y_test = df_test[['name']], df_test['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    V = set([c for cl in letters for c in cl])\n",
    "    V = sorted(list(V))\n",
    "    ctoi = {c:i for i, c in enumerate(V)}\n",
    "    return V, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, ctoi = vocab(X['name'])\n",
    "ctoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target language (class)\n",
    "\n",
    "Get categories from training only, not valid/test sets. Then apply cats to those set y's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('category').cat.as_ordered()\n",
    "y_cats = y_train.cat.categories\n",
    "y_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.cat.codes\n",
    "y_train.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.Categorical(y_valid, categories=y_cats, ordered=True).codes\n",
    "y_test = pd.Categorical(y_test, categories=y_cats, ordered=True).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[:5], y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode each letter of each name\n",
    "\n",
    "Each name becomes a matrix of size vocab_size x max_len. Each column represents a char and we pad with zeros out to max_len number of columns since tensors have to be same length in same dimension. \n",
    "\n",
    "This approach is wasteful in that it expands each word to len of longest but avoids having to pad explicitly, simplifying the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(strings:Sequence[str], V, ctoi, max_len=None) -> torch.tensor:\n",
    "    if max_len is None:\n",
    "        max_len = maxlen(strings)\n",
    "    X_onehot = torch.zeros(len(strings),len(V),max_len)\n",
    "    for i,name in enumerate(strings):\n",
    "        onehot = torch.zeros((len(V),max_len))\n",
    "        for j,c in enumerate(name):\n",
    "            onehot[ctoi[c],j] = 1\n",
    "        X_onehot[i] = onehot\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['cat','a','at'] # always debug with a small representative example\n",
    "o = onehot(sample, *vocab(sample))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o[0,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = onehot(X_train['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_train_onehot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_onehot = onehot(X_valid['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_valid_onehot[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Switching to W, U, V notation from $W_hh$ etc... from [Goodfellow and Yoshua Bengio and Aaron Courville book](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "\n",
    "We have a sequence of one-hot vectors for each word and need to predict a language for each sequence.  We need to know: vocab size (len of one hots), hidden len, and the number of target classes (langs).\n",
    "\n",
    "We must combine a name's onehots into a single vector representing word then use a simple dense linear layer to make a prediction\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W h^{(t-1)} + U x^{(t)} )\n",
    "$$\n",
    "\n",
    "where $t$ iterates through name length (or max pad length).\n",
    "\n",
    "Note this is same as concatenating old state and current input vector and applying a single $W$ matrix of size nhidden x (nhidden+|V|):\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W [h^{(t-1)};x^{(t)}] )\n",
    "$$\n",
    "\n",
    "The output is avail at every char but we only need the last one:\n",
    "\n",
    "$$\n",
    "y^{(t)} = V h^{(t)}\n",
    "$$\n",
    "\n",
    "This $V$ acts like the last dense linear layer which converts the hidden state to likelihood of each target class.\n",
    "\n",
    "*What are the embeddings?* I think those are the final $h^{(t)}$ vectors, one of which is computed per name.  What are char-vec embeddings? Maybe $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record-by-record (slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN_slow(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN_slow, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Help avoid vanishing gradient. Start with identity, which has\n",
    "        # effect of summing char vector embeddings\n",
    "        self.W  = torch.eye(hidden_size, hidden_size).double() #randn(hidden_size, hidden_size, std=0.01).double()\n",
    "        self.U  = randn(hidden_size, input_size).double()\n",
    "        self.V  = randn(output_size, hidden_size).double()\n",
    "        self.W  = nn.Parameter(self.W)\n",
    "        self.U  = nn.Parameter(self.U)\n",
    "        self.V  = nn.Parameter(self.V)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).double().to(device)\n",
    "        for i in range(batch_size):\n",
    "            # Reset hidden state (history) at start of every record\n",
    "            # Use same W and U matrices for all records until SGD update step\n",
    "            h = torch.zeros((self.hidden_size, 1)).double().to(device)\n",
    "            for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "                h = self.W.mm(h) + self.U.mm(X[i,:,j].reshape(-1,1))\n",
    "                h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "            # we now have an h vector that is the embedding for the ith record\n",
    "            # we have encoded/embedded the X[i] record into h\n",
    "            # compute an output value, one per record\n",
    "            ot = self.V.mm(h)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "            o[i] = ot.reshape(-1)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "rnn = LastNameRNN_slow(input_size=len(V), hidden_size=10, output_size=len(y_cats)).to(device)\n",
    "y_pred = rnn(torch.tensor(X_train_onehot[:100],device=device).double())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrain(model:nn.Module, train_data:TensorDataset, valid_data:TensorDataset,\n",
    "           epochs=350,\n",
    "           test_size=0.20,\n",
    "           learning_rate = 0.002,\n",
    "           batch_size=32,\n",
    "           weight_decay=1.e-4,\n",
    "           loss_fn=F.cross_entropy,\n",
    "           metric=accuracy_score,\n",
    "           print_every=30):\n",
    "    \"Train a regressor\"\n",
    "    history = []\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for ei in range(epochs): # epochs\n",
    "        for bi, (batch_x, batch_y) in enumerate(train_loader): # mini-batch\n",
    "#             if len(batch_x)!=batch_size:\n",
    "#                 print(f\"\\tBatch {bi:3d} len {len(batch_x)}\")\n",
    "            y_prob = model(batch_x)\n",
    "#             print(\"y pred\", y_prob, \"batch_y\", batch_y)\n",
    "            loss = loss_fn(y_prob, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad and M.grad\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o           = model(train_data.tensors[0])\n",
    "            loss        = loss_fn(o, train_data.tensors[1])\n",
    "            o           = model(valid_data.tensors[0])\n",
    "            loss_valid  = loss_fn(o, valid_data.tensors[1])\n",
    "            y_prob = model(train_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_train = metric(y_pred.cpu(), train_data.tensors[1].cpu())\n",
    "            y_prob = model(valid_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_valid = metric(y_pred.cpu(), valid_data.tensors[1].cpu())\n",
    "\n",
    "        history.append( (loss, loss_valid) )\n",
    "        if ei % print_every == 0:\n",
    "            print(f\"Epoch {ei:3d} loss {loss:7.4f}, {loss_valid:7.4f}   accur {metric_train:4.3f}, {metric_valid:4.3f}\")\n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LastNameRNN_slow(input_size=len(V),\n",
    "                      hidden_size=100,\n",
    "                      output_size=len(y_cats)).to(device)\n",
    "subset=2000\n",
    "train = TensorDataset(X_train_onehot[:subset].double().to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].double().to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=10,\n",
    "                        learning_rate=.01,\n",
    "                        weight_decay=0.00001,\n",
    "                        batch_size=32,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep-by-step (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # combine W and U into W then cat h and input\n",
    "        self.W  = nn.Linear(hidden_size+input_size, hidden_size)\n",
    "        self.V  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).to(device)\n",
    "        # now that we do all char j in a batch, h is a matrix\n",
    "        h = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "            xj = X[:,:,j] # jth char for all records in batch\n",
    "#             print(\"W\", self.W.weight.shape, \"h\", h.shape, \"xj\", xj.shape)\n",
    "            combined = torch.cat((h, xj),dim=1)\n",
    "#             print(\"combined\", combined.shape)\n",
    "            h = self.W(combined)\n",
    "            h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "        # we now have an h vector that is the embedding for the ith record\n",
    "        # we have encoded/embedded the X[i] record into h\n",
    "        # compute an output value, one per record\n",
    "        ot = self.V(h)\n",
    "#         print(\"ot shape\", ot.shape)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "#         o[i] = ot.reshape(-1)\n",
    "        return ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LastNameRNN(input_size=len(V),\n",
    "                  hidden_size=50,\n",
    "                  output_size=len(y_cats)).to(device)\n",
    "subset=5_000\n",
    "train = TensorDataset(X_train_onehot[:subset].to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=30,\n",
    "                        learning_rate=.001,\n",
    "                        weight_decay=0.000001,#002,\n",
    "                        batch_size=64,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN_split(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN_split, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.W  = torch.eye(hidden_size, hidden_size)\n",
    "        self.U  = randn(hidden_size, input_size)\n",
    "        self.V  = randn(output_size, hidden_size)\n",
    "        self.W  = nn.Parameter(self.W)\n",
    "        self.U  = nn.Parameter(self.U)\n",
    "        self.V  = nn.Parameter(self.V)\n",
    "\n",
    "#         self.W  = nn.Linear(hidden_size+input_size, hidden_size)\n",
    "#         self.V  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).to(device)\n",
    "        # now that we do all char j in a batch, h is a matrix\n",
    "        h = torch.zeros((self.hidden_size, batch_size)).to(device)\n",
    "        for j in range(namelen):  # for all chars in max name length\n",
    "            # xj is batchsize x |V| but U is hidden x |V| so need transpose\n",
    "            xj = X[:,:,j].T # jth char dim for all records in batch\n",
    "#             print(self.W.shape, h.shape, self.U.shape, xj.shape)\n",
    "            h = self.W.mm(h) + self.U.mm(xj)\n",
    "            h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "        # we now have an h vector that is the embedding for the ith record\n",
    "        # we have encoded/embedded the X[i] record into h\n",
    "        # compute an output value, one per record\n",
    "        ot = self.V.mm(h).T\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "#         print(\"ot shape\", ot.shape, h.shape)\n",
    "        return ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LastNameRNN_split(input_size=len(V),\n",
    "                      hidden_size=50,\n",
    "                      output_size=len(y_cats)).to(device)\n",
    "subset=5_000\n",
    "train = TensorDataset(X_train_onehot[:subset].to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=30,\n",
    "                        learning_rate=.001,\n",
    "                        weight_decay=0.000001,#002,\n",
    "                        batch_size=64,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting.  Combined W and U seems to get better results, even though it should be equivalent. I must have some other different like precision or other numerical diffs. Oh duh. I'm using nn.Linear, which has a bias. wait it should be worse w/o bias. hmm.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TensorDataset, if you see `TypeError: 'int' object is not callable`, it means you've passed a numpy array.\n",
    "\n",
    "If it says \"expected Long got Char\", it might mean int8 not char."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
