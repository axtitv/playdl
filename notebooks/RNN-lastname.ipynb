{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the language of the last name via RNN\n",
    "\n",
    "The idea is to one hot encode characters and then create dense embeddings for them based upon some classification problem, such as predicting the next letter or predicting nationality of last name (a common example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float,\n",
    "          mean=0.0, std=0.01, requires_grad=False):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "Let's download [training](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz) and [testing](https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz) data for last names.   This data set is a bunch of last names and the nationality or language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/names_train.csv\", header=None)\n",
    "df_train.columns = ['name','language']\n",
    "df_test = pd.read_csv(\"data/names_train.csv\", header=None)\n",
    "df_test.columns = ['name','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 2), (13374, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name language\n",
       "0   Adsit    Czech\n",
       "1  Ajdrna    Czech"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8340</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8341</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8342</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8353</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8354</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>To The First Page</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name language\n",
       "8340  To The First Page  Russian\n",
       "8341  To The First Page  Russian\n",
       "8342  To The First Page  Russian\n",
       "8343  To The First Page  Russian\n",
       "8344  To The First Page  Russian\n",
       "8345  To The First Page  Russian\n",
       "8346  To The First Page  Russian\n",
       "8347  To The First Page  Russian\n",
       "8348  To The First Page  Russian\n",
       "8349  To The First Page  Russian\n",
       "8350  To The First Page  Russian\n",
       "8351  To The First Page  Russian\n",
       "8352  To The First Page  Russian\n",
       "8353  To The First Page  Russian\n",
       "8354  To The First Page  Russian\n",
       "8355  To The First Page  Russian"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train[badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>Jevolojnov,</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6549</th>\n",
       "      <td>Lytkin,</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name language\n",
       "5976  Jevolojnov,  Russian\n",
       "6549      Lytkin,  Russian"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma = df_train['name'].str.contains(',') # might as well keep\n",
    "df_train[comma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>Awak'Yan</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>Dan'Ko</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>Dar'Kin</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name language\n",
       "3609  Awak'Yan  Russian\n",
       "4454    Dan'Ko  Russian\n",
       "4471   Dar'Kin  Russian"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['name'].str.contains(\"'\")][:3] # there are ok so keep quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "badname = df_train['name']=='To The First Page'\n",
    "df_train = df_train[~badname]\n",
    "\n",
    "badname = df_test['name']=='To The First Page'\n",
    "df_test = df_test[~badname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['name'] = df_train['name'].str.lower()\n",
    "df_test['name'] = df_test['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxlen(strings:Sequence[str]) -> int:\n",
    "    return max([len(l) for l in strings])\n",
    "\n",
    "max_len = max(maxlen(df_train['name']), maxlen(df_test['name']))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train[['name']], df_train['language']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)\n",
    "X_test, y_test = df_test[['name']], df_test['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    V = set([c for cl in letters for c in cl])\n",
    "    V = sorted(list(V))\n",
    "    ctoi = {c:i for i, c in enumerate(V)}\n",
    "    return V, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " \"'\": 1,\n",
       " ',': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'f': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'i': 11,\n",
       " 'j': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'q': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, ctoi = vocab(X['name'])\n",
    "ctoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target language (class)\n",
    "\n",
    "Get categories from training only, not valid/test sets. Then apply cats to those set y's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German',\n",
       "       'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish',\n",
       "       'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.astype('category').cat.as_ordered()\n",
    "y_cats = y_train.cat.categories\n",
    "y_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 14, 14,  4,  9, 14,  0, 14, 14,  4], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.cat.codes\n",
    "y_train.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.Categorical(y_valid, categories=y_cats, ordered=True).codes\n",
    "y_test = pd.Categorical(y_test, categories=y_cats, ordered=True).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2, 14, 14, 14, 16], dtype=int8), array([2, 2, 2, 2, 2], dtype=int8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[:5], y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode each letter of each name\n",
    "\n",
    "Each name becomes a matrix of size vocab_size x max_len. Each column represents a char and we pad with zeros out to max_len number of columns since tensors have to be same length in same dimension. \n",
    "\n",
    "This approach is wasteful in that it expands each word to len of longest but avoids having to pad explicitly, simplifying the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(strings:Sequence[str], V, ctoi, max_len=None) -> torch.tensor:\n",
    "    if max_len is None:\n",
    "        max_len = maxlen(strings)\n",
    "    X_onehot = torch.zeros(len(strings),len(V),max_len)\n",
    "    for i,name in enumerate(strings):\n",
    "        onehot = torch.zeros((len(V),max_len))\n",
    "        for j,c in enumerate(name):\n",
    "            onehot[ctoi[c],j] = 1\n",
    "        X_onehot[i] = onehot\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 1., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['cat','a','at'] # always debug with a small representative example\n",
    "o = onehot(sample, *vocab(sample))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 19])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_onehot = onehot(X_train['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_train_onehot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 19])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_onehot = onehot(X_valid['name'], V, ctoi, max_len=max_len).to(device)\n",
    "X_valid_onehot[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "\n",
    "Switching to W, U, V notation from $W_hh$ etc... from [Goodfellow and Yoshua Bengio and Aaron Courville book](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "\n",
    "We have a sequence of one-hot vectors for each word and need to predict a language for each sequence.  We need to know: vocab size (len of one hots), hidden len, and the number of target classes (langs).\n",
    "\n",
    "We must combine a name's onehots into a single vector representing word then use a simple dense linear layer to make a prediction\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W h^{(t-1)} + U x^{(t)} )\n",
    "$$\n",
    "\n",
    "where $t$ iterates through name length (or max pad length).\n",
    "\n",
    "Note this is same as concatenating old state and current input vector and applying a single $W$ matrix of size nhidden x (nhidden+|V|):\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\text{ReLU}( W [h^{(t-1)};x^{(t)}] )\n",
    "$$\n",
    "\n",
    "The output is avail at every char but we only need the last one:\n",
    "\n",
    "$$\n",
    "y^{(t)} = V h^{(t)}\n",
    "$$\n",
    "\n",
    "This $V$ acts like the last dense linear layer which converts the hidden state to likelihood of each target class.\n",
    "\n",
    "*What are the embeddings?* I think those are the final $h^{(t)}$ vectors, one of which is computed per name.  What are char-vec embeddings? Maybe $U$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record-by-record (slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN_slow(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN_slow, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Help avoid vanishing gradient. Start with identity, which has\n",
    "        # effect of summing char vector embeddings\n",
    "        self.W  = torch.eye(hidden_size, hidden_size).double() #randn(hidden_size, hidden_size, std=0.01).double()\n",
    "        self.U  = torch.eye(hidden_size, input_size).double()\n",
    "        self.V  = torch.eye(output_size, hidden_size).double()\n",
    "        self.W  = nn.Parameter(self.W)\n",
    "        self.U  = nn.Parameter(self.U)\n",
    "        self.V  = nn.Parameter(self.V)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).double().to(device)\n",
    "        for i in range(batch_size):\n",
    "            # Reset hidden state (history) at start of every record\n",
    "            # Use same W and U matrices for all records until SGD update step\n",
    "            h = torch.zeros((self.hidden_size, 1)).double().to(device)\n",
    "            for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "                h = self.W.mm(h) + self.U.mm(X[i,:,j].reshape(-1,1))\n",
    "                h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "            # we now have an h vector that is the embedding for the ith record\n",
    "            # we have encoded/embedded the X[i] record into h\n",
    "            # compute an output value, one per record\n",
    "            ot = self.V.mm(h)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "            o[i] = ot.reshape(-1)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "rnn = LastNameRNN_slow(input_size=len(V), hidden_size=10, output_size=len(y_cats)).to(device)\n",
    "y_pred = rnn(torch.tensor(X_train_onehot[:100],device=device).double())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctrain(model:nn.Module, train_data:TensorDataset, valid_data:TensorDataset,\n",
    "           epochs=350,\n",
    "           test_size=0.20,\n",
    "           learning_rate = 0.002,\n",
    "           batch_size=32,\n",
    "           weight_decay=1.e-4,\n",
    "           loss_fn=F.cross_entropy,\n",
    "           metric=accuracy_score,\n",
    "           print_every=30):\n",
    "    \"Train a regressor\"\n",
    "    history = []\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for ei in range(epochs): # epochs\n",
    "        for bi, (batch_x, batch_y) in enumerate(train_loader): # mini-batch\n",
    "#             if len(batch_x)!=batch_size:\n",
    "#                 print(f\"\\tBatch {bi:3d} len {len(batch_x)}\")\n",
    "            y_prob = model(batch_x)\n",
    "#             print(\"y pred\", y_prob, \"batch_y\", batch_y)\n",
    "            loss = loss_fn(y_prob, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad and M.grad\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss        = loss_fn(model(train_data.tensors[0]), train_data.tensors[1])\n",
    "            loss_valid  = loss_fn(model(valid_data.tensors[0]), valid_data.tensors[1])\n",
    "            y_prob = model(train_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_train = metric(y_pred.cpu(), train_data.tensors[1].cpu())\n",
    "            y_prob = model(valid_data.tensors[0])\n",
    "            y_prob = F.softmax(y_prob, dim=1)\n",
    "            y_pred = torch.argmax(y_prob, dim=1)\n",
    "            metric_valid = metric(y_pred.cpu(), valid_data.tensors[1].cpu())\n",
    "\n",
    "        history.append( (loss, loss_valid) )\n",
    "        if ei % print_every == 0:\n",
    "            print(f\"Epoch {ei:3d} loss {loss:7.4f}, {loss_valid:7.4f}   {metric.__class__.__name__} {metric_train:4.3f}, {metric_valid:4.3f}\")\n",
    "\n",
    "    history = torch.tensor(history)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428266983/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  1.6426,  1.7720   function 0.543, 0.538\n",
      "Epoch   1 loss  1.5139,  1.7041   function 0.572, 0.546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cfca991fc007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         print_every=1)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f9603efa00fd>\u001b[0m in \u001b[0;36mctrain\u001b[0;34m(model, train_data, valid_data, epochs, test_size, learning_rate, batch_size, weight_decay, loss_fn, metric, print_every)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#             if len(batch_x)!=batch_size:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#                 print(f\"\\tBatch {bi:3d} len {len(batch_x)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#             print(\"y pred\", y_prob, \"batch_y\", batch_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-788c22c69203>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamelen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for all chars in max name length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# better than sigmoid for vanishing gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# we now have an h vector that is the embedding for the ith record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = LastNameRNN_slow(input_size=len(V),\n",
    "                      hidden_size=100,\n",
    "                      output_size=len(y_cats)).to(device)\n",
    "subset=1000\n",
    "train = TensorDataset(X_train_onehot[:subset].double().to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].double().to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=10,\n",
    "                        learning_rate=.02,\n",
    "                        weight_decay=0.00001,\n",
    "                        batch_size=32,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep-by-step (fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNameRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LastNameRNN, self).__init__()\n",
    "#         print(\"Model: \",input_size, hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # combine W and U into W then cat h and input\n",
    "        self.W  = nn.Linear(hidden_size+input_size, hidden_size)\n",
    "        self.V  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "#         print(\"X\", X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        namelen = X.shape[2]\n",
    "        # record softmax vec of output_size for each record\n",
    "        o = torch.zeros((batch_size, self.output_size)).to(device)\n",
    "        # now that we do all char j in a batch, h is a matrix\n",
    "        h = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        for j in range(namelen):  # for all chars in max name length\n",
    "#                 print(h.shape, X[i].shape, X[i,:,j].shape, self.U.shape)\n",
    "            xj = X[:,:,j] # jth char for all records in batch\n",
    "#             print(\"W\", self.W.weight.shape, \"h\", h.shape, \"xj\", xj.shape)\n",
    "            combined = torch.cat((h, xj),dim=1)\n",
    "#             print(\"combined\", combined.shape)\n",
    "            h = self.W(combined)\n",
    "            h = torch.relu(h)  # better than sigmoid for vanishing gradient\n",
    "        # we now have an h vector that is the embedding for the ith record\n",
    "        # we have encoded/embedded the X[i] record into h\n",
    "        # compute an output value, one per record\n",
    "        ot = self.V(h)\n",
    "#         print(\"ot shape\", ot.shape)\n",
    "#             o[i] = F.softmax(ot.reshape(-1))\n",
    "#         o[i] = ot.reshape(-1)\n",
    "        return ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  1.6701,  1.6807   function 0.467, 0.475\n",
      "Epoch   1 loss  1.4818,  1.5143   function 0.554, 0.545\n",
      "Epoch   2 loss  1.4367,  1.4832   function 0.568, 0.553\n",
      "Epoch   3 loss  1.4913,  1.5338   function 0.551, 0.530\n",
      "Epoch   4 loss  1.4123,  1.4630   function 0.569, 0.551\n",
      "Epoch   5 loss  1.4146,  1.4785   function 0.576, 0.560\n",
      "Epoch   6 loss  1.3917,  1.4516   function 0.577, 0.563\n",
      "Epoch   7 loss  1.3888,  1.4609   function 0.579, 0.564\n",
      "Epoch   8 loss  1.3687,  1.4430   function 0.582, 0.567\n",
      "Epoch   9 loss  1.3506,  1.4277   function 0.584, 0.569\n",
      "Epoch  10 loss  1.3457,  1.4369   function 0.582, 0.571\n",
      "Epoch  11 loss  1.2772,  1.3621   function 0.620, 0.598\n",
      "Epoch  12 loss  1.2708,  1.3756   function 0.615, 0.595\n",
      "Epoch  13 loss  1.2910,  1.4323   function 0.618, 0.606\n",
      "Epoch  14 loss  1.1435,  1.2635   function 0.677, 0.662\n",
      "Epoch  15 loss  1.1574,  1.2842   function 0.665, 0.646\n",
      "Epoch  16 loss  1.0826,  1.2262   function 0.688, 0.667\n",
      "Epoch  17 loss  1.0594,  1.1986   function 0.699, 0.672\n",
      "Epoch  18 loss  1.0208,  1.1515   function 0.704, 0.679\n",
      "Epoch  19 loss  1.0259,  1.1549   function 0.705, 0.682\n",
      "Epoch  20 loss  0.9993,  1.1645   function 0.710, 0.683\n",
      "Epoch  21 loss  1.0111,  1.1730   function 0.719, 0.691\n",
      "Epoch  22 loss  0.9870,  1.1536   function 0.730, 0.698\n",
      "Epoch  23 loss  0.9470,  1.1420   function 0.733, 0.699\n",
      "Epoch  24 loss  0.9254,  1.1356   function 0.741, 0.711\n",
      "Epoch  25 loss  0.9185,  1.1306   function 0.737, 0.705\n",
      "Epoch  26 loss  0.9277,  1.1553   function 0.741, 0.708\n",
      "Epoch  27 loss  0.9067,  1.1009   function 0.746, 0.713\n",
      "Epoch  28 loss  0.9052,  1.1067   function 0.744, 0.711\n",
      "Epoch  29 loss  0.9279,  1.1320   function 0.743, 0.703\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn = LastNameRNN(input_size=len(V),\n",
    "                  hidden_size=100,\n",
    "                  output_size=len(y_cats)).to(device)\n",
    "subset=10_000\n",
    "train = TensorDataset(X_train_onehot[:subset].to(device), torch.tensor(y_train[:subset].values).long().to(device))\n",
    "valid = TensorDataset(X_valid_onehot[:subset].to(device), torch.tensor(y_valid[:subset]).long().to(device))\n",
    "model, history = ctrain(rnn, train, valid,\n",
    "#                         loss_fn=torch.nn.BCELoss(),\n",
    "                        loss_fn=F.cross_entropy,\n",
    "                        metric=accuracy_score,\n",
    "                        epochs=30,\n",
    "                        learning_rate=.001,\n",
    "                        weight_decay=0.000001,#002,\n",
    "                        batch_size=64,\n",
    "                        print_every=1)\n",
    "\n",
    "plot_history(history, yrange=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TensorDataset, if you see `TypeError: 'int' object is not callable`, it means you've passed a numpy array.\n",
    "\n",
    "If it says \"expected Long got Char\", it might mean int8 not char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5)\n",
    "input, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
