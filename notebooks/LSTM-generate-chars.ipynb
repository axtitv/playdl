{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with an LSTM and Keras\n",
    "\n",
    "Redo with chars not tokens.  Also, step by 3 through chars when getting windows (didn't do this for tokens might make big difference so go back and try.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers, Sequential, losses\n",
    "import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus\n",
    "\n",
    "Let's use [Alexander Hamilton's federalist papers 1-10](https://guides.loc.gov/federalist-papers/text-1-10#s-lg-box-wrapper-25493264) as our corpus.\n",
    "\n",
    "Try with https://s3.amazonaws.com/text-datasets/nietzsche.txt which is 6x bigger.\n",
    "\n",
    "Ah. also lowercase it to be like keras book and reduce target space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface supposing that truth is a woman--what then? is there not ground for suspecting that all philosophers, in so far as they have been dogmatists, have failed to understand women--that the terrible seriousness and clumsy importunity with which they have usually paid their addresses to truth, have'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = get_text(\"data/federalist-papers.txt\")\n",
    "text = get_text(\"data/nietzsche.txt\").lower()\n",
    "text = compress_whitespace(text)\n",
    "text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "#text = text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab and get X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = sorted(set(tokens))\n",
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {c:i for i,c in enumerate(V)}\n",
    "def ctoi(c):\n",
    "    return index[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 60\n",
    "step = 1\n",
    "Xy = [np.array((np.array(tokens[i-k:i],dtype=object),tokens[i])) for i in range(k,len(tokens)-1,step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([array(['p', 'r', 'e', 'f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o',\n",
       "        's', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u',\n",
       "        't', 'h', ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n',\n",
       "        '-', '-', 'w', 'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ',\n",
       "        'i', 's', ' ', 't', 'h', 'e', 'r', 'e'], dtype=object),\n",
       "        ' '], dtype=object),\n",
       " array([array(['r', 'e', 'f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o', 's',\n",
       "        'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u', 't',\n",
       "        'h', ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n', '-',\n",
       "        '-', 'w', 'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ', 'i',\n",
       "        's', ' ', 't', 'h', 'e', 'r', 'e', ' '], dtype=object),\n",
       "        'n'], dtype=object),\n",
       " array([array(['e', 'f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o', 's', 'i',\n",
       "        'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u', 't', 'h',\n",
       "        ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n', '-', '-',\n",
       "        'w', 'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ', 'i', 's',\n",
       "        ' ', 't', 'h', 'e', 'r', 'e', ' ', 'n'], dtype=object),\n",
       "        'o'], dtype=object),\n",
       " array([array(['f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o', 's', 'i', 'n',\n",
       "        'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u', 't', 'h', ' ',\n",
       "        'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n', '-', '-', 'w',\n",
       "        'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ', 'i', 's', ' ',\n",
       "        't', 'h', 'e', 'r', 'e', ' ', 'n', 'o'], dtype=object),\n",
       "        't'], dtype=object),\n",
       " array([array(['a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o', 's', 'i', 'n', 'g',\n",
       "        ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u', 't', 'h', ' ', 'i',\n",
       "        's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n', '-', '-', 'w', 'h',\n",
       "        'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ', 'i', 's', ' ', 't',\n",
       "        'h', 'e', 'r', 'e', ' ', 'n', 'o', 't'], dtype=object),\n",
       "        ' '], dtype=object)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = np.array(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Xy[:,0], Xy[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['p', 'r', 'e', 'f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o',\n",
       "        's', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u',\n",
       "        't', 'h', ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n',\n",
       "        '-', '-', 'w', 'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ',\n",
       "        'i', 's', ' ', 't', 'h', 'e', 'r', 'e'],\n",
       "       ['r', 'e', 'f', 'a', 'c', 'e', ' ', 's', 'u', 'p', 'p', 'o', 's',\n",
       "        'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 't', 'r', 'u', 't',\n",
       "        'h', ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'm', 'a', 'n', '-',\n",
       "        '-', 'w', 'h', 'a', 't', ' ', 't', 'h', 'e', 'n', '?', ' ', 'i',\n",
       "        's', ' ', 't', 'h', 'e', 'r', 'e', ' ']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack(X)\n",
    "X[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encode tokens in X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = np.vectorize(ctoi)\n",
    "X = encode(X)\n",
    "y = encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.unique(y)   # not every word in V will be in target classes (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598808, 60), (598808,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 43, 30, 31, 26, 28, 30,  0, 44, 46, 41, 41, 40, 44, 34, 39, 32,\n",
       "        0, 45, 33, 26, 45,  0, 45, 43, 46, 45, 33,  0, 34, 44,  0, 26,  0,\n",
       "       48, 40, 38, 26, 39,  7,  7, 48, 33, 26, 45,  0, 45, 33, 30, 39, 22,\n",
       "        0, 34, 44,  0, 45, 33, 30, 43, 30])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X to shape (num sequences, window width k, len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598808,), 58, 58)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, len(V), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598808, 58)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot the tokens (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_onehot = True\n",
    "#do_onehot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(X):\n",
    "    X_onehot = np.zeros((len(X), k, len(V)), dtype=np.bool)\n",
    "    for i,record in enumerate(X):\n",
    "        onehot = np.zeros((k,len(V)), dtype=np.bool)\n",
    "        for j,wi in enumerate(record):\n",
    "            onehot[j,wi] = 1\n",
    "        X_onehot[i] = onehot\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_onehot:\n",
    "    X = onehot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "if do_onehot:\n",
    "    # Must one hot X as num records x k x len(V)\n",
    "    model.add(layers.LSTM(units=128, input_shape=(k,len(V))))\n",
    "else:\n",
    "    # If you don't want to onehot, you can leave X as 2D num records x k.\n",
    "    model.add(layers.Embedding(input_dim=len(V), output_dim=10, input_length=k))\n",
    "    model.add(layers.LSTM(units=128, input_shape=(k,1)))\n",
    "# model.add(layers.Dropout(0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(len(targets), activation='softmax'))\n",
    "#model.add(layers.Lambda(lambda x: tf.cast(K.argmax(x, axis=-1),dtype=float)))\n",
    "\n",
    "# opt = optimizers.Adam(learning_rate=0.001)\n",
    "opt = optimizers.RMSprop(lr=0.01) # keras book uses this\n",
    "\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfit(epochs, batch_size=1, verbose=0):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        shuffle=True,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose\n",
    "#                         , callbacks=[tfa.callbacks.TQDMProgressBar(show_epoch_progress=True)]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_train_function_2664]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-86227132fcdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-af8a38f2616b>\u001b[0m in \u001b[0;36mmyfit\u001b[0;34m(epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#                         , callbacks=[tfa.callbacks.TQDMProgressBar(show_epoch_progress=True)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_train_function_2664]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "myfit(19, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Deep Learning with Python by François Chollet\n",
    "# Gets a single int target class from a distribution described by probabilities\n",
    "# (from softmax) in probs.  The temperature adds noise where temperature=0 means\n",
    "# pick most likely always.\n",
    "def sample(probs, temperature=1.0):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    probs = np.log(probs) / temperature\n",
    "    exp_probs = np.exp(probs)\n",
    "    probs = exp_probs / np.sum(exp_probs)\n",
    "    probs = np.random.multinomial(1, probs, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the text with k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(tokens) - k - 1)\n",
    "generated_words = tokens[start: start + k]\n",
    "print(''.join(generated_words))\n",
    "generated_tokens = [ctoi(w) for w in generated_words]\n",
    "generated_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(1,40):\n",
    "    print(f\"-------- {epochs} epochs --------------------------------\")\n",
    "    myfit(epochs=1, verbose=1) # fits one iteration\n",
    "    print('-'.join(generated_words), end=' ') # same seed\n",
    "    for i in range(400):\n",
    "        if do_onehot:\n",
    "            onehot = np.zeros((1,k,len(V)), dtype=np.bool)\n",
    "            for j,ci in enumerate(generated_tokens):\n",
    "                onehot[0,j,ci] = 1\n",
    "            X1 = onehot\n",
    "        else:\n",
    "            X1 = np.array(generated_tokens).reshape(1,k)\n",
    "        y_prob = model.predict(X1, verbose=0)[0]\n",
    "        next_token = sample(y_prob, temperature=0.5)\n",
    "        print(V[next_token], end='')\n",
    "        generated_tokens.append(next_token)\n",
    "        generated_tokens = generated_tokens[1:]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "* gotta use a lot of data. started working well with 6x nietchse not federalist papers. \n",
    "* hmm...step seems to be just an efficiency issue\n",
    "* what about batch size vs max len? Seems like we gotta line up sentences so they line up across batches, unless it resets h each batch. fastai book for LMModel3 inits h in `__init__` not `forward` but then uses truncated backprop (of len equal to seqence length k). It also then has to line up the batches.\n",
    "* what is effect of onehot vs embedding layer? With same setup but with len(V) sized embeddings for chars going into LSTM rather than one hot: got weird div by zero errors and valid accuracy maxed out at .49 with loss 2.0 whereas with no embedding before LSTM, got valid .56 accur and loss 1.59.  Maybe a function of embedding size? `layers.Embedding(input_dim=len(V), output_dim=len(V), input_length=k)`\n",
    "\n",
    "W/o embeddings at about epoch 60:\n",
    "\n",
    "```\n",
    "1248/1248 [==============================] - 15s 12ms/step - loss: 1.1914 - accuracy: 0.6397 - val_loss: 1.5969 - val_accuracy: 0.5601\n",
    "r-d-e-r-)-,- -r-e-l-i-g-i-o-n- -i-t-s-e-l-f- -m-a-y- -b-e- -u-s-e-d- -a-s- -a- -m-e-a-n-s- -f-o-r- -o-b-t-a-i-n-i-n-g-  nce and simultage perseined to do a desire, that he understand of the best to the world of the contemplation of the so and at the desiress and strength, and accuiration to from the his esseced to such as a stronger man and worst of the soul in a soully of the best to cause the recognized in the sense of any constant their literal, and so much man of the problems to the self-explained by the sight \n",
    "```\n",
    "\n",
    "With embeddings:\n",
    "\n",
    "```\n",
    "1248/1248 [==============================] - 17s 14ms/step - loss: 1.8657 - accuracy: 0.5002 - val_loss: 2.0086 - val_accuracy: 0.4913\n",
    "a-t-e- -o-f- -h-i-s- -s-o-u-l-,- -h-e- -w-i-s-h-e-d- -t-o- -b-e- -d-o-u-b-t-f-u-l- -o-f- -h-i-s- -o-w-n- -c-a-p-a-c-i-t e bei\n",
    "dency of who the is a pain of world and the present the regariss. The now to constinh-all alon a not or the possible and the powerful maken usfections of the under skecoflune and the makes of the sociement: in the to the greates all all the laid the should respection to a very the subject and that all the repxing the world of the sothing in the because bet the being bess that really of the ma\n",
    "```\n",
    "\n",
    "which looks much worse.\n",
    "\n",
    "Accuracy is higher for char than for tokens likely due to much larger token space than char space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
