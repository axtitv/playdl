{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation vectorized\n",
    "\n",
    "Let's do French -> English. French has multiple phrases that map to single English phrase so can't do English->French as well. E.g.,\n",
    "\n",
    "```\n",
    "Get ready.      Prépare-toi.\n",
    "Get ready.      Préparez-vous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "import editdistance # Get Levenshtein (pip install editdistance)\n",
    "import re\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, input_size, embed_sz):\n",
    "        self.E = torch.randn(embed_sz, input_size, device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "        self.input_size = input_size\n",
    "        self.embed_sz = embed_sz\n",
    "#         with torch.no_grad():\n",
    "#             self.E *= 0.01\n",
    "    def parameters(self): return [self.E]\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, int) or (x.dim()==0 or isinstance(x, torch.Tensor) and x.dim()==1 and len(x)==1):\n",
    "            batch_size = 1\n",
    "        elif isinstance(x, torch.Tensor) and x.dim()==1:\n",
    "            batch_size = x.shape[0]\n",
    "        if isinstance(x, torch.Tensor): x.dim()==1\n",
    "        \n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        return self.E[:,x].reshape(self.embed_sz, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_sz, nhidden):\n",
    "        self.W = torch.eye(nhidden,    nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.U = torch.randn(nhidden,  input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.bx = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             self.W *= 0.01\n",
    "#             self.U *= 0.01\n",
    "    def parameters(self): return [self.W, self.U, self.bx]\n",
    "    def __call__(self, h, x):\n",
    "        h = self.W@h + self.U@x + self.bx\n",
    "        h = torch.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(RNN):\n",
    "    def __init__(self, input_sz, context_sz, nhidden):\n",
    "        super().__init__(input_sz, nhidden)\n",
    "        self.C = torch.eye(nhidden,    context_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "    def parameters(self): return super().parameters()+[self.C]\n",
    "    def __call__(self, h, c, x):\n",
    "        h = self.W@h + self.C@c + self.U@x + self.bx\n",
    "        h = torch.tanh(h)\n",
    "        return h    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_sz, nhidden, include_bias=False):\n",
    "        self.Whz  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whr  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whh_ = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxh_ = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxz  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxr  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        # if include_bias these stay 0\n",
    "        self.bz   = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.br   = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.bh_  = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.include_bias = include_bias\n",
    "    def parameters(self):\n",
    "        p = [self.Whz, self.Whr, self.Whh_, self.Uxh_, self.Uxz, self.Uxr]\n",
    "        if self.include_bias:\n",
    "            p += [self.bz, self.br, self.bh_]    \n",
    "        return p\n",
    "    def __call__(self, h, x):\n",
    "        z = torch.sigmoid(self.Whz@h    + self.Uxz@x  + self.bz)\n",
    "        r = torch.sigmoid(self.Whr@h    + self.Uxr@x  + self.br)\n",
    "        h_ = torch.tanh(self.Whh_@(r*h) + self.Uxh_@x + self.bh_)\n",
    "#         print(h.shape, z.shape, r.shape, h_.shape)\n",
    "        h = torch.tanh( (1-z)*h + z*h_ )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(GRU):\n",
    "    def __init__(self, input_sz, context_sz, nhidden, include_bias=False):\n",
    "        super().__init__(input_sz, nhidden, include_bias)\n",
    "        self.C = torch.eye(nhidden,    context_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "    def parameters(self): return super().parameters()+[self.C]\n",
    "    def __call__(self, h, c, x):\n",
    "        z = torch.sigmoid(self.Whz@h    + self.C@c + self.Uxz@x  + self.bz)\n",
    "        r = torch.sigmoid(self.Whr@h    + self.C@c + self.Uxr@x  + self.br)\n",
    "        h_ = torch.tanh(self.Whh_@(r*h) + self.C@c + self.Uxh_@x + self.bh_)\n",
    "        h = torch.tanh( (1-z)*h + z*h_ )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.V = torch.randn(output_size,  input_size, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, 1,          device=device, dtype=torch.float64, requires_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             self.V *= 0.01\n",
    "    def parameters(self): return [self.V, self.by]\n",
    "    def __call__(self, h):\n",
    "        o = self.V@h + self.by\n",
    "        o = o.T # make it input_size x output_size\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.0, fixed=False):\n",
    "        \"\"\"\n",
    "        If fixed, reuse same mask for all future uses of this layer.\n",
    "        Assumes v columns are the layer activations. If batch size is 1, then this will be a column vector.\n",
    "        Same column knockout used for each column in incoming matrix and future invocations if fixed.\n",
    "        If not fixed, different knockout mask used for each column.\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.fixed = fixed\n",
    "        self.mask = None\n",
    "    def __call__(self, v):\n",
    "        return v\n",
    "        \"\"\"\n",
    "        Column(s) are activation vectors. Get a new column mask and knockout elements with\n",
    "        it for each column (unless fixed).\n",
    "        \"\"\"\n",
    "        if isinstance(v, list):\n",
    "            v = torch.tensor(v, device=device)\n",
    "\n",
    "        if self.fixed and self.mask is None:\n",
    "            mast = self.mask = (usample>self.p).int()\n",
    "\n",
    "        usample = torch.empty_like(v).uniform_(0, 1)     # get random value for each activation matrix element\n",
    "        mask = (usample>self.p).int()                    # get boolean mask as \"those with value greater than p\"\n",
    "        v = v * mask                                     # kill masked activations\n",
    "        v /= 1 - self.p                                  # scale during training by 1/(1-p) to avoid scaling by p at test time\n",
    "                                                         # after dropping p activations, (1-p) are left untouched, on average\n",
    "        return v            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 20.],\n",
       "        [ 1., 21.],\n",
       "        [ 2., 22.],\n",
       "        [ 3., 23.],\n",
       "        [ 4., 24.],\n",
       "        [ 5., 25.],\n",
       "        [ 6., 26.],\n",
       "        [ 7., 27.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([range(8),range(20,28)], dtype=torch.float64).T\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20., 21., 22., 23., 24., 25., 26., 27.], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/eng-fra.txt\") as f:\n",
    "    text = f.read().strip().lower()\n",
    "\n",
    "# clean up, normalize\n",
    "text = re.sub(r\"[ \\u202f\\u209f\\u20bf\\u2009\\u3000\\xa0]+\", \" \", text)  # there are lots of space chars in unicode\n",
    "text = re.sub(r\"\\u200b|\\xad|‐|–\", \"-\", text)  # there are lots of space chars in unicode\n",
    "text = re.sub(r\"‘|’\", \"'\", text)  # there are lots of space chars in unicode\n",
    "text = text.replace(\"‽\", \"?\")\n",
    "text = text.replace(\"…\", \"\")\n",
    "text = text.replace(\"₂\", \"\")\n",
    "# text = text.replace(\"\\u202f\", \" \")\n",
    "# text = text.replace(\"\\u209f\", \" \")\n",
    "# text = text.replace(\"\\u20bf\", \" \")\n",
    "text = text.replace(\" !\", \"\")\n",
    "text = text.replace(\" .\", \"\")\n",
    "text = re.sub(r\"([.!?])\", \"\", text)\n",
    "lines = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135614"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for line in lines if not len(set(line).intersection({'(',')','~','€','$','%','&','/','«','»'}))]\n",
    "pairs = [line.split('\\t') for line in lines]\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9748"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 18\n",
    "pairs = [p for p in pairs if len(p[0])<=MAX_LENGTH and len(p[1])<=MAX_LENGTH]\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = False\n",
    "if FILTER:\n",
    "    eng_prefixes = (\n",
    "        \"i am \", \"i'm \",\n",
    "        \"he is \", \"he's \",\n",
    "        \"she is \", \"she's \",\n",
    "        \"you are \", \"you're \",\n",
    "        \"we are \", \"we're \",\n",
    "        \"they are \", \"they're \"\n",
    "        )\n",
    "    filtered_pairs = []\n",
    "    for p in pairs:\n",
    "        en,fr = p\n",
    "        for pre in eng_prefixes:\n",
    "            if en.startswith(pre):\n",
    "                filtered_pairs.append(p)\n",
    "                break\n",
    "\n",
    "    pairs = filtered_pairs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[0:1000] # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(p[1],p[0]) for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "pairs = list(dict(pairs).items())\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set('\\n'.join(lines)))\n",
    "vocab = vocab[2:] # drop \\t and \\n\n",
    "vocab = ['<','>']+vocab # add delimiters as 0, 1\n",
    "ctoi = {c:i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<> \"\\'+,-0123456789:;abcdefghijklmnopqrstuvwxyzàâçèéêëîïòôöùúûœас'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('va', 'go'),\n",
       " ('cours', 'run'),\n",
       " ('courez', 'run'),\n",
       " ('ça alors', 'wow'),\n",
       " ('au feu', 'fire'),\n",
       " (\"à l'aide\", 'help'),\n",
       " ('saute', 'jump'),\n",
       " ('ça suffit', 'stop'),\n",
       " ('stop', 'stop'),\n",
       " ('arrête-toi', 'stop')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5],\n",
       "        [1, 6],\n",
       "        [2, 7],\n",
       "        [3, 8],\n",
       "        [4, 9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(range(5)).reshape(-1,1)\n",
    "b = torch.tensor(range(5,10)).reshape(-1,1)\n",
    "torch.cat([a,b], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap in <...> and Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('va', '<go>'),\n",
       " ('cours', '<run>'),\n",
       " ('courez', '<run>'),\n",
       " ('ça alors', '<wow>'),\n",
       " ('au feu', '<fire>')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(f\"{p[0]}\",f\"<{p[1]}>\") for p in pairs]  # X doesn't need <...> brackets\n",
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('va', '<go>'),\n",
       " ('cours', '<run>'),\n",
       " ('courez', '<run>'),\n",
       " ('ça alors', '<wow>'),\n",
       " ('au feu', '<fire>')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 46,  2, 31,  4, 20, 28, 23, 24],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 38, 20, 40, 39, 24],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 48, 20,  2, 38, 40, 25, 25, 28, 39],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 38, 39, 34, 35],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 37, 37, 51, 39, 24,  7, 39, 34, 28]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numericalize and left pad\n",
    "X = torch.zeros(len(pairs), MAX_LENGTH, device=device, dtype=torch.long) # zero implies padding\n",
    "for i,p in enumerate(pairs):\n",
    "    fr, en = p\n",
    "    pad = MAX_LENGTH - len(fr)\n",
    "    for j in range(len(fr)):\n",
    "        X[i,j+pad] = ctoi[fr[j]]\n",
    "X[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 26, 34,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1],\n",
       "        [ 0, 37, 40, 33,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1],\n",
       "        [ 0, 37, 40, 33,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1],\n",
       "        [ 0, 42, 34, 42,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1],\n",
       "        [ 0, 25, 28, 37, 24,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for i,p in enumerate(pairs):\n",
    "    fr, en = p\n",
    "    pad = MAX_LENGTH - len(en) + 2 # include <...>\n",
    "    Y.append([ctoi[d] for d in en]+[ctoi['>']]*pad)  # pad with \"end of string\" symbols '>'\n",
    "Y = torch.tensor(Y, device=device)\n",
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 884, after 884\n"
     ]
    }
   ],
   "source": [
    "char_embed_sz = 10\n",
    "nhidden = 512\n",
    "nclasses = len(vocab) # char output vocab\n",
    "batch_size = 16\n",
    "\n",
    "n = len(X)\n",
    "n = batch_size * n//batch_size\n",
    "print(f\"before {len(X)}, after {n}\")\n",
    "X, Y = X[:n], Y[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridx = torch.randperm(len(X))\n",
    "# shuffle\n",
    "X = X[ridx]\n",
    "Y = Y[ridx]\n",
    "# split\n",
    "ntrain = int(0.8 * len(X))\n",
    "X_train, X_test = X[:ntrain], X[ntrain:]\n",
    "Y_train, Y_test = Y[:ntrain], Y[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tostr(x):\n",
    "    s = ''.join([vocab[v] for v in x])\n",
    "    if '>' in s:\n",
    "        i = s.index('>')\n",
    "        return s[0:i+1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer:\n",
    "    def __init__(self, input_sz, output_sz, input_embed_sz, output_embed_sz, nhidden, \n",
    "                 dropout=0.0,\n",
    "                 useGRU=False):\n",
    "        self.dropout = dropout\n",
    "        self.embx = Embedding(input_sz, input_embed_sz)\n",
    "        self.emby = Embedding(output_sz, output_embed_sz)\n",
    "        self.lin = Linear(nhidden, output_sz)\n",
    "        if useGRU:\n",
    "            self.encoder = GRU(input_embed_sz, nhidden)\n",
    "            self.decoder = DecoderGRU(output_embed_sz, nhidden, nhidden)\n",
    "        else:\n",
    "            self.encoder = RNN(input_embed_sz, nhidden)\n",
    "            self.decoder = DecoderRNN(output_embed_sz, nhidden, nhidden)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.embx.parameters()+\\\n",
    "               self.emby.parameters()+\\\n",
    "               self.lin.parameters()+\\\n",
    "               self.encoder.parameters()+\\\n",
    "               self.decoder.parameters()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        encoder_h_dropout = Dropout(p=self.dropout, fixed=False)\n",
    "        decoder_h_dropout = Dropout(p=self.dropout, fixed=False)\n",
    "        \n",
    "        x_dropout = Dropout(p=self.dropout, fixed=False)\n",
    "        y_dropout = Dropout(p=self.dropout, fixed=False)\n",
    "        z_dropout = Dropout(p=self.dropout, fixed=False)\n",
    "        \n",
    "        if isinstance(x, list):\n",
    "            x = torch.tensor(x, device=device)\n",
    "        if isinstance(y, list):\n",
    "            y = torch.tensor(y, device=device)\n",
    "        \n",
    "        assert x.dim()==1 or x.dim()==2\n",
    "        assert y.dim()==1 or y.dim()==2\n",
    "        \n",
    "        if x.dim()==1:\n",
    "            batch_size = 1\n",
    "            x = x.reshape(1,-1)\n",
    "        else:\n",
    "            batch_size = x.shape[0]\n",
    "        if y.dim()==1:\n",
    "            y = y.reshape(1,-1)\n",
    "            \n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(x.shape[1]):\n",
    "            embedding_step_t = self.embx(x[:,t])\n",
    "            embedding_step_t = x_dropout(embedding_step_t)\n",
    "#             print(embedding_step_t.shape, embedding_step_t)\n",
    "            h = self.encoder(h, embedding_step_t)\n",
    "            h = encoder_h_dropout(h)\n",
    "        c = h\n",
    "\n",
    "        # DECODER\n",
    "        output = []\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(y.shape[1]-1): # don't predict next char at final '>'\n",
    "            embedding_step_t = self.emby(y[:,t])\n",
    "            embedding_step_t = y_dropout(embedding_step_t)\n",
    "            h = self.decoder(h, c, embedding_step_t)\n",
    "            h = decoder_h_dropout(h)\n",
    "            o = self.lin(h)\n",
    "#             print(embedding_step_t.shape, o.shape, torch.tensor([y[t+1]], device=device).shape)\n",
    "            o = z_dropout(o)\n",
    "            # From y we want to predict y[1:]. at y[t], predict y[t+1] using c as context vector\n",
    "            y_true = torch.tensor(y[:,t+1], device=device).reshape(batch_size)\n",
    "            loss += F.cross_entropy(o, y_true, reduction=\"sum\")\n",
    "            p = F.softmax(o, dim=1)\n",
    "            y_pred = torch.argmax(p, dim=1) # y_pred has prediction for each record in batch\n",
    "            correct += torch.sum(y_pred==y[:,t+1])\n",
    "            output.append(y_pred.reshape(-1,1))\n",
    "        output = torch.cat(output, dim=1) # should be batch_size by (columns(y)-1)\n",
    "        return output, loss, int(correct)\n",
    "    \n",
    "    def predict(self, x, y=None):\n",
    "        \"if y not none, compute loss, accuracy\"\n",
    "        with torch.no_grad():\n",
    "            if isinstance(x, list):\n",
    "                x = torch.tensor(x, device=device)\n",
    "\n",
    "            assert x.dim()==1 or x.dim()==2 \n",
    "\n",
    "            if x.dim()==1:\n",
    "                batch_size = 1\n",
    "                x = x.reshape(1,-1)\n",
    "            else:\n",
    "                batch_size = x.shape[0]\n",
    "\n",
    "            # ENCODER\n",
    "            h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for t in range(x.shape[1]):\n",
    "                embedding_step_t = self.embx(x[:,t])\n",
    "                h = self.encoder(h, embedding_step_t)\n",
    "            c = h\n",
    "\n",
    "            # DECODER\n",
    "            loss = 0.0\n",
    "            correct = 0\n",
    "            output = []\n",
    "            # y_pred is column vector starting with '<' for each record in the batch\n",
    "            y_pred = ctoi['<']\n",
    "            y_pred = torch.full(size=(batch_size,1), fill_value=y_pred, device=device, dtype=torch.long) # begin with \"start of sequence\" char\n",
    "            output.append(y_pred)\n",
    "            h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            while len(output)<MAX_LENGTH+2: # max plus last '>' char\n",
    "                embedding_step_t = self.emby(y_pred.flatten())  # make it a list of symbols to use in embedding\n",
    "                h = self.decoder(h, c, embedding_step_t)\n",
    "                o = self.lin(h)\n",
    "                p = F.softmax(o, dim=1)\n",
    "                y_pred = torch.argmax(p, dim=1).reshape(-1,1)\n",
    "                output.append(y_pred)\n",
    "            output = torch.cat(output, dim=1) # should be batch_size by (columns(y)-1)\n",
    "        return output  \n",
    "    \n",
    "    def score(self, X_test, Y_test):\n",
    "        \"Return raw accuracy of perfect translations to total records\"\n",
    "        with torch.no_grad():\n",
    "            y_pred = trans.predict(X_test)\n",
    "            correct = 0\n",
    "            for i in range(len(X_test)):\n",
    "                correct += tostr(y_pred[i])==tostr(Y_test[i])\n",
    "    #     y_pred_real_char = torch.sum(y_pred>1)\n",
    "    #     y_real_char = torch.sum(Y_test>1)\n",
    "    #     print(torch.sum(Y_test>1))\n",
    "    #     print(y_pred)\n",
    "    #     print(Y_test)\n",
    "    #     print(y_pred==Y_test)\n",
    "        return correct/float(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss   17.295   char accur 0.1752  phrase accur  0.0000   test accur 0.000   LR 0.000050\n",
      "Epoch   2 training loss    4.165   char accur 0.6220  phrase accur  0.0028   test accur 0.000   LR 0.002700\n",
      "Epoch   3 training loss    1.649   char accur 0.7203  phrase accur  0.0198   test accur 0.000   LR 0.005350\n",
      "Epoch   4 training loss    1.200   char accur 0.7716  phrase accur  0.0354   test accur 0.011   LR 0.008000\n",
      "Epoch   5 training loss    0.730   char accur 0.8338  phrase accur  0.1768   test accur 0.034   LR 0.005350\n",
      "Epoch   6 training loss    0.294   char accur 0.9040  phrase accur  0.5884   test accur 0.119   LR 0.002700\n",
      "Epoch   7 training loss    0.093   char accur 0.9546  phrase accur  0.6181   test accur 0.102   LR 0.000050\n",
      "Epoch   8 training loss    0.072   char accur 0.9609  phrase accur  0.8685   test accur 0.164   LR 0.001375\n",
      "Epoch   9 training loss    0.044   char accur 0.9705  phrase accur  0.9180   test accur 0.175   LR 0.002700\n",
      "Epoch  10 training loss    0.042   char accur 0.9726  phrase accur  0.9137   test accur 0.181   LR 0.004025\n"
     ]
    }
   ],
   "source": [
    "trans = Transducer(input_sz=len(ctoi),\n",
    "                   output_sz=len(ctoi),\n",
    "                   input_embed_sz=char_embed_sz,\n",
    "                   output_embed_sz=char_embed_sz,\n",
    "                   nhidden=nhidden,\n",
    "                   dropout=0.0,\n",
    "                   useGRU=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(trans.parameters(), lr=0.0005, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=3,\n",
    "                                              base_lr=0.00005, max_lr=0.008,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accum_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for p in range(0, len(X_train), batch_size):  # do one epoch\n",
    "        batch_X = X_train[p:p+batch_size]\n",
    "        batch_Y = Y_train[p:p+batch_size]\n",
    "        y_pred, loss, correct = trans(batch_X, batch_Y)\n",
    "        \n",
    "#         print([tostr(y_) for y_ in y_pred])\n",
    "#         if epoch==10:\n",
    "#             print(f\"{tostr(x)}->{tostr(y)}: {tostr(y_pred)}, {correct} correct\")\n",
    "        epoch_training_accum_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += batch_size * (MAX_LENGTH + 1) # For each \"<foo>\" predict and count \"foo>\" but MAX_LENGTH doesn't include <...>\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_training_accur = trans.score(X_train, Y_train)\n",
    "    epoch_test_accur = trans.score(X_test, Y_test)\n",
    "\n",
    "    epoch_training_accum_accur /= total_compares\n",
    "    epoch_training_loss /= total_compares\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f} char accur {epoch_training_accum_accur:.4f} phrase accur {epoch_training_accur:.4f}    test accur {epoch_test_accur:.3f}   LR {scheduler.get_last_lr()[0]:7.6f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<vous conduisez : <you drive>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<you drive>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST SINGLE RECORD\n",
    "print(tostr(X_test[2]), \":\", tostr(Y_test[2]))\n",
    "y_pred = trans.predict(X_test[2], Y_test[2])\n",
    "tostr(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cuff him> == <follow him>\n",
      "<i am fad> == <i had fun>\n",
      "<you drive> == <you drive>\n",
      "<stay down> == <stay down>\n",
      "<we seated> == <after you>\n",
      "<i'm him> == <i'm buying>\n",
      "<i'm fat> == <i'm a hero>\n",
      "<it worked> == <i phoned>\n",
      "<i felmo> == <i want you>\n",
      "<we won> == <it snowed>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# TEST ALL TEST RECORDS\n",
    "y_pred = trans.predict(X_test, Y_test)\n",
    "total_correct = 0\n",
    "for i,y_ in enumerate(y_pred[0:10]):\n",
    "    total_correct += tostr(Y_test[i])==tostr(y_)\n",
    "    print(tostr(y_), \"==\", tostr(Y_test[i]))\n",
    "print(total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189533239038189"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1807909604519774"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(X,Y,verbose=(0,1,2)):\n",
    "    \"Use Levenshtein to measure how close output predictions are to truth.\"\n",
    "    with torch.no_grad():\n",
    "        total_compares = 0\n",
    "        total_correct = 0\n",
    "        total_d = 0\n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            y_pred = trans.predict(x)\n",
    "            y_pred = y_pred[0] # only one record for now\n",
    "            total_compares += len(y) - 1 # From \"<foo>\" predict \"foo>\" but don't count last '>' for metrics\n",
    "            total_correct += tostr(y)==tostr(y_pred)\n",
    "            d = editdistance.eval(tostr(y),tostr(y_pred))\n",
    "            total_d += d\n",
    "            if verbose>0:\n",
    "                if verbose>1 or d>0:\n",
    "                    print(f\"{tostr(x):20s} : {tostr(y)}\")\n",
    "                    print(f\"{'':20s} : {tostr(y_pred):20s} Levenshtein {d} out of {len(y)}\")\n",
    "    return total_d/float(len(X)), total_correct/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training n=707 average Levenshtein score     0.85, perfect accuracy     0.82\n"
     ]
    }
   ],
   "source": [
    "avg_d, accur = check(X_train, Y_train, verbose=0)\n",
    "print(f\"Training n={len(X_train)} average Levenshtein score {avg_d:8.2f}, perfect accuracy {accur:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing n=177 average Levenshtein score     4.85, perfect accuracy     0.18\n"
     ]
    }
   ],
   "source": [
    "avg_d, accur = check(X_test, Y_test, verbose=0)\n",
    "print(f\"Testing n={len(X_test)} average Levenshtein score {avg_d:8.2f}, perfect accuracy {accur:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
