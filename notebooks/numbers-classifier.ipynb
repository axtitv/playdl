{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized RNN human numbers classifier\n",
    "\n",
    "Use fastai human numbers data to train a classifier.  The classifier is 1-to-1 so no possibility of generalizing. Just verifying my training loop and RNN.\n",
    "\n",
    "The data is from [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb). Looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/parrt/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one \n",
      "two \n",
      "three \n",
      "four \n",
      "five \n",
      "['one ', 'two ', 'three ', 'four ', 'five ']\n"
     ]
    }
   ],
   "source": [
    "text = get_text(path/'train.txt').strip()\n",
    "print(text[:28])\n",
    "lines = text.lower().split('\\n')\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "# use '#' to indicate padded (unused) char for embedding purposes\n",
    "v = set('#')\n",
    "X_vocab = ['#']\n",
    "for t in text.split():\n",
    "    if t not in v:\n",
    "        X_vocab.append(t)\n",
    "        v.add(t)\n",
    "X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nineteen'],\n",
       " ['twenty'],\n",
       " ['twenty', 'one'],\n",
       " ['twenty', 'two'],\n",
       " ['twenty', 'three']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens = [line.strip().split(' ') for line in lines]\n",
    "X_tokens[18:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vocab = {w:i for i,w in enumerate(X_vocab)}\n",
    "X_vocab['#'], X_vocab['one'], X_vocab['two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7999, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0,  2],\n",
       "        [ 0,  0,  0,  0,  0,  3],\n",
       "        ...,\n",
       "        [ 7, 29,  9, 28, 27,  7],\n",
       "        [ 7, 29,  9, 28, 27,  8],\n",
       "        [ 7, 29,  9, 28, 27,  9]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_max_len = get_max_len(X_tokens)\n",
    "X = torch.zeros(len(X_tokens), X_max_len, device=device, dtype=torch.long) # zero implies padding\n",
    "print(X.shape)\n",
    "for i in range(len(X_tokens)):\n",
    "    x = X_tokens[i]\n",
    "    pad = X_max_len - len(x)\n",
    "    for j in range(len(x)):\n",
    "        X[i,j+pad] = X_vocab[X_tokens[i][j]]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "### Create y target class vector\n",
    "\n",
    "y  is just 1..len(X_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,  ..., 7997, 7998, 7999], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(range(1,len(X_tokens)+1), device=device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['one'], ['seven', 'thousand', 'nine', 'hundred', 'ninety', 'nine'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens[0], X_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,968 training records, batch size 32, 30 features (words), 7999 target classes, state is 128-vector\n"
     ]
    }
   ],
   "source": [
    "n = len(X)\n",
    "\n",
    "nhidden = 128\n",
    "batch_size = 32\n",
    "embed_sz = 10\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X = X[0:n]\n",
    "y = y[0:n]\n",
    "nclasses = len(X_tokens) # they are unique targets\n",
    "\n",
    "print(f\"{n:,d} training records, batch size {batch_size}, {len(X_vocab)} features (words), {nclasses} target classes, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch_X, max_len:int, vocab:dict):\n",
    "    H = torch.zeros(nhidden, len(batch_X), device=device, dtype=torch.float64, requires_grad=False)\n",
    "    for t in range(max_len):\n",
    "        x_step_t = batch_X[:,t]\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = E[:,x_step_t]\n",
    "        H = W.mm(H) + U.mm(embedding_step_t) + Bx\n",
    "        H = torch.tanh(H)        \n",
    "    o = V.mm(H) + Bo\n",
    "    o = o.T # make it batch_size x nclasses\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 29.2978 accur  0.0001   LR 0.002000\n",
      "Epoch   2 training loss 20.3775 accur  0.0001   LR 0.003000\n",
      "Epoch   3 training loss 15.3777 accur  0.0004   LR 0.004000\n",
      "Epoch   4 training loss 12.2157 accur  0.0010   LR 0.005000\n",
      "Epoch   5 training loss 10.4732 accur  0.0016   LR 0.004000\n",
      "Epoch   6 training loss  8.7580 accur  0.0041   LR 0.003000\n",
      "Epoch   7 training loss  7.4546 accur  0.0126   LR 0.002000\n",
      "Epoch   8 training loss  6.3240 accur  0.0294   LR 0.001000\n",
      "Epoch   9 training loss  5.4212 accur  0.0545   LR 0.001500\n",
      "Epoch  10 training loss  5.2719 accur  0.0574   LR 0.002000\n",
      "Epoch  11 training loss  5.3260 accur  0.0606   LR 0.002500\n",
      "Epoch  12 training loss  5.3728 accur  0.0640   LR 0.003000\n",
      "Epoch  13 training loss  5.4092 accur  0.0689   LR 0.002500\n",
      "Epoch  14 training loss  5.0891 accur  0.0940   LR 0.002000\n",
      "Epoch  15 training loss  4.7393 accur  0.1150   LR 0.001500\n",
      "Epoch  16 training loss  4.2040 accur  0.1550   LR 0.001000\n",
      "Epoch  17 training loss  3.7977 accur  0.1954   LR 0.001250\n",
      "Epoch  18 training loss  3.5004 accur  0.2380   LR 0.001500\n",
      "Epoch  19 training loss  3.3342 accur  0.2677   LR 0.001750\n",
      "Epoch  20 training loss  3.1933 accur  0.2969   LR 0.002000\n",
      "Epoch  21 training loss  3.0466 accur  0.3204   LR 0.001750\n",
      "Epoch  22 training loss  2.7918 accur  0.3672   LR 0.001500\n",
      "Epoch  23 training loss  2.5929 accur  0.3947   LR 0.001250\n",
      "Epoch  24 training loss  2.4154 accur  0.4207   LR 0.001000\n",
      "Epoch  25 training loss  2.2522 accur  0.4507   LR 0.001125\n",
      "Epoch  26 training loss  2.0420 accur  0.5072   LR 0.001250\n",
      "Epoch  27 training loss  1.9026 accur  0.5479   LR 0.001375\n",
      "Epoch  28 training loss  1.7685 accur  0.5776   LR 0.001500\n",
      "Epoch  29 training loss  1.6485 accur  0.6062   LR 0.001375\n",
      "Epoch  30 training loss  1.5021 accur  0.6432   LR 0.001250\n",
      "Epoch  31 training loss  1.3776 accur  0.6746   LR 0.001125\n",
      "Epoch  32 training loss  1.2723 accur  0.7009   LR 0.001000\n",
      "Epoch  33 training loss  1.1603 accur  0.7244   LR 0.001063\n",
      "Epoch  34 training loss  1.0447 accur  0.7584   LR 0.001125\n",
      "Epoch  35 training loss  0.9458 accur  0.7917   LR 0.001187\n",
      "Epoch  36 training loss  0.8621 accur  0.8121   LR 0.001250\n",
      "Epoch  37 training loss  0.7866 accur  0.8311   LR 0.001187\n",
      "Epoch  38 training loss  0.7105 accur  0.8557   LR 0.001125\n",
      "Epoch  39 training loss  0.6637 accur  0.8579   LR 0.001063\n",
      "Epoch  40 training loss  0.6134 accur  0.8798   LR 0.001000\n",
      "Epoch  41 training loss  0.5263 accur  0.9024   LR 0.001031\n",
      "Epoch  42 training loss  0.4583 accur  0.9221   LR 0.001063\n",
      "Epoch  43 training loss  0.4113 accur  0.9350   LR 0.001094\n",
      "Epoch  44 training loss  0.3725 accur  0.9436   LR 0.001125\n",
      "Epoch  45 training loss  0.3419 accur  0.9518   LR 0.001094\n",
      "Epoch  46 training loss  0.3137 accur  0.9552   LR 0.001063\n",
      "Epoch  47 training loss  0.2880 accur  0.9611   LR 0.001031\n",
      "Epoch  48 training loss  0.2551 accur  0.9677   LR 0.001000\n",
      "Epoch  49 training loss  0.2151 accur  0.9777   LR 0.001016\n",
      "Epoch  50 training loss  0.1816 accur  0.9849   LR 0.001031\n",
      "Epoch  51 training loss  0.1544 accur  0.9890   LR 0.001047\n",
      "Epoch  52 training loss  0.1326 accur  0.9931   LR 0.001063\n",
      "Epoch  53 training loss  0.1112 accur  0.9949   LR 0.001047\n",
      "Epoch  54 training loss  0.0943 accur  0.9957   LR 0.001031\n",
      "Epoch  55 training loss  0.0791 accur  0.9984   LR 0.001016\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(embed_sz,      len(X_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       embed_sz,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "Bx = torch.zeros(nhidden,      batch_size,    device=device, dtype=torch.float64, requires_grad=True)\n",
    "Bo = torch.zeros(nclasses,     batch_size,    device=device, dtype=torch.float64, requires_grad=True)\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "with torch.no_grad():\n",
    "    E[:,0] = 0.0  # padding word gives 0 vector\n",
    "\n",
    "optimizer = torch.optim.Adam([E,W,U,V,Bx,Bo], lr=0.005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.001, max_lr=0.005,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 70 # gets to 100% at 70 with lr=0.001\n",
    "epochs = 55 # gets to 100% at 50 with cyclic base_lr=0.001, max_lr=0.005 every 4\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X[p:p+batch_size]\n",
    "        batch_y = y[p:p+batch_size]\n",
    "        o = forward(batch_X, X_max_len, X_vocab)\n",
    "        correct = torch.argmax(softmax(o), dim=1)==batch_y\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "\n",
    "        loss = F.cross_entropy(o, batch_y)\n",
    "#         print(loss.item())\n",
    "        total += len(batch_y)\n",
    "\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= n\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
