{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with attention\n",
    "\n",
    "Let's do French -> English. French has multiple phrases that map to single English phrase so can't do English->French as well. E.g.,\n",
    "\n",
    "```\n",
    "Get ready.      Prépare-toi.\n",
    "Get ready.      Préparez-vous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "import editdistance # Get Levenshtein (pip install editdistance)\n",
    "import re\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, input_size, embed_sz):\n",
    "        self.E = torch.randn(embed_sz, input_size, device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "        self.input_size = input_size\n",
    "        self.embed_sz = embed_sz\n",
    "#         with torch.no_grad():\n",
    "#             self.E *= 0.01\n",
    "    def parameters(self): return [self.E]\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, int):\n",
    "            return self.E[:,x].reshape(self.embed_sz, 1)\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        return self.E[:,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_sz, nhidden):\n",
    "        self.W = torch.eye(nhidden,    nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.U = torch.randn(nhidden,  input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.bx = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             self.W *= 0.01\n",
    "#             self.U *= 0.01\n",
    "    def parameters(self): return [self.W, self.U, self.bx]\n",
    "    def __call__(self, h, x):\n",
    "        h = self.W@h + self.U@x + self.bx\n",
    "        h = torch.tanh(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(RNN):\n",
    "    def __init__(self, input_sz, context_sz, nhidden):\n",
    "        super().__init__(input_sz, nhidden)\n",
    "        self.C = torch.eye(nhidden,    context_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "    def parameters(self): return super().parameters()+[self.C]\n",
    "    def __call__(self, h, c, x):\n",
    "        h = self.W@h + self.C@c + self.U@x + self.bx\n",
    "        h = torch.tanh(h)\n",
    "        return h    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_sz, nhidden, include_bias=False):\n",
    "        self.Whz  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whr  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whh_ = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxh_ = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxz  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxr  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        # if include_bias these stay 0\n",
    "        self.bz   = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.br   = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.bh_  = torch.zeros(nhidden, 1,        device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.include_bias = include_bias\n",
    "        self.first_h_shape = None # debugging\n",
    "    def parameters(self):\n",
    "        p = [self.Whz, self.Whr, self.Whh_, self.Uxh_, self.Uxz, self.Uxr]\n",
    "        if self.include_bias:\n",
    "            p += [self.bz, self.br, self.bh_]    \n",
    "        return p\n",
    "    def __call__(self, h, x):\n",
    "        if self.first_h_shape is None:\n",
    "            self.first_h_shape = h.shape\n",
    "        elif self.first_h_shape != h.shape:\n",
    "            raise ValueError(f\"hidden h vector changed shape in {self.__class__.__name__} from {self.first_h_shape} to {h.shape}\")\n",
    "        z = torch.sigmoid(self.Whz@h    + self.Uxz@x  + self.bz)\n",
    "        r = torch.sigmoid(self.Whr@h    + self.Uxr@x  + self.br)\n",
    "        h_ = torch.tanh(self.Whh_@(r*h) + self.Uxh_@x + self.bh_)\n",
    "#         print(h.shape, z.shape, r.shape, h_.shape)\n",
    "        h = torch.tanh( (1-z)*h + z*h_ )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(GRU):\n",
    "    def __init__(self, input_sz, context_sz, nhidden, include_bias=False):\n",
    "        super().__init__(input_sz, nhidden, include_bias)\n",
    "        self.C = torch.eye(nhidden,    context_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "    def parameters(self): return super().parameters()+[self.C]\n",
    "    def __call__(self, h, c, x):\n",
    "        z = torch.sigmoid(self.Whz@h    + self.C@c + self.Uxz@x  + self.bz)\n",
    "        r = torch.sigmoid(self.Whr@h    + self.C@c + self.Uxr@x  + self.br)\n",
    "        h_ = torch.tanh(self.Whh_@(r*h) + self.C@c + self.Uxh_@x + self.bh_)\n",
    "        h = torch.tanh( (1-z)*h + z*h_ )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.V = torch.randn(output_size,  input_size, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, 1,          device=device, dtype=torch.float64, requires_grad=True)\n",
    "#         with torch.no_grad():\n",
    "#             self.V *= 0.01\n",
    "    def parameters(self): return [self.V, self.by]\n",
    "    def __call__(self, h):\n",
    "        o = self.V@h + self.by\n",
    "        o = o.T # make it input_size x output_size\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.0, fixed=False):\n",
    "        \"If fixed, reuse same mask for all future uses of this layer.\"\n",
    "        self.p = p\n",
    "        self.fixed = fixed\n",
    "        self.mask = None\n",
    "    def __call__(self, v):\n",
    "        if self.fixed:\n",
    "            if self.mask is None:\n",
    "                usample = torch.empty_like(v).uniform_(0, 1) # get random value for each activation\n",
    "                self.mask = (usample>self.p).int()           # get mask as those with value greater than p\n",
    "            mask = self.mask\n",
    "        else:\n",
    "            usample = torch.empty_like(v).uniform_(0, 1) # get random value for each activation\n",
    "            mask = (usample>self.p).int()                # get mask as those with value greater than p\n",
    "        v = v * mask                                     # kill masked activations\n",
    "        v /= 1 - self.p                                  # scale during training by 1/(1-p) to avoid scaling by p at test time\n",
    "                                                         # after dropping p activations, (1-p) are left untouched, on average\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer:\n",
    "    def __init__(self, input_sz, output_sz, input_embed_sz, output_embed_sz, nhidden, \n",
    "                 dropout=0.0,\n",
    "                 useGRU=False):\n",
    "        self.dropout = dropout\n",
    "        self.embx = Embedding(input_sz, input_embed_sz)\n",
    "        self.emby = Embedding(output_sz, output_embed_sz)\n",
    "        self.lin = Linear(nhidden, output_sz)\n",
    "        if useGRU:\n",
    "            self.encoder = GRU(input_embed_sz, nhidden)\n",
    "            self.decoder = DecoderGRU(output_embed_sz, nhidden, nhidden)\n",
    "        else:\n",
    "            self.encoder = RNN(input_embed_sz, nhidden)\n",
    "            self.decoder = DecoderRNN(output_embed_sz, nhidden, nhidden)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.embx.parameters()+\\\n",
    "               self.emby.parameters()+\\\n",
    "               self.lin.parameters()+\\\n",
    "               self.encoder.parameters()+\\\n",
    "               self.decoder.parameters()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        x_dropout = Dropout(p=self.dropout, fixed=True)\n",
    "        y_dropout = Dropout(p=self.dropout, fixed=True)\n",
    "        z_dropout = Dropout(p=self.dropout, fixed=True)\n",
    "        \n",
    "        if isinstance(x, list):\n",
    "            x = torch.tensor(x)\n",
    "        if isinstance(y, list):\n",
    "            y = torch.tensor(y)\n",
    "            \n",
    "        assert x.dim()==1 or x.dim()==2\n",
    "        assert y.dim()==1 or y.dim()==2\n",
    "        \n",
    "        if x.dim()==1:\n",
    "            batch_size = 1\n",
    "            x = x.reshape(1,-1)\n",
    "        else:\n",
    "            batch_size = x.shape[0]\n",
    "        if y.dim()==1:\n",
    "            y = y.reshape(1,-1)\n",
    "            \n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(x.shape[1]):\n",
    "            embedding_step_t = self.embx(x[:,t])\n",
    "            embedding_step_t = x_dropout(embedding_step_t)\n",
    "#             print(embedding_step_t.shape, embedding_step_t)\n",
    "            h = self.encoder(h, embedding_step_t)\n",
    "        c = h\n",
    "\n",
    "        # DECODER\n",
    "        output = []\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(y.shape[1]-1): # don't predict next char at final '>'\n",
    "            embedding_step_t = self.emby(y[:,t])\n",
    "            embedding_step_t = y_dropout(embedding_step_t)\n",
    "            h = self.decoder(h, c, embedding_step_t)\n",
    "            o = self.lin(h)\n",
    "#             print(embedding_step_t.shape, o.shape, torch.tensor([y[t+1]], device=device).shape)\n",
    "            o = z_dropout(o)\n",
    "            # From y we want to predict y[1:]. at y[t], predict y[t+1] using c as context vector\n",
    "            y_true = torch.tensor(y[:,t+1], device=device).reshape(batch_size)\n",
    "            loss += F.cross_entropy(o, y_true, reduction=\"sum\")\n",
    "            p = F.softmax(o, dim=1)\n",
    "            y_pred = torch.argmax(p, dim=1)#.item()\n",
    "            correct += torch.sum(y_pred==y[:,t+1])\n",
    "            output.append(y_pred)\n",
    "        return output, loss, int(correct)\n",
    "    \n",
    "    def predict(self, x, Y_ctoi):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        assert x.dim()==1 or x.dim()==2\n",
    "        \n",
    "        if x.dim()==1:\n",
    "            batch_size = 1\n",
    "            x = x.reshape(1,-1)\n",
    "        else:\n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(x.shape[1]):\n",
    "            embedding_step_t = self.embx(x[:,t])\n",
    "            h = self.encoder(h, embedding_step_t)\n",
    "        c = h\n",
    "\n",
    "        # DECODER\n",
    "        loss = 0.0\n",
    "        output = []\n",
    "        y_pred = Y_ctoi['<'] # begin with \"start of sequence\" char\n",
    "        output.append(y_pred)\n",
    "        h = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        MAX = 20 # for safety\n",
    "        while y_pred!=Y_ctoi['>'] and len(output)<=MAX:\n",
    "            embedding_step_t = self.emby(y_pred)\n",
    "            h = self.decoder(h, c, embedding_step_t)\n",
    "            o = self.lin(h)\n",
    "            p = F.softmax(o, dim=1)\n",
    "            y_pred = torch.argmax(p, dim=1).item()\n",
    "            output.append(y_pred)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/eng-fra.txt\") as f:\n",
    "    text = f.read().strip().lower()\n",
    "\n",
    "# clean up, normalize\n",
    "text = re.sub(r\"[ \\u202f\\u209f\\u20bf\\u2009\\u3000\\xa0]+\", \" \", text)  # there are lots of space chars in unicode\n",
    "text = re.sub(r\"\\u200b|\\xad|‐|–\", \"-\", text)  # there are lots of space chars in unicode\n",
    "text = re.sub(r\"‘|’\", \"'\", text)  # there are lots of space chars in unicode\n",
    "text = text.replace(\"‽\", \"?\")\n",
    "text = text.replace(\"…\", \"\")\n",
    "text = text.replace(\"₂\", \"\")\n",
    "# text = text.replace(\"\\u202f\", \" \")\n",
    "# text = text.replace(\"\\u209f\", \" \")\n",
    "# text = text.replace(\"\\u20bf\", \" \")\n",
    "text = text.replace(\" !\", \"\")\n",
    "text = text.replace(\" .\", \"\")\n",
    "text = re.sub(r\"([.!?])\", \"\", text)\n",
    "lines = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line for line in lines if not len(set(line).intersection({'(',')','~','€','$','%','&','/','«','»'}))]\n",
    "pairs = [line.split('\\t') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "pairs = [p for p in pairs if len(p[0])<=MAX_LENGTH and len(p[1])<=MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = False\n",
    "if FILTER:\n",
    "    eng_prefixes = (\n",
    "        \"i am \", \"i'm \",\n",
    "        \"he is \", \"he's \",\n",
    "        \"she is \", \"she's \",\n",
    "        \"you are \", \"you're \",\n",
    "        \"we are \", \"we're \",\n",
    "        \"they are \", \"they're \"\n",
    "        )\n",
    "    filtered_pairs = []\n",
    "    for p in pairs:\n",
    "        en,fr = p\n",
    "        for pre in eng_prefixes:\n",
    "            if en.startswith(pre):\n",
    "                filtered_pairs.append(p)\n",
    "                break\n",
    "\n",
    "    pairs = filtered_pairs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[0:100] # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(p[1],p[0]) for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "pairs = list(dict(pairs).items())\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set('\\n'.join(lines)))\n",
    "vocab = vocab[2:] # drop \\t and \\n\n",
    "vocab = ['<','>']+vocab # add delimiters as 0, 1\n",
    "ctoi = {c:i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<> \"\\'+,-0123456789:;abcdefghijklmnopqrstuvwxyzàâçèéêëîïòôöùúûœас'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('va', 'go'),\n",
       " ('cours', 'run'),\n",
       " ('courez', 'run'),\n",
       " ('ça alors', 'wow'),\n",
       " ('au feu', 'fire'),\n",
       " (\"à l'aide\", 'help'),\n",
       " ('saute', 'jump'),\n",
       " ('ça suffit', 'stop'),\n",
       " ('stop', 'stop'),\n",
       " ('arrête-toi', 'stop')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap in <...> and Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(f\"<{p[0]}>\",f\"<{p[1]}>\") for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<va>', '<go>'),\n",
       " ('<cours>', '<run>'),\n",
       " ('<courez>', '<run>'),\n",
       " ('<ça alors>', '<wow>'),\n",
       " ('<au feu>', '<fire>')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for p in pairs:\n",
    "    fr, en = p\n",
    "    X.append([ctoi[c] for c in fr])\n",
    "    Y.append([ctoi[c] for c in en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 41, 20, 1],\n",
       " [0, 22, 34, 40, 37, 38, 1],\n",
       " [0, 22, 34, 40, 37, 24, 45, 1],\n",
       " [0, 48, 20, 2, 20, 31, 34, 37, 38, 1],\n",
       " [0, 20, 40, 2, 25, 24, 40, 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 26, 34, 1],\n",
       " [0, 37, 40, 33, 1],\n",
       " [0, 37, 40, 33, 1],\n",
       " [0, 42, 34, 42, 1],\n",
       " [0, 25, 28, 37, 24, 1]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 training records, 10 embedding size, 64 target classes, state is 300-vector\n"
     ]
    }
   ],
   "source": [
    "n = len(X_train)\n",
    "char_embed_sz = 10\n",
    "nhidden = 300\n",
    "nclasses = len(vocab) # char output vocab\n",
    "\n",
    "print(f\"{n:,d} training records, {char_embed_sz} embedding size, {nclasses} target classes, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tostr(x):\n",
    "    return ''.join([vocab[v] for v in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss   23.443   accur  0.0019   LR 0.000001\n",
      "Epoch   2 training loss   18.332   accur  0.0194   LR 0.000126\n",
      "Epoch   3 training loss    9.734   accur  0.1609   LR 0.000251\n",
      "Epoch   4 training loss    4.885   accur  0.4167   LR 0.000375\n",
      "Epoch   5 training loss    2.420   accur  0.5988   LR 0.000500\n",
      "Epoch   6 training loss    1.211   accur  0.7539   LR 0.000375\n",
      "Epoch   7 training loss    0.728   accur  0.8430   LR 0.000251\n",
      "Epoch   8 training loss    0.499   accur  0.8973   LR 0.000126\n",
      "Epoch   9 training loss    0.409   accur  0.9167   LR 0.000001\n",
      "Epoch  10 training loss    0.410   accur  0.9205   LR 0.000063\n"
     ]
    }
   ],
   "source": [
    "trans = Transducer(input_sz=len(ctoi),\n",
    "                   output_sz=len(ctoi),\n",
    "                   input_embed_sz=char_embed_sz,\n",
    "                   output_embed_sz=char_embed_sz,\n",
    "                   nhidden=nhidden,\n",
    "                   dropout=0.0,\n",
    "                   useGRU=True)\n",
    "optimizer = torch.optim.Adam(trans.parameters(), lr=0.0005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.000001, max_lr=0.0005,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for i in range(n):\n",
    "        x = X_train[i]\n",
    "        y = Y_train[i]\n",
    "        y_pred, loss, correct = trans(x, y)\n",
    "#         if epoch==10:\n",
    "#             print(f\"{tostr(x)}->{tostr(y)}: {tostr(y_pred)}, {correct} correct\")\n",
    "        epoch_training_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += len(y) - 1  # From \"<foo>\" predict and count \"foo>\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_training_accur /= total_compares\n",
    "    epoch_training_loss /= total_compares\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")\n",
    "    scheduler.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def same(a,b):\n",
    "#     return sum(c1==c2 for c1,c2 in zip(a,b))\n",
    "\n",
    "def check(X,Y,verbose=False):\n",
    "    \"Use Levenshtein to measure how close output predictions are to truth.\"\n",
    "    with torch.no_grad():\n",
    "        valid_accur = 0\n",
    "        total_compares = 0\n",
    "        total_correct = 0\n",
    "        total_d = 0\n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            y = Y[i]\n",
    "            y_pred = trans.predict(x, ctoi)\n",
    "            total_compares += len(y) - 1 # From \"<foo>\" predict \"foo>\" but don't count last '>' for metrics\n",
    "            total_correct += tostr(y)==tostr(y_pred)\n",
    "            d = editdistance.eval(tostr(y),tostr(y_pred))\n",
    "            total_d += d\n",
    "            if verbose:\n",
    "                print(f\"{tostr(x):20s} : {tostr(y)}\")\n",
    "                print(f\"{'':20s} : {tostr(y_pred):20s} Levenshtein {d} out of {len(y)}\")\n",
    "    return total_d, total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average Levenshtein score     2.21, perfect accuracy     0.61\n"
     ]
    }
   ],
   "source": [
    "total_d, total_correct = check(X_train, Y_train)\n",
    "print(f\"Training average Levenshtein score {total_d/len(X_train):8.2f}, perfect accuracy {total_correct/len(X_train):8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stop>               : <stop>\n",
      "                     : <-4p>                Levenshtein 3 out of 6\n",
      "<va>                 : <go>\n",
      "                     : <isyr5555555p55555555 Levenshtein 20 out of 4\n",
      "<soyez juste>        : <be fair>\n",
      "                     : <be fair>            Levenshtein 0 out of 9\n",
      "<compris >           : <got it>\n",
      "                     : <got it>             Levenshtein 0 out of 8\n",
      "<entrez>             : <come in>\n",
      "                     : <come on>            Levenshtein 1 out of 9\n",
      "<impossible>         : <no way>\n",
      "                     : <be fair>            Levenshtein 5 out of 8\n",
      "<je suis tombée>     : <i fell>\n",
      "                     : <be fair>            Levenshtein 5 out of 8\n",
      "<on essaye>          : <we try>\n",
      "                     : <i tif,>             Levenshtein 5 out of 8\n",
      "<sois détendu>       : <be cool>\n",
      "                     : <listyсrœgggiggiggigg Levenshtein 20 out of 9\n",
      "<laissez tomber>     : <drop it>\n",
      "                     : <ruœgggo  çt;p>      Levenshtein 10 out of 9\n",
      "<venez>              : <come on>\n",
      "                     : <hop in>             Levenshtein 4 out of 9\n",
      "<soyez gentille>     : <be nice>\n",
      "                     : <be nice>            Levenshtein 0 out of 9\n",
      "<je vais bien>       : <i'm ok>\n",
      "                     : <go away>            Levenshtein 7 out of 8\n",
      "<sois gentil>        : <be nice>\n",
      "                     : <be nice>            Levenshtein 0 out of 9\n",
      "<arrête-toi>         : <stop>\n",
      "                     : <get up>             Levenshtein 4 out of 6\n",
      "<je suis tombé>      : <i fell>\n",
      "                     : <got it>             Levenshtein 6 out of 8\n",
      "<courez>             : <run>\n",
      "                     : <gotop>              Levenshtein 5 out of 5\n",
      "<je sais>            : <i know>\n",
      "                     : <go away>            Levenshtein 6 out of 8\n",
      "Testing average Levenshtein score     5.61, perfect accuracy     0.22\n"
     ]
    }
   ],
   "source": [
    "total_d, total_correct = check(X_test, Y_test, verbose=True)\n",
    "print(f\"Testing average Levenshtein score {total_d/len(X_test):8.2f}, perfect accuracy {total_correct/len(X_test):8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
