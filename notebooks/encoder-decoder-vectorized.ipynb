{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Encoder-decoder vectorized\n",
    "\n",
    "Use [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) human numbers data to train translator from english like \"two hundred seven\" to sequence of digits like \"207\". Data looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```\n",
    "\n",
    "This is vectorized version of [previous](encoder-decoder.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/parrt/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one \n",
      "two \n",
      "three \n",
      "four \n",
      "five \n",
      "['one ', 'two ', 'three ', 'four ', 'five ']\n"
     ]
    }
   ],
   "source": [
    "text = get_text(path/'train.txt').strip()\n",
    "print(text[:28])\n",
    "lines = text.lower().split('\\n')\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = lines[0:500] # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "# use '#' to indicate padded (unused) char for embedding purposes\n",
    "v = set('#')\n",
    "X_vocab = ['#']  # position 0 means pad symbol\n",
    "for t in text.split():\n",
    "    if t not in v:\n",
    "        X_vocab.append(t)\n",
    "        v.add(t)\n",
    "X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nineteen'],\n",
       " ['twenty'],\n",
       " ['twenty', 'one'],\n",
       " ['twenty', 'two'],\n",
       " ['twenty', 'three']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens = [line.strip().split(' ') for line in lines]\n",
    "X_tokens[18:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 7999)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tokens), len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 'one', 'eleven')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X_tokens)\n",
    "batch_size = 32\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_tokens = X_tokens[:n]\n",
    "\n",
    "X_vocab = {w:i for i,w in enumerate(X_vocab)}\n",
    "X_idx = {i:w for i,w in enumerate(X_vocab)}\n",
    "X_vocab['one'], X_vocab['eleven'], X_idx[1], X_idx[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7968, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0, 20,  6],\n",
       "        [ 0,  0,  0,  0, 20,  7],\n",
       "        [ 0,  0,  0,  0, 20,  8],\n",
       "        [ 0,  0,  0,  0, 20,  9],\n",
       "        [ 0,  0,  0,  0,  0, 21],\n",
       "        [ 0,  0,  0,  0, 21,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numericalize and left pad\n",
    "X_max_len = get_max_len(X_tokens)\n",
    "X = torch.zeros(len(X_tokens), X_max_len, device=device, dtype=torch.long) # zero implies padding\n",
    "print(X.shape)\n",
    "for i in range(len(X_tokens)):\n",
    "    x = X_tokens[i]\n",
    "    pad = X_max_len - len(x)\n",
    "    for j in range(len(x)):\n",
    "        X[i,j+pad] = X_vocab[X_tokens[i][j]]\n",
    "X[25:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define y sequence of digits\n",
    "\n",
    "Let's use Y as list of lists like X; targets like `'one' -> '1'`, `['twenty', 'three'] -> ['2','3']`, etc...\n",
    "\n",
    "Use '<' for start of sequence and '>' for end. So sequence `ab` is stored `<ab>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '<': 10,\n",
       " '>': 11}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_vocab = {d:i for i,d in enumerate(\"0123456789<>\")}\n",
    "Y_idx = {i:w for i,w in enumerate(\"0123456789<>\")}\n",
    "Y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1>', '<2>', '<3>', '<4>', '<5>', '<6>', '<7>', '<8>', '<9>', '<10>', '<11>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ystr = [f\"<{i+1}>\" for i in range(0,len(X))]\n",
    "Y_max_len = get_max_len(Ystr)\n",
    "Ystr[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  2,  0, 11, 11, 11],\n",
       "        [10,  2,  1, 11, 11, 11],\n",
       "        [10,  2,  2, 11, 11, 11],\n",
       "        [10,  2,  3, 11, 11, 11],\n",
       "        [10,  2,  4, 11, 11, 11],\n",
       "        [10,  2,  5, 11, 11, 11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for i in range(0,len(X)):\n",
    "    y = Ystr[i]\n",
    "    pad = Y_max_len - len(y)\n",
    "    Y.append([Y_vocab[d] for d in y]+[Y_vocab['>']]*pad)  # pad with \"end of string\" symbols '>'\n",
    "Y = torch.tensor(Y)\n",
    "Y[19:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1, 11, 11, 11, 11],\n",
       "        [10,  2, 11, 11, 11, 11],\n",
       "        [10,  3, 11, 11, 11, 11],\n",
       "        [10,  4, 11, 11, 11, 11],\n",
       "        [10,  5, 11, 11, 11, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1,  3,  1, 11, 11],\n",
       "        [10,  1,  3,  2, 11, 11],\n",
       "        [10,  1,  3,  3, 11, 11],\n",
       "        [10,  1,  3,  4, 11, 11],\n",
       "        [10,  1,  3,  5, 11, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[130:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,968 training records, 30 X symbols, batch size 32, 12 target classes, h state is 200-vector\n"
     ]
    }
   ],
   "source": [
    "embed_sz = 20\n",
    "y_embed_sz = 5\n",
    "nhidden = 200\n",
    "nclasses = len(Y_vocab) # char output vocab\n",
    "\n",
    "print(f\"{n:,d} training records, {len(X_vocab)} X symbols, batch size {batch_size}, {nclasses} target classes, h state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch_X, X_max_len, batch_Y, Y_max_len):\n",
    "    # ENCODER\n",
    "    H = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    for t in range(X_max_len):\n",
    "        x_step_t = batch_X[:,t]\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = Ex[:,x_step_t]\n",
    "        H = W@H + U@embedding_step_t + bx\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "    # DECODER\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "#     print(\"DECODE\", batch_Y)\n",
    "    for t in range(Y_max_len-1): # don't predict next char at final '>'\n",
    "        embedding_step_t = Ey[:,batch_Y[:,t]]\n",
    "#         print(\"H, W2, U2, Ey, embedding_step_t, By\")\n",
    "#         print(H.shape, W2.shape, U2.shape, Ey.shape, embedding_step_t.shape, By.shape)\n",
    "        H = W2 @ H + U2 @ embedding_step_t + by\n",
    "        H = torch.tanh(H)\n",
    "        o = V @ H + bo\n",
    "        o = o.T # reshape to be batch_size x nclasses\n",
    "#         print(\"O\",o.shape)\n",
    "#         o = o.reshape(batch_size,nclasses)\n",
    "        # From y we want to predict y[1:]. at y[t], predict y[t+1]\n",
    "        loss += F.cross_entropy(o, torch.tensor(batch_Y[:,t+1], device=device))\n",
    "\n",
    "        p = softmax(o)\n",
    "#         print(torch.argmax(p, dim=1), torch.tensor(batch_Y[:,t+1], device=device))\n",
    "        c = torch.sum(torch.argmax(p, dim=1)==torch.tensor(batch_Y[:,t+1], device=device))\n",
    "#         print(c.item())\n",
    "        correct += c.item()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 63.5504 accur  0.2970   LR 0.001625\n",
      "Epoch   2 training loss 41.9993 accur  0.3891   LR 0.002750\n",
      "Epoch   3 training loss 35.0720 accur  0.4332   LR 0.003875\n",
      "Epoch   4 training loss 30.8525 accur  0.4569   LR 0.005000\n",
      "Epoch   5 training loss 28.2849 accur  0.4651   LR 0.003875\n",
      "Epoch   6 training loss 25.0557 accur  0.4730   LR 0.002750\n",
      "Epoch   7 training loss 21.3112 accur  0.4823   LR 0.001625\n",
      "Epoch   8 training loss 17.5209 accur  0.5156   LR 0.000500\n",
      "Epoch   9 training loss 16.5303 accur  0.5329   LR 0.001063\n",
      "Epoch  10 training loss 13.2333 accur  0.5778   LR 0.001625\n",
      "Epoch  11 training loss 12.3623 accur  0.5927   LR 0.002188\n",
      "Epoch  12 training loss 11.6250 accur  0.6112   LR 0.002750\n",
      "Epoch  13 training loss 11.5823 accur  0.6164   LR 0.002188\n",
      "Epoch  14 training loss 11.1372 accur  0.6279   LR 0.001625\n",
      "Epoch  15 training loss 10.4598 accur  0.6497   LR 0.001063\n",
      "Epoch  16 training loss  9.1295 accur  0.6922   LR 0.000500\n",
      "Epoch  17 training loss  8.1999 accur  0.7235   LR 0.000781\n",
      "Epoch  18 training loss  7.4263 accur  0.7454   LR 0.001063\n",
      "Epoch  19 training loss  7.0716 accur  0.7548   LR 0.001344\n",
      "Epoch  20 training loss  6.8372 accur  0.7655   LR 0.001625\n",
      "Epoch  21 training loss  6.5966 accur  0.7663   LR 0.001344\n",
      "Epoch  22 training loss  6.2586 accur  0.7802   LR 0.001063\n",
      "Epoch  23 training loss  5.8719 accur  0.7970   LR 0.000781\n",
      "Epoch  24 training loss  5.3910 accur  0.8139   LR 0.000500\n",
      "Epoch  25 training loss  5.0737 accur  0.8302   LR 0.000641\n",
      "Epoch  26 training loss  4.4649 accur  0.8510   LR 0.000781\n",
      "Epoch  27 training loss  4.0111 accur  0.8659   LR 0.000922\n",
      "Epoch  28 training loss  3.6445 accur  0.8761   LR 0.001063\n",
      "Epoch  29 training loss  3.4287 accur  0.8856   LR 0.000922\n",
      "Epoch  30 training loss  3.2956 accur  0.8883   LR 0.000781\n",
      "Epoch  31 training loss  3.1075 accur  0.8953   LR 0.000641\n",
      "Epoch  32 training loss  2.6880 accur  0.9096   LR 0.000500\n",
      "Epoch  33 training loss  2.3327 accur  0.9257   LR 0.000570\n",
      "Epoch  34 training loss  2.0221 accur  0.9369   LR 0.000641\n",
      "Epoch  35 training loss  1.8068 accur  0.9458   LR 0.000711\n",
      "Epoch  36 training loss  1.5676 accur  0.9552   LR 0.000781\n",
      "Epoch  37 training loss  1.3686 accur  0.9594   LR 0.000711\n",
      "Epoch  38 training loss  1.1868 accur  0.9664   LR 0.000641\n",
      "Epoch  39 training loss  1.0118 accur  0.9725   LR 0.000570\n",
      "Epoch  40 training loss  0.8136 accur  0.9809   LR 0.000500\n",
      "Epoch  41 training loss  0.6869 accur  0.9848   LR 0.000535\n",
      "Epoch  42 training loss  0.5736 accur  0.9891   LR 0.000570\n",
      "Epoch  43 training loss  0.4750 accur  0.9916   LR 0.000605\n",
      "Epoch  44 training loss  0.3913 accur  0.9941   LR 0.000641\n",
      "Epoch  45 training loss  0.3433 accur  0.9951   LR 0.000605\n",
      "Epoch  46 training loss  0.3099 accur  0.9954   LR 0.000570\n",
      "Epoch  47 training loss  0.2644 accur  0.9965   LR 0.000535\n",
      "Epoch  48 training loss  0.2325 accur  0.9973   LR 0.000500\n",
      "Epoch  49 training loss  0.2084 accur  0.9971   LR 0.000518\n",
      "Epoch  50 training loss  0.1574 accur  0.9989   LR 0.000535\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "Ex = torch.randn(embed_sz,     len(X_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       embed_sz,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "bo = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "Ey = torch.randn(y_embed_sz,   len(Y_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W2 = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U2 = torch.randn(nhidden,      y_embed_sz,    device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V], lr=0.001, weight_decay=0.0)\n",
    "optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V,bx,by,bo], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.0005, max_lr=0.005,\n",
    "                                              cycle_momentum=False)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "history = []\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X[p:p+batch_size]\n",
    "        batch_Y = Y[p:p+batch_size]\n",
    "        loss, correct = forward(batch_X, X_max_len, batch_Y, Y_max_len)\n",
    "        epoch_training_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += batch_size * (Y_max_len - 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= total_compares\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    n = len(x)\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(x)):\n",
    "            embedding_step_t = Ex[:,x[t]]\n",
    "            embedding_step_t = embedding_step_t.reshape(embed_sz,1)\n",
    "            h = W @ h + U @ embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "\n",
    "        # DECODER\n",
    "        y = [Y_vocab['<']] # begin with \"start of sequence\" char\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        while y!=Y_vocab['>']:\n",
    "            embedding_step_t = Ey[:,y]\n",
    "            embedding_step_t = embedding_step_t.reshape(y_embed_sz,1)\n",
    "            h = W2 @ h + U2 @ embedding_step_t + by\n",
    "            h = torch.tanh(h)\n",
    "            o = V @ h + bo\n",
    "            o = o.reshape(1,nclasses)\n",
    "            p = softmax(o[0])\n",
    "            y = torch.argmax(p).item()\n",
    "            if y!=Y_vocab['>']:\n",
    "                output.append(Y_idx[y])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one'] => ['1', '0', '5', '1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred'] => ['6', '1', '7', '0']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'ten'] => ['1', '1', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred ten\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'thirty', 'two'] => ['1', '1', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred thirty two\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eleven'] => ['6', '0', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"eleven\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ninety', 'nine'] => ['2', '0', '9', '9']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"ninety nine\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fifty', 'three'] => ['5', '5', '5', '3']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"fifty three\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
