{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Encoder-decoder vectorized\n",
    "\n",
    "Use [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) human numbers data to train translator from english like \"two hundred seven\" to sequence of digits like \"207\". Data looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```\n",
    "\n",
    "This is vectorized version of [previous](encoder-decoder.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/parrt/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one \n",
      "two \n",
      "three \n",
      "four \n",
      "five \n",
      "['one ', 'two ', 'three ', 'four ', 'five ']\n"
     ]
    }
   ],
   "source": [
    "text = get_text(path/'train.txt').strip()\n",
    "print(text[:28])\n",
    "lines = text.lower().split('\\n')\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[0:2000] # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "# use '#' to indicate padded (unused) char for embedding purposes\n",
    "v = set('#')\n",
    "X_vocab = ['#']  # position 0 means pad symbol\n",
    "for t in text.split():\n",
    "    if t not in v:\n",
    "        X_vocab.append(t)\n",
    "        v.add(t)\n",
    "X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nineteen'],\n",
       " ['twenty'],\n",
       " ['twenty', 'one'],\n",
       " ['twenty', 'two'],\n",
       " ['twenty', 'three']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens = [line.strip().split(' ') for line in lines]\n",
    "X_tokens[18:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tokens), len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 'one', 'eleven')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X_tokens)\n",
    "batch_size = 64\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_tokens = X_tokens[:n]\n",
    "\n",
    "X_vocab = {w:i for i,w in enumerate(X_vocab)}\n",
    "X_idx = {i:w for i,w in enumerate(X_vocab)}\n",
    "X_vocab['one'], X_vocab['eleven'], X_idx[1], X_idx[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1984, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0, 20,  6],\n",
       "        [ 0,  0,  0,  0, 20,  7],\n",
       "        [ 0,  0,  0,  0, 20,  8],\n",
       "        [ 0,  0,  0,  0, 20,  9],\n",
       "        [ 0,  0,  0,  0,  0, 21],\n",
       "        [ 0,  0,  0,  0, 21,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numericalize and left pad\n",
    "X_max_len = get_max_len(X_tokens)\n",
    "X = torch.zeros(len(X_tokens), X_max_len, device=device, dtype=torch.long) # zero implies padding\n",
    "print(X.shape)\n",
    "for i in range(len(X_tokens)):\n",
    "    x = X_tokens[i]\n",
    "    pad = X_max_len - len(x)\n",
    "    for j in range(len(x)):\n",
    "        X[i,j+pad] = X_vocab[X_tokens[i][j]]\n",
    "X[25:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define y sequence of digits\n",
    "\n",
    "Let's use Y as list of lists like X; targets like `'one' -> '1'`, `['twenty', 'three'] -> ['2','3']`, etc...\n",
    "\n",
    "Use '<' for start of sequence and '>' for end. So sequence `ab` is stored `<ab>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '<': 10,\n",
       " '>': 11}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_vocab = {d:i for i,d in enumerate(\"0123456789<>\")}\n",
    "Y_idx = {i:w for i,w in enumerate(\"0123456789<>\")}\n",
    "Y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1>', '<2>', '<3>', '<4>', '<5>', '<6>', '<7>', '<8>', '<9>', '<10>', '<11>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ystr = [f\"<{i+1}>\" for i in range(0,len(X))]\n",
    "Y_max_len = get_max_len(Ystr)\n",
    "Ystr[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  2,  0, 11, 11, 11],\n",
       "        [10,  2,  1, 11, 11, 11],\n",
       "        [10,  2,  2, 11, 11, 11],\n",
       "        [10,  2,  3, 11, 11, 11],\n",
       "        [10,  2,  4, 11, 11, 11],\n",
       "        [10,  2,  5, 11, 11, 11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for i in range(0,len(X)):\n",
    "    y = Ystr[i]\n",
    "    pad = Y_max_len - len(y)\n",
    "    Y.append([Y_vocab[d] for d in y]+[Y_vocab['>']]*pad)  # pad with \"end of string\" symbols '>'\n",
    "Y = torch.tensor(Y)\n",
    "Y[19:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1, 11, 11, 11, 11],\n",
       "        [10,  2, 11, 11, 11, 11],\n",
       "        [10,  3, 11, 11, 11, 11],\n",
       "        [10,  4, 11, 11, 11, 11],\n",
       "        [10,  5, 11, 11, 11, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1,  3,  1, 11, 11],\n",
       "        [10,  1,  3,  2, 11, 11],\n",
       "        [10,  1,  3,  3, 11, 11],\n",
       "        [10,  1,  3,  4, 11, 11],\n",
       "        [10,  1,  3,  5, 11, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[130:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,984 training records, 30 X symbols, batch size 64, 12 target classes, h state is 400-vector\n"
     ]
    }
   ],
   "source": [
    "embed_sz = 20\n",
    "y_embed_sz = 8\n",
    "nhidden = 512\n",
    "nclasses = len(Y_vocab) # char output vocab\n",
    "\n",
    "print(f\"{n:,d} training records, {len(X_vocab)} X symbols, batch size {batch_size}, {nclasses} target classes, h state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch_X, X_max_len, batch_Y, Y_max_len):\n",
    "    # ENCODER\n",
    "    H = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    for t in range(X_max_len):\n",
    "        x_step_t = batch_X[:,t]\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = Ex[:,x_step_t]\n",
    "        H = W@H + U@embedding_step_t + bx\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "    # DECODER\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "#     print(\"DECODE\", batch_Y)\n",
    "    for t in range(Y_max_len-1): # don't predict next char at final '>'\n",
    "        embedding_step_t = Ey[:,batch_Y[:,t]]\n",
    "#         print(\"H, W2, U2, Ey, embedding_step_t, By\")\n",
    "#         print(H.shape, W2.shape, U2.shape, Ey.shape, embedding_step_t.shape, By.shape)\n",
    "        H = W2 @ H + U2 @ embedding_step_t + by\n",
    "        H = torch.tanh(H)\n",
    "        o = V @ H + bo\n",
    "        o = o.T # reshape to be batch_size x nclasses\n",
    "#         print(\"O\",o.shape)\n",
    "#         o = o.reshape(batch_size,nclasses)\n",
    "        # From y we want to predict y[1:]. at y[t], predict y[t+1]\n",
    "        loss += F.cross_entropy(o, torch.tensor(batch_Y[:,t+1], device=device))\n",
    "\n",
    "        p = softmax(o)\n",
    "#         print(torch.argmax(p, dim=1), torch.tensor(batch_Y[:,t+1], device=device))\n",
    "        c = torch.sum(torch.argmax(p, dim=1)==torch.tensor(batch_Y[:,t+1], device=device))\n",
    "#         print(c.item())\n",
    "        correct += c.item()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 144.6496 accur  0.2526   LR 0.000475\n",
      "Epoch   2 training loss 88.1870 accur  0.3407   LR 0.000650\n",
      "Epoch   3 training loss 68.8263 accur  0.3605   LR 0.000825\n",
      "Epoch   4 training loss 53.2260 accur  0.3961   LR 0.001000\n",
      "Epoch   5 training loss 50.9873 accur  0.4537   LR 0.000825\n",
      "Epoch   6 training loss 49.2652 accur  0.4836   LR 0.000650\n",
      "Epoch   7 training loss 51.2966 accur  0.4935   LR 0.000475\n",
      "Epoch   8 training loss 50.7478 accur  0.5006   LR 0.000300\n",
      "Epoch   9 training loss 28.4984 accur  0.5533   LR 0.000387\n",
      "Epoch  10 training loss 16.0806 accur  0.6409   LR 0.000475\n",
      "Epoch  11 training loss 14.2301 accur  0.6696   LR 0.000563\n",
      "Epoch  12 training loss 12.7836 accur  0.6830   LR 0.000650\n",
      "Epoch  13 training loss 11.5011 accur  0.7051   LR 0.000563\n",
      "Epoch  14 training loss 13.1394 accur  0.6979   LR 0.000475\n",
      "Epoch  15 training loss 14.1991 accur  0.6897   LR 0.000387\n",
      "Epoch  16 training loss 10.8243 accur  0.7328   LR 0.000300\n",
      "Epoch  17 training loss  9.0964 accur  0.7606   LR 0.000344\n",
      "Epoch  18 training loss  8.2340 accur  0.7855   LR 0.000387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e3c5c0e84edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# autograd computes U.grad, M.grad, ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "Ex = torch.randn(embed_sz,     len(X_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       embed_sz,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "bo = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "Ey = torch.randn(y_embed_sz,   len(Y_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W2 = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U2 = torch.randn(nhidden,      y_embed_sz,    device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V], lr=0.001, weight_decay=0.0)\n",
    "optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V,bx,by,bo], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.0003, max_lr=0.001,\n",
    "                                              cycle_momentum=False)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "history = []\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X[p:p+batch_size]\n",
    "        batch_Y = Y[p:p+batch_size]\n",
    "        loss, correct = forward(batch_X, X_max_len, batch_Y, Y_max_len)\n",
    "        epoch_training_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += batch_size * (Y_max_len - 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= total_compares\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    n = len(x)\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(x)):\n",
    "            embedding_step_t = Ex[:,x[t]]\n",
    "            embedding_step_t = embedding_step_t.reshape(embed_sz,1)\n",
    "            h = W @ h + U @ embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "\n",
    "        # DECODER\n",
    "        y = [Y_vocab['<']] # begin with \"start of sequence\" char\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        while y!=Y_vocab['>']:\n",
    "            embedding_step_t = Ey[:,y]\n",
    "            embedding_step_t = embedding_step_t.reshape(y_embed_sz,1)\n",
    "            h = W2 @ h + U2 @ embedding_step_t + by\n",
    "            h = torch.tanh(h)\n",
    "            o = V @ h + bo\n",
    "            o = o.reshape(1,nclasses)\n",
    "            p = softmax(o[0])\n",
    "            y = torch.argmax(p).item()\n",
    "            if y!=Y_vocab['>']:\n",
    "                output.append(Y_idx[y])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"one\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred ten\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred thirty two\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"eleven\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"ninety nine\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [X_vocab[w] for w in \"fifty three\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
