{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Encoder-decoder vectorized\n",
    "\n",
    "Use [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) human numbers data to train translator from english like \"two hundred seven\" to sequence of digits like \"207\". Data looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```\n",
    "\n",
    "This is vectorized version of [previous](encoder-decoder.ipynb).\n",
    "\n",
    "Wow. much more accurate to use encoder h as context vector rather than init value for decoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/parrt/.fastai/data/human_numbers')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one \n",
      "two \n",
      "three \n",
      "four \n",
      "five \n",
      "['one ', 'two ', 'three ', 'four ', 'five ']\n"
     ]
    }
   ],
   "source": [
    "text = get_text(path/'train.txt').strip()\n",
    "print(text[:28])\n",
    "lines = text.lower().split('\\n')\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[0:2000] # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "# use '#' to indicate padded (unused) char for embedding purposes\n",
    "v = set('#')\n",
    "X_vocab = ['#']  # position 0 means pad symbol\n",
    "for t in text.split():\n",
    "    if t not in v:\n",
    "        X_vocab.append(t)\n",
    "        v.add(t)\n",
    "X_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nineteen'],\n",
       " ['twenty'],\n",
       " ['twenty', 'one'],\n",
       " ['twenty', 'two'],\n",
       " ['twenty', 'three']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokens = [line.strip().split(' ') for line in lines]\n",
    "X_tokens[18:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tokens), len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 'one', 'eleven')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(X_tokens)\n",
    "batch_size = 64\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_tokens = X_tokens[:n]\n",
    "\n",
    "X_vocab = {w:i for i,w in enumerate(X_vocab)}\n",
    "X_idx = {i:w for i,w in enumerate(X_vocab)}\n",
    "X_vocab['one'], X_vocab['eleven'], X_idx[1], X_idx[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1984, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0, 20,  6],\n",
       "        [ 0,  0,  0,  0, 20,  7],\n",
       "        [ 0,  0,  0,  0, 20,  8],\n",
       "        [ 0,  0,  0,  0, 20,  9],\n",
       "        [ 0,  0,  0,  0,  0, 21],\n",
       "        [ 0,  0,  0,  0, 21,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numericalize and left pad\n",
    "X_max_len = get_max_len(X_tokens)\n",
    "X = torch.zeros(len(X_tokens), X_max_len, device=device, dtype=torch.long) # zero implies padding\n",
    "print(X.shape)\n",
    "for i in range(len(X_tokens)):\n",
    "    x = X_tokens[i]\n",
    "    pad = X_max_len - len(x)\n",
    "    for j in range(len(x)):\n",
    "        X[i,j+pad] = X_vocab[X_tokens[i][j]]\n",
    "X[25:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define y sequence of digits\n",
    "\n",
    "Let's use Y as list of lists like X; targets like `'one' -> '1'`, `['twenty', 'three'] -> ['2','3']`, etc...\n",
    "\n",
    "Use '<' for start of sequence and '>' for end. So sequence `ab` is stored `<ab>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '<': 10,\n",
       " '>': 11}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_vocab = {d:i for i,d in enumerate(\"0123456789<>\")}\n",
    "Y_idx = {i:w for i,w in enumerate(\"0123456789<>\")}\n",
    "Y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1>', '<2>', '<3>', '<4>', '<5>', '<6>', '<7>', '<8>', '<9>', '<10>', '<11>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ystr = [f\"<{i+1}>\" for i in range(0,len(X))]\n",
    "Y_max_len = get_max_len(Ystr)\n",
    "Ystr[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  2,  0, 11, 11, 11],\n",
       "        [10,  2,  1, 11, 11, 11],\n",
       "        [10,  2,  2, 11, 11, 11],\n",
       "        [10,  2,  3, 11, 11, 11],\n",
       "        [10,  2,  4, 11, 11, 11],\n",
       "        [10,  2,  5, 11, 11, 11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = []\n",
    "for i in range(0,len(X)):\n",
    "    y = Ystr[i]\n",
    "    pad = Y_max_len - len(y)\n",
    "    Y.append([Y_vocab[d] for d in y]+[Y_vocab['>']]*pad)  # pad with \"end of string\" symbols '>'\n",
    "Y = torch.tensor(Y)\n",
    "Y[19:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1, 11, 11, 11, 11],\n",
       "        [10,  2, 11, 11, 11, 11],\n",
       "        [10,  3, 11, 11, 11, 11],\n",
       "        [10,  4, 11, 11, 11, 11],\n",
       "        [10,  5, 11, 11, 11, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1,  3,  1, 11, 11],\n",
       "        [10,  1,  3,  2, 11, 11],\n",
       "        [10,  1,  3,  3, 11, 11],\n",
       "        [10,  1,  3,  4, 11, 11],\n",
       "        [10,  1,  3,  5, 11, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[130:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,984 training records, 30 X symbols, batch size 64, 12 target classes, h state is 512-vector\n"
     ]
    }
   ],
   "source": [
    "embed_sz = 20\n",
    "y_embed_sz = 8\n",
    "nhidden = 512\n",
    "nclasses = len(Y_vocab) # char output vocab\n",
    "\n",
    "print(f\"{n:,d} training records, {len(X_vocab)} X symbols, batch size {batch_size}, {nclasses} target classes, h state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch_X, X_max_len, batch_Y, Y_max_len):\n",
    "    # ENCODER\n",
    "    H = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    for t in range(X_max_len):\n",
    "        x_step_t = batch_X[:,t]\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = Ex[:,x_step_t]\n",
    "        H = W@H + U@embedding_step_t + bx\n",
    "        H = torch.tanh(H)\n",
    "    C = H\n",
    "    \n",
    "    # DECODER\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "#     print(\"DECODE\", batch_Y)\n",
    "    H = torch.zeros(nhidden, batch_size, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    for t in range(Y_max_len-1): # don't predict next char at final '>'\n",
    "        embedding_step_t = Ey[:,batch_Y[:,t]]\n",
    "#         print(\"H, W2, U2, Ey, embedding_step_t, By\")\n",
    "#         print(H.shape, W2.shape, U2.shape, Ey.shape, embedding_step_t.shape, By.shape)\n",
    "        H = W2 @ H + Cx@C + U2 @ embedding_step_t + by\n",
    "        H = torch.tanh(H)\n",
    "        o = V @ H + bo\n",
    "        o = o.T # reshape to be batch_size x nclasses\n",
    "#         print(\"O\",o.shape)\n",
    "#         o = o.reshape(batch_size,nclasses)\n",
    "        # From y we want to predict y[1:]. at y[t], predict y[t+1]\n",
    "        loss += F.cross_entropy(o, torch.tensor(batch_Y[:,t+1], device=device))\n",
    "\n",
    "        p = softmax(o)\n",
    "#         print(torch.argmax(p, dim=1), torch.tensor(batch_Y[:,t+1], device=device))\n",
    "        c = torch.sum(torch.argmax(p, dim=1)==torch.tensor(batch_Y[:,t+1], device=device))\n",
    "#         print(c.item())\n",
    "        correct += c.item()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/parrt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 144.2157 accur  0.3804   LR 0.000475\n",
      "Epoch   2 training loss 59.9173 accur  0.5636   LR 0.000650\n",
      "Epoch   3 training loss 43.4087 accur  0.6148   LR 0.000825\n",
      "Epoch   4 training loss 35.6588 accur  0.6423   LR 0.001000\n",
      "Epoch   5 training loss 26.6445 accur  0.6783   LR 0.000825\n",
      "Epoch   6 training loss 15.1577 accur  0.7553   LR 0.000650\n",
      "Epoch   7 training loss 14.5702 accur  0.7723   LR 0.000475\n",
      "Epoch   8 training loss  6.5942 accur  0.8702   LR 0.000300\n",
      "Epoch   9 training loss  3.0505 accur  0.9329   LR 0.000387\n",
      "Epoch  10 training loss  1.1049 accur  0.9659   LR 0.000475\n",
      "Epoch  11 training loss  0.4836 accur  0.9856   LR 0.000563\n",
      "Epoch  12 training loss  0.3146 accur  0.9928   LR 0.000650\n",
      "Epoch  13 training loss  0.1960 accur  0.9959   LR 0.000563\n",
      "Epoch  14 training loss  0.1272 accur  0.9984   LR 0.000475\n",
      "Epoch  15 training loss  0.0837 accur  0.9997   LR 0.000387\n",
      "Epoch  16 training loss  0.0758 accur  0.9998   LR 0.000300\n",
      "Epoch  17 training loss  0.0608 accur  1.0000   LR 0.000344\n",
      "Epoch  18 training loss  0.0594 accur  1.0000   LR 0.000387\n",
      "Epoch  19 training loss  0.0539 accur  1.0000   LR 0.000431\n",
      "Epoch  20 training loss  0.0504 accur  1.0000   LR 0.000475\n",
      "Epoch  21 training loss  0.0459 accur  1.0000   LR 0.000431\n",
      "Epoch  22 training loss  0.0416 accur  1.0000   LR 0.000387\n",
      "Epoch  23 training loss  0.0379 accur  1.0000   LR 0.000344\n",
      "Epoch  24 training loss  0.0352 accur  1.0000   LR 0.000300\n",
      "Epoch  25 training loss  0.0329 accur  1.0000   LR 0.000322\n",
      "Epoch  26 training loss  0.0314 accur  1.0000   LR 0.000344\n",
      "Epoch  27 training loss  0.0298 accur  1.0000   LR 0.000366\n",
      "Epoch  28 training loss  0.0283 accur  1.0000   LR 0.000387\n",
      "Epoch  29 training loss  0.0267 accur  1.0000   LR 0.000366\n",
      "Epoch  30 training loss  0.0251 accur  1.0000   LR 0.000344\n",
      "Epoch  31 training loss  0.0238 accur  1.0000   LR 0.000322\n",
      "Epoch  32 training loss  0.0226 accur  1.0000   LR 0.000300\n",
      "Epoch  33 training loss  0.0216 accur  1.0000   LR 0.000311\n",
      "Epoch  34 training loss  0.0208 accur  1.0000   LR 0.000322\n",
      "Epoch  35 training loss  0.0200 accur  1.0000   LR 0.000333\n",
      "Epoch  36 training loss  0.0192 accur  1.0000   LR 0.000344\n",
      "Epoch  37 training loss  0.0184 accur  1.0000   LR 0.000333\n",
      "Epoch  38 training loss  0.0176 accur  1.0000   LR 0.000322\n",
      "Epoch  39 training loss  0.0169 accur  1.0000   LR 0.000311\n",
      "Epoch  40 training loss  0.0163 accur  1.0000   LR 0.000300\n",
      "Epoch  41 training loss  0.0157 accur  1.0000   LR 0.000305\n",
      "Epoch  42 training loss  0.0152 accur  1.0000   LR 0.000311\n",
      "Epoch  43 training loss  0.0147 accur  1.0000   LR 0.000316\n",
      "Epoch  44 training loss  0.0142 accur  1.0000   LR 0.000322\n",
      "Epoch  45 training loss  0.0137 accur  1.0000   LR 0.000316\n",
      "Epoch  46 training loss  0.0132 accur  1.0000   LR 0.000311\n",
      "Epoch  47 training loss  0.0128 accur  1.0000   LR 0.000305\n",
      "Epoch  48 training loss  0.0124 accur  1.0000   LR 0.000300\n",
      "Epoch  49 training loss  0.0120 accur  1.0000   LR 0.000303\n",
      "Epoch  50 training loss  0.0116 accur  1.0000   LR 0.000305\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "Ex = torch.randn(embed_sz,     len(X_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       embed_sz,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "bo = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "Ey = torch.randn(y_embed_sz,   len(Y_vocab),  device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W2 = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "Cx = torch.eye(nhidden,        nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U2 = torch.randn(nhidden,      y_embed_sz,    device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,U2,V], lr=0.001, weight_decay=0.0)\n",
    "optimizer = torch.optim.Adam([Ex,W,U,Ey,W2,Cx,U2,V,bx,by,bo], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.0003, max_lr=0.001,\n",
    "                                              cycle_momentum=False)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "history = []\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_compares = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X[p:p+batch_size]\n",
    "        batch_Y = Y[p:p+batch_size]\n",
    "        loss, correct = forward(batch_X, X_max_len, batch_Y, Y_max_len)\n",
    "        epoch_training_accur += correct\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "        total_compares += batch_size * (Y_max_len - 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= total_compares\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    n = len(x)\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        # ENCODER\n",
    "        h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(x)):\n",
    "            embedding_step_t = Ex[:,x[t]]\n",
    "            embedding_step_t = embedding_step_t.reshape(embed_sz,1)\n",
    "            h = W @ h + U @ embedding_step_t + bx\n",
    "            h = torch.tanh(h)\n",
    "        c = h\n",
    "        \n",
    "        # DECODER\n",
    "        y = [Y_vocab['<']] # begin with \"start of sequence\" char\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        h = torch.zeros(nhidden, 1, device=device, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "        while y!=Y_vocab['>']:\n",
    "            embedding_step_t = Ey[:,y]\n",
    "            embedding_step_t = embedding_step_t.reshape(y_embed_sz,1)\n",
    "            h = W2 @ h + Cx@c + U2 @ embedding_step_t + by\n",
    "            h = torch.tanh(h)\n",
    "            o = V @ h + bo\n",
    "            o = o.reshape(1,nclasses)\n",
    "            p = softmax(o[0])\n",
    "            y = torch.argmax(p).item()\n",
    "            if y!=Y_vocab['>']:\n",
    "                output.append(Y_idx[y])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one'] => ['1', '1', '2', '9', '1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred'] => ['1', '1', '1', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'ten'] => ['1', '1', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred ten\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'hundred', 'thirty', 'two'] => ['1', '1', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one hundred thirty two\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eleven'] => ['1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"eleven\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ninety', 'nine'] => ['1', '9', '9']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"ninety nine\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fifty', 'three'] => ['1', '5', '3']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"fifty three\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'thousand', 'four', 'hundred', 'fifteen'] => ['1', '4', '1', '5']\n"
     ]
    }
   ],
   "source": [
    "x = [X_vocab[w] for w in \"one thousand four hundred fifteen\".split()]\n",
    "output = sample(x)\n",
    "print([X_idx[n] for n in x],'=>', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
