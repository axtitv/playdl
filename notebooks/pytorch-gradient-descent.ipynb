{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch gradient descent\n",
    "\n",
    "Following along with the excellent [PyTorch tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  28446274.00\n",
      "Epoch  30 loss     76031.40\n",
      "Epoch  60 loss      6711.67\n",
      "Epoch  90 loss      1043.08\n",
      "Epoch 120 loss       203.26\n",
      "Epoch 150 loss        43.90\n",
      "Epoch 180 loss        10.02\n",
      "Epoch 210 loss         2.37\n",
      "Epoch 240 loss         0.58\n",
      "Epoch 270 loss         0.14\n",
      "Epoch 300 loss         0.04\n",
      "Epoch 330 loss         0.01\n",
      "Epoch 360 loss         0.00\n",
      "Epoch 390 loss         0.00\n",
      "Epoch 420 loss         0.00\n",
      "Epoch 450 loss         0.00\n",
      "Epoch 480 loss         0.00\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)  # instead of np.random.randn()\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)  # mm is dot() in numpy\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum() # np: loss = np.square(y_pred - y).sum()\n",
    "    if t % 30 == 0:\n",
    "        print(f\"Epoch {t:3d} loss {loss:12.2f}\")\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.T)\n",
    "    grad_h = grad_h_relu.clone()  # clone() is copy() in numpy\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.mm(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  35567960.00\n",
      "Epoch  30 loss     66783.85\n",
      "Epoch  60 loss      6043.99\n",
      "Epoch  90 loss       884.66\n",
      "Epoch 120 loss       157.14\n",
      "Epoch 150 loss        31.17\n",
      "Epoch 180 loss         6.67\n",
      "Epoch 210 loss         1.52\n",
      "Epoch 240 loss         0.36\n",
      "Epoch 270 loss         0.09\n",
      "Epoch 300 loss         0.02\n",
      "Epoch 330 loss         0.01\n",
      "Epoch 360 loss         0.00\n",
      "Epoch 390 loss         0.00\n",
      "Epoch 420 loss         0.00\n",
      "Epoch 450 loss         0.00\n",
      "Epoch 480 loss         0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)  # instead of np.random.randn()\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)  # <-- track for autograd\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 30 == 0:\n",
    "        print(f\"Epoch {t:3d} loss {loss:12.2f}\")\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    loss.backward() # autograd\n",
    "\n",
    "    # Update weights; weights have requires_grad=True but we don't need to track these updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
