{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate last names for a specific natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "    p = y_prob[range(n),y_true]\n",
    "    return torch.mean(-torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using pickled data from my RNN article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X-lastnames.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y-lastnames.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick language of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['m', 'c', 'g', 'o', 'w', 'a', 'n'],\n",
       " ['m', 'c', 'a', 'u', 'l', 'e', 'y'],\n",
       " ['b', 'u', 'r', 't', 'o', 'n'],\n",
       " ['t', 'a', 't', 'l', 'o', 'c', 'k'],\n",
       " ['r', 'i', 'c', 'h', 'e', 's']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2idx = {\n",
    "    'Arabic': 0,\n",
    "    'Chinese': 1,\n",
    "    'Czech': 2,\n",
    "    'Dutch': 3,\n",
    "    'English': 4,\n",
    "    'French': 5,\n",
    "    'German': 6,\n",
    "    'Greek': 7,\n",
    "    'Irish': 8,\n",
    "    'Italian': 9,\n",
    "    'Japanese': 10,\n",
    "    'Korean': 11,\n",
    "    'Polish': 12,\n",
    "    'Portuguese': 13,\n",
    "    'Russian': 14,\n",
    "    'Scottish': 15,\n",
    "    'Spanish': 16,\n",
    "    'Vietnamese': 17\n",
    "}\n",
    "\n",
    "# get just these names and then we can ignore y\n",
    "X_train = np.array(X)[np.array(y)==lang2idx['English']]\n",
    "list(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 training records, 28 features (chars), state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "nhidden = 100\n",
    "nfeatures = len(vocab)\n",
    "nclasses = nfeatures\n",
    "n = len(X_train)\n",
    "print(f\"{n:,d} training records, {nfeatures} features (chars), state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    loss = 0.0\n",
    "    outputs = []\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for j in range(len(x)):  # for each char in a name\n",
    "        h = W@h + U@onehot(x[j])\n",
    "        h = torch.tanh(h)\n",
    "        o = V@h\n",
    "        o = o.reshape(1,nclasses)\n",
    "        o = softmax(o)\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)\n",
    "\n",
    "def forwardN(X:Sequence[Sequence]):#, apply_softmax=True):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    outputs = []\n",
    "    for i in range(0, len(X)): # for each input record\n",
    "        o = forward1(X[i])\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss  9.1463 accur  0.0968\n",
      "Epoch   2 training loss  5.4063 accur  0.1915\n",
      "Epoch   3 training loss  4.0409 accur  0.2519\n",
      "Epoch   4 training loss  3.2545 accur  0.3057\n",
      "Epoch   5 training loss  2.7545 accur  0.3682\n",
      "Epoch   6 training loss  2.4158 accur  0.3984\n",
      "Epoch   7 training loss  2.1724 accur  0.4388\n",
      "Epoch   8 training loss  1.9830 accur  0.4639\n",
      "Epoch   9 training loss  1.8307 accur  0.4895\n",
      "Epoch  10 training loss  1.7046 accur  0.5131\n",
      "Epoch  11 training loss  1.5976 accur  0.5371\n",
      "Epoch  12 training loss  1.5076 accur  0.5663\n",
      "Epoch  13 training loss  1.4317 accur  0.5837\n",
      "Epoch  14 training loss  1.3674 accur  0.6001\n",
      "Epoch  15 training loss  1.3133 accur  0.6098\n",
      "Epoch  16 training loss  1.2672 accur  0.6272\n",
      "Epoch  17 training loss  1.2284 accur  0.6354\n",
      "Epoch  18 training loss  1.1947 accur  0.6426\n",
      "Epoch  19 training loss  1.1655 accur  0.6462\n",
      "Epoch  20 training loss  1.1404 accur  0.6534\n",
      "Epoch  21 training loss  1.1184 accur  0.6621\n",
      "Epoch  22 training loss  1.0993 accur  0.6687\n",
      "Epoch  23 training loss  1.0829 accur  0.6708\n",
      "Epoch  24 training loss  1.0685 accur  0.6743\n",
      "Epoch  25 training loss  1.0559 accur  0.6743\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.001, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 25\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_char = 0\n",
    "    for i in range(0, n): # an epoch trains all names\n",
    "        x = X_train[i]    # get one name and compute y as x shifted\n",
    "#         print(\"NAME\", x)\n",
    "        x, y = x[0:-1], [ctoi[y] for y in x[1:]]\n",
    "        total_char += len(x)\n",
    "        o = forward(x)\n",
    "        loss = cross_entropy(o, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "#         print(loss.item())\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "#         print(torch.argmax(o, dim=1), 'vs', y)\n",
    "        correct = torch.sum( torch.argmax(o, dim=1)==torch.tensor(y) )\n",
    "        epoch_training_accur += correct\n",
    "\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= total_char\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         o = forward(X_train)#, apply_softmax=False)\n",
    "#         train_loss = cross_entropy(o, y)\n",
    "#         correct = torch.argmax(o, dim=1).detach()==y_train\n",
    "#         train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "#         o = forward(X_valid)\n",
    "#         valid_loss = cross_entropy(o, y_valid)\n",
    "#         correct = torch.argmax(o, dim=1).detach()==y_valid\n",
    "#         valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "#         history.append((train_loss, valid_loss))\n",
    "#         print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "# history = torch.tensor(history)\n",
    "# plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                h = W@h + U@onehot(chars[j])\n",
    "                h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p)\n",
    "#             print(p)\n",
    "#             print(wi)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'u', 'b', 's', 'o', 'l', 'l']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('hub'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'o', 'y', 'c', 'e', 's', 's']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('j'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 't', 'h', 'e', 'r', 'i', 's']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('et'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['m', 'c', 'g', 'o', 'w', 'a', 'n'],\n",
       " ['m', 'c', 'a', 'u', 'l', 'e', 'y'],\n",
       " ['b', 'u', 'r', 't', 'o', 'n'],\n",
       " ['t', 'a', 't', 'l', 'o', 'c', 'k'],\n",
       " ['r', 'i', 'c', 'h', 'e', 's'],\n",
       " ['t', 'o', 'l', 'l', 'e', 'y'],\n",
       " ['p', 'r', 'e', 'n', 't', 'i', 'c', 'e'],\n",
       " ['t', 'e', 'a', 'l', 'e'],\n",
       " ['w', 'i', 'n', 'f', 'i', 'e', 'l', 'd'],\n",
       " ['w', 'o', 'o', 'l', 'l', 'e', 'y'],\n",
       " ['s', 'h', 'o', 't', 't', 'o', 'n'],\n",
       " ['k', 'i', 'n', 'g', 'd', 'o', 'n'],\n",
       " ['a', 'b', 'e', 'l'],\n",
       " ['f', 'a', 'l', 's', 'h'],\n",
       " ['e', 'p', 't', 'o', 'n'],\n",
       " ['s', 'n', 'e', 'd', 'd', 'o', 'n'],\n",
       " ['h', 'a', 'n', 's', 'e', 'n'],\n",
       " ['n', 'e', 'v', 'i', 's'],\n",
       " ['p', 'a', 'r', 'k', 'e', 's'],\n",
       " ['m', 'a', 'i', 'n']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)[0:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
