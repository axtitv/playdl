{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate last names for a specific natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "    p = y_prob[range(n),y_true]\n",
    "    return torch.mean(-torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load using pickled data from my RNN article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X-lastnames.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y-lastnames.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick language of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['y', 'e', 'a', 't', 'm', 'a', 'n'],\n",
       " ['j', 'o', 'h', 'n', 's', 'o', 'n'],\n",
       " ['d', 'u', 'p', 'o', 'n', 't'],\n",
       " ['d', 'o', 'w', 'n', 'e', 'r'],\n",
       " ['c', 'o', 'o', 'm', 'b', 'e', 's']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2idx = {\n",
    "    'Arabic': 0,\n",
    "    'Chinese': 1,\n",
    "    'Czech': 2,\n",
    "    'Dutch': 3,\n",
    "    'English': 4,\n",
    "    'French': 5,\n",
    "    'German': 6,\n",
    "    'Greek': 7,\n",
    "    'Irish': 8,\n",
    "    'Italian': 9,\n",
    "    'Japanese': 10,\n",
    "    'Korean': 11,\n",
    "    'Polish': 12,\n",
    "    'Portuguese': 13,\n",
    "    'Russian': 14,\n",
    "    'Scottish': 15,\n",
    "    'Spanish': 16,\n",
    "    'Vietnamese': 17\n",
    "}\n",
    "\n",
    "# get just these names and then we can ignore y\n",
    "X_train = np.array(X)[np.array(y)==lang2idx['English']]\n",
    "list(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 training records, 28 features (chars), state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "nhidden = 100\n",
    "nfeatures = len(vocab)\n",
    "nclasses = nfeatures\n",
    "n = len(X_train)\n",
    "print(f\"{n:,d} training records, {nfeatures} features (chars), state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    loss = 0.0\n",
    "    outputs = []\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for j in range(len(x)):  # for each char in a name\n",
    "        h = W@h + U@onehot(x[j])\n",
    "        h = torch.tanh(h)\n",
    "        o = V@h\n",
    "        o = o.reshape(1,nclasses)\n",
    "        o = softmax(o)\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)\n",
    "\n",
    "def forwardN(X:Sequence[Sequence]):#, apply_softmax=True):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    outputs = []\n",
    "    for i in range(0, len(X)): # for each input record\n",
    "        o = forward1(X[i])\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss  8.7718 accur  0.0950\n",
      "Epoch   2 training loss  4.7915 accur  0.2344\n",
      "Epoch   3 training loss  3.3553 accur  0.3432\n",
      "Epoch   4 training loss  2.5772 accur  0.4238\n",
      "Epoch   5 training loss  2.0987 accur  0.4849\n",
      "Epoch   6 training loss  1.7790 accur  0.5461\n",
      "Epoch   7 training loss  1.5576 accur  0.5813\n",
      "Epoch   8 training loss  1.3970 accur  0.6230\n",
      "Epoch   9 training loss  1.2784 accur  0.6480\n",
      "Epoch  10 training loss  1.1817 accur  0.6698\n",
      "Epoch  11 training loss  1.1002 accur  0.6846\n",
      "Epoch  12 training loss  1.0319 accur  0.7008\n",
      "Epoch  13 training loss  0.9764 accur  0.7147\n",
      "Epoch  14 training loss  0.9324 accur  0.7249\n",
      "Epoch  15 training loss  0.8972 accur  0.7341\n",
      "Epoch  16 training loss  0.8689 accur  0.7383\n",
      "Epoch  17 training loss  0.8455 accur  0.7434\n",
      "Epoch  18 training loss  0.8275 accur  0.7466\n",
      "Epoch  19 training loss  0.8127 accur  0.7485\n",
      "Epoch  20 training loss  0.8022 accur  0.7485\n",
      "Epoch  21 training loss  0.7931 accur  0.7499\n",
      "Epoch  22 training loss  0.7826 accur  0.7517\n",
      "Epoch  23 training loss  0.7730 accur  0.7531\n",
      "Epoch  24 training loss  0.7650 accur  0.7522\n",
      "Epoch  25 training loss  0.7579 accur  0.7536\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.001, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 25\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total_char = 0\n",
    "    for i in range(0, n): # an epoch trains all names\n",
    "        x = X_train[i]    # get one name and compute y as x shifted\n",
    "#         print(\"NAME\", x)\n",
    "        x, y = x[0:-1], [ctoi[y] for y in x[1:]]\n",
    "        total_char += len(x)\n",
    "        o = forward(x)\n",
    "        loss = cross_entropy(o, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "#         print(loss.item())\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "#         print(torch.argmax(o, dim=1), 'vs', y)\n",
    "        correct = torch.sum( torch.argmax(o, dim=1)==torch.tensor(y) )\n",
    "        epoch_training_accur += correct\n",
    "\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= total_char\n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         o = forward(X_train)#, apply_softmax=False)\n",
    "#         train_loss = cross_entropy(o, y)\n",
    "#         correct = torch.argmax(o, dim=1).detach()==y_train\n",
    "#         train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "#         o = forward(X_valid)\n",
    "#         valid_loss = cross_entropy(o, y_valid)\n",
    "#         correct = torch.argmax(o, dim=1).detach()==y_valid\n",
    "#         valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "#         history.append((train_loss, valid_loss))\n",
    "#         print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "# history = torch.tensor(history)\n",
    "# plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                h = W@h + U@onehot(chars[j])\n",
    "                h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p)\n",
    "#             print(p)\n",
    "#             print(wi)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'u', 'b', 'b', 'a', 'r', 'd']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('hub'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'o', 'h', 'n', 's', 'o', 'n']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('j'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'r', 'c', 'h', 'e', 'r', 's']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list('ar'), 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['y', 'e', 'a', 't', 'm', 'a', 'n'],\n",
       " ['j', 'o', 'h', 'n', 's', 'o', 'n'],\n",
       " ['d', 'u', 'p', 'o', 'n', 't'],\n",
       " ['d', 'o', 'w', 'n', 'e', 'r'],\n",
       " ['c', 'o', 'o', 'm', 'b', 'e', 's'],\n",
       " ['b', 'y', 'r', 'n', 'e'],\n",
       " ['d', 'u', 'f', 'f', 'i', 'e', 'l', 'd'],\n",
       " ['d', 'o', 'r', 'a', 'n'],\n",
       " ['n', 'u', 'g', 'e', 'n', 't'],\n",
       " ['p', 'e', 'a', 'c', 'h'],\n",
       " ['a', 'l', 'd', 'e', 'n'],\n",
       " ['f', 'i', 'e', 'l', 'd', 'e', 'r'],\n",
       " ['p', 'i', 'c', 'k', 't', 'h', 'a', 'l', 'l'],\n",
       " ['d', 'o', 'w', 'n', 'e', 'r'],\n",
       " ['p', 'i', 'c', 'k'],\n",
       " ['d', 'o', 'w', 'n', 'i', 'e'],\n",
       " ['c', 'l', 'i', 'f', 'f'],\n",
       " ['b', 'r', 'e', 'l', 's', 'f', 'o', 'r', 'd'],\n",
       " ['a', 'r', 'c', 'h', 'e', 'r'],\n",
       " ['e', 't', 't', 'r', 'i', 'c', 'k']]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)[0:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
