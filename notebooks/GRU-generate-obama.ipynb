{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU: Generate obama speeches\n",
    "\n",
    "Using truncated back propagation and add embedding layer instead of one-hot encoding going into GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    if torch.isnan(y_prob).any():\n",
    "        raise ValueError(\"cross_entropy: y_prob has NaN!\",y_prob)\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "    p = y_prob[range(n),y_true]\n",
    "    p_ = p.detach()\n",
    "    if torch.isnan(p).any():\n",
    "        raise ValueError(\"cross_entropy: p has NaN! p=\",p_,\"y_prob=\",y_prob)\n",
    "    if (p_<0).any():\n",
    "        raise ValueError(\"cross_entropy: y_prob has negative value!:\",p_)\n",
    "    m = torch.mean(-torch.log(p))\n",
    "    if torch.isnan(m):\n",
    "        raise ValueError(\"cross_entropy: mean is NaN! p=\",p_)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks\n",
    "\n",
    "The stochastic part of SGD is critical for training models. The idea is simply to use a small subset of the data when computing gradients to update the model parameters. Generally we take a small batch size of say 32 records, run that through the model, and then compute a loss. From that loss we compute the gradient and then update the model parameters and move onto the next batch.  Once all batches are complete, we have completed an epoch.  We should shuffle the batches and keep going.\n",
    "\n",
    "We can also be stochastic by updating the gradient in the middle of long sequences, rather than waiting until after a complete batch of long sequences.  If the sequences are really long, waiting till the end of a batch reduces the stochastic nature. Instead I'm going to try breaking up the entire input into a small number of very long sequences. In this way the RNN can keep the hidden state going for the complete sequence. Of course the only problem is that we cannot compute back propagation that far, so at some sequence length I can update the gradient and wipe it out then continue. I think this is easier than modifying the data set stride so that a standard training loop for an RNN keeps the same hidden state across long sequences even if we have broken into chunks.\n",
    "\n",
    "Let's say that we have a large text and we break it up into six chunks: A,B,C,D,E,F. then, six is our batch size and we will process each long sequence exactly once per epic. However to get stochastic nature, we will update the gradient after only a small sequence of characters.  We pick the chunk size and then the batch sizes computed instead of having to specify both. I think the chunk size is more important: how much can you store in a single hidden state vector.\n",
    "\n",
    "Come to think of it, all we need to specify is the number of chunks we want to break the text into.  There won't be any batch size because we have a single batch with `nchunks`  long records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:1_000_000] # testing\n",
    "n = len(text)\n",
    "\n",
    "bptt = 32                 # only look back this many time steps for gradients\n",
    "nhidden = 512\n",
    "char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN \n",
    "nchunks = 64              # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "\n",
    "# bptt = 30                 # only look back this many time steps for gradients\n",
    "# nhidden = 512\n",
    "# char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN \n",
    "# nchunks = 64              # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "chunk_size = n // nchunks # the sequences will be very long\n",
    "n = nchunks * chunk_size  # reset size so it's an even multiple of chunk size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]], device=device)\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]],   device=device)\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = len(ctoi)\n",
    "print(f\"{nchunks:,d} training records, chunk length {chunk_size}, vocab size {len(ctoi)}, char_embed_sz {char_embed_sz}, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(char_embed_sz, len(ctoi),     device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "Whz = torch.eye(nhidden,       nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "Whr = torch.eye(nhidden,       nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "Whh_ = torch.eye(nhidden,       nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "Uxh_ = torch.randn(nhidden,    char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "Uxz = torch.randn(nhidden,     char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "Uxr = torch.randn(nhidden,     char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "bx = torch.zeros(nhidden,      1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "by = torch.zeros(nclasses,     1,             device=device, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# if using relu, b must be 0. W must be identity so don't mess with sd. others must have low stdev\n",
    "# From [Le 2015] https://arxiv.org/abs/1504.00941\n",
    "# \"For IRNNs, in addition to the recurrent weights being initialized at identity, the non-recurrent\n",
    "#  weights are initialized with a random matrix, whose entries are sampled from a\n",
    "#  Gaussian distribution with mean of zero and standard deviation of 0.001.\"\n",
    "sd = 0.001  # weight stddev init for relu\n",
    "sd = 0.01   # weight stddev init for tanh\n",
    "# with torch.no_grad():\n",
    "#     E *= sd\n",
    "#     U *= sd\n",
    "#     V *= sd\n",
    "    \n",
    "# gradient clipping values \n",
    "gc = {1, 10, 100, 1000}\n",
    "\n",
    "parameters = [E,Whz,Whr,Whh_,Uxz,Uxr,Uxh_,V]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=3,\n",
    "                                              base_lr=0.00005, max_lr=0.0007,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        chars_step_t = X[:,t] # char_embed_sz x nchunks\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = E[:,chars_step_t] # char_embed_sz x nchunks\n",
    "#         print(embedding_step_t.shape, E.shape, H.shape, W.shape, U.shape)\n",
    "\n",
    "        Z = torch.sigmoid(Whz@H + Uxz@embedding_step_t)\n",
    "        R = torch.sigmoid(Whr@H + Uxr@embedding_step_t)\n",
    "        H_ = torch.tanh(Whh_.mm(R*H) + Uxh_.mm(embedding_step_t))\n",
    "        H = torch.tanh( (1-Z)*H + Z*H_ )\n",
    "\n",
    "        o = V.mm(H)\n",
    "        o = o.T # make it nchunks x nclasses\n",
    "        p = softmax(o)\n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "#         print(f\"loss {loss:7.4f}\")\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling to generate fake sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus(probabilities, P=.95):\n",
    "    \"\"\"\n",
    "    Given probabilities array (summing to 1.0), find and return new array\n",
    "    containing just the largest probabilities that sum to less than or\n",
    "    equal to P. Normalize to sum to 1.0 by dividing by new sum. All\n",
    "    other probabilities are 0.\n",
    "    \"\"\"\n",
    "    P = max(P,torch.max(probabilities))\n",
    "    p_idx = torch.flip( torch.argsort(probabilities), dims=[0] )\n",
    "    c = torch.cumsum(probabilities[p_idx], dim=0)\n",
    "    probabilities_ = torch.zeros_like(probabilities)\n",
    "    top_P = p_idx[torch.where(c<=P)[0]]\n",
    "    probabilities_[top_P] = probabilities[top_P]\n",
    "    return probabilities_ / torch.sum(probabilities_) # normalize so sum is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, use_nucleus=True):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    n -= len(initial_chars)\n",
    "    output = initial_chars.copy()\n",
    "    with torch.no_grad():\n",
    "        # get h for initial char\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(initial_chars)-1):\n",
    "            c = initial_chars[t]\n",
    "            ci = ctoi[c]\n",
    "            embedding_step_t = E[:,ci].reshape(char_embed_sz,1)\n",
    "            z = torch.sigmoid(Whz@h + Uxz@embedding_step_t)\n",
    "            r = torch.sigmoid(Whr@h + Uxr@embedding_step_t)\n",
    "            h_ = torch.tanh(Whh_.mm(r*h) + Uxh_.mm(embedding_step_t))\n",
    "            h = torch.tanh( (1-z)*h + z*h_ )\n",
    "\n",
    "        ci = ctoi[initial_chars[-1]] # get last initial char (it's unprocessed)\n",
    "        \n",
    "        for i in range(n):\n",
    "            embedding_step_t = E[:,ci].reshape(char_embed_sz,1) # col is embedding for c; must be column\n",
    "#                 print(embedding_step_t.shape, E.shape, h.shape, Whz.shape, Uxz.shape)#, V.shape)\n",
    "            z = torch.sigmoid(Whz@h + Uxz@embedding_step_t)\n",
    "            r = torch.sigmoid(Whr@h + Uxr@embedding_step_t)\n",
    "            h_ = torch.tanh(Whh_.mm(r*h) + Uxh_.mm(embedding_step_t))\n",
    "            h = torch.tanh( (1-z)*h + z*h_ )\n",
    "            o = V@h\n",
    "#             print(o.shape, h.shape)\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             print(p.cpu().numpy())\n",
    "            if use_nucleus:\n",
    "                p = nucleus(p)\n",
    "#             print(\"AFTER\")\n",
    "#             print(p.cpu().numpy())\n",
    "            ci = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            output.append(vocab[ci])\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(''.join( sample(list('yes we can'), 300) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments to compute nucleus sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0,  0.6,  0,  0,  0,  0,  0,  0,  0,  0,  0.03, 0,  0.07, 0,\n",
    "     0.02, 0,  0.02, 0,  0,  0,  0.09, 0,  0,  0,  0.03, 0,  0,  0,\n",
    "     0.01, 0.03, 0,  0,  0,  0.01, 0.05, 0,  0,  0,  0,  0,  0.01]\n",
    "p = torch.tensor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 20, 12, 34, 10, 24, 29, 14, 16, 28, 40, 33, 31, 38, 11, 39,  9,  8,\n",
       "        13,  0,  5,  4,  3,  2,  7,  6, 30, 36, 32, 27, 26, 25, 37, 22, 15, 35,\n",
       "        19, 18, 17, 23, 21])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_idx = torch.argsort(p)\n",
    "p_idx = torch.flip(p_idx, dims=[0])\n",
    "p_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.0900, 0.0700, 0.0500, 0.0300, 0.0300, 0.0300, 0.0200, 0.0200,\n",
       "        0.0100, 0.0100, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[p_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.6900, 0.7600, 0.8100, 0.8400, 0.8700, 0.9000, 0.9200, 0.9400,\n",
       "        0.9500, 0.9600, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700,\n",
       "        0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700,\n",
       "        0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700, 0.9700,\n",
       "        0.9700, 0.9700, 0.9700, 0.9700, 0.9700])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.cumsum(p[p_idx], dim=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# goal is probability P\n",
    "P = .82\n",
    "torch.where(c<=P)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 20, 12, 34])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_P = p_idx[torch.where(c<=P)[0]]\n",
    "top_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6000, 0.0900, 0.0700, 0.0500])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[top_P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.7407, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0617, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nucleus(p, 0.81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factoring GRU into object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, input_size, embed_sz):\n",
    "        self.E = torch.randn(embed_sz, input_size, device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "        self.input_size = input_size\n",
    "        self.embed_sz = embed_sz\n",
    "        with torch.no_grad():\n",
    "            self.E *= 0.01\n",
    "    def __call__(self, x):\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        return self.E[:,x].reshape(self.embed_sz,len(x)) # embed_sz by len(x) (is this shape except when len(x)==1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_sz, nhidden):\n",
    "        self.Whz  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whr  = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Whh_ = torch.eye(nhidden,   nhidden,  device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxh_ = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxz  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.Uxr  = torch.randn(nhidden, input_sz, device=device, dtype=torch.float64, requires_grad=True)\n",
    "    def __call__(self, h, x):\n",
    "        z = torch.sigmoid(self.Whz@h + self.Uxz@x)\n",
    "        r = torch.sigmoid(self.Whr@h + self.Uxr@x)\n",
    "        h_ = torch.tanh(self.Whh_@(r*h) + self.Uxh_@x)\n",
    "        h = torch.tanh( (1-z)*h + z*h_ )\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.V = torch.randn(output_size,  input_size, device=device, dtype=torch.float64, requires_grad=True)\n",
    "        self.by = torch.zeros(output_size, 1,          device=device, dtype=torch.float64, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            self.V *= 0.01\n",
    "    def __call__(self, h):\n",
    "        o = self.V@h + self.by\n",
    "        o = o.T # make it input_size x output_size\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss   45.950   accur  0.1731   LR 0.000212\n",
      "Epoch   2 training loss   34.300   accur  0.3515   LR 0.000375\n",
      "Epoch   3 training loss   26.790   accur  0.4804   LR 0.000538\n",
      "Epoch   4 training loss   22.547   accur  0.5592   LR 0.000700\n",
      "Epoch   5 training loss   20.333   accur  0.5982   LR 0.000538\n",
      "Epoch   6 training loss   18.943   accur  0.6218   LR 0.000375\n",
      "Epoch   7 training loss   18.133   accur  0.6365   LR 0.000212\n",
      "Epoch   8 training loss   17.594   accur  0.6463   LR 0.000050\n",
      "Epoch   9 training loss   17.247   accur  0.6529   LR 0.000131\n",
      "Epoch  10 training loss   17.217   accur  0.6534   LR 0.000212\n",
      "Epoch  11 training loss   17.147   accur  0.6544   LR 0.000294\n",
      "Epoch  12 training loss   17.033   accur  0.6562   LR 0.000375\n",
      "Epoch  13 training loss   16.890   accur  0.6583   LR 0.000294\n",
      "Epoch  14 training loss   16.560   accur  0.6648   LR 0.000212\n",
      "Epoch  15 training loss   16.283   accur  0.6700   LR 0.000131\n",
      "Epoch  16 training loss   16.056   accur  0.6743   LR 0.000050\n",
      "Epoch  17 training loss   15.879   accur  0.6778   LR 0.000091\n",
      "Epoch  18 training loss   15.862   accur  0.6780   LR 0.000131\n",
      "Epoch  19 training loss   15.842   accur  0.6783   LR 0.000172\n",
      "Epoch  20 training loss   15.804   accur  0.6790   LR 0.000212\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(ctoi), char_embed_sz)\n",
    "gru = GRU(char_embed_sz, nhidden)\n",
    "lin = Linear(nhidden, nclasses)\n",
    "parameters = [emb.E,\n",
    "              gru.Whz,gru.Whr,gru.Whh_,gru.Uxh_,gru.Uxz,gru.Uxr,\n",
    "              lin.V,lin.by]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.00005, max_lr=0.0007,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        x = emb(X[:,t]) # char_embed_sz x nchunks\n",
    "        H = gru(H, x)\n",
    "        o = lin(H)\n",
    "\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "\n",
    "        p = softmax(o)        \n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, use_nucleus=True):    \n",
    "    n -= len(initial_chars)\n",
    "    output = initial_chars.copy()\n",
    "    with torch.no_grad():\n",
    "        # get h for initial char\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(initial_chars)-1):\n",
    "            ci = ctoi[initial_chars[t]]\n",
    "            x = emb(torch.tensor([ci])) # char_embed_sz x nchunks\n",
    "            h = gru(h, x)\n",
    "\n",
    "        ci = ctoi[initial_chars[-1]] # get last initial char (it's unprocessed)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x = emb(torch.tensor([ci]))\n",
    "            h = gru(h, x)\n",
    "            o = lin(h)\n",
    "            p = softmax(o).flatten()\n",
    "            if use_nucleus:\n",
    "                p = nucleus(p)\n",
    "            ci = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            output.append(vocab[ci])\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes we can't realize from side. in inamis is without cleaner. on the more executive secretary of nuclear weapons in my own lives for both opportunity for those who would race to make the most ignorancy of the extremists in the united states of any, and we have to inevitable bow level the afghan peop\n"
     ]
    }
   ],
   "source": [
    "print(''.join( sample(list('yes we can'), 300) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked GRU\n",
    "\n",
    "Stacking doesn't help with gru/gru2.\n",
    "\n",
    "Also tried deep transitions:\n",
    "\n",
    "```\n",
    "Ho = linO(H).T\n",
    "```\n",
    "\n",
    "Didn't help. was worse.  \n",
    "\n",
    "Now try short circuiting x into gru2.  That improved things 61%->63%. nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss   45.903   accur  0.1755   LR 0.000212\n",
      "Epoch   2 training loss   33.454   accur  0.3735   LR 0.000375\n",
      "Epoch   3 training loss   25.471   accur  0.5111   LR 0.000538\n",
      "Epoch   4 training loss   21.896   accur  0.5731   LR 0.000700\n",
      "Epoch   5 training loss   20.260   accur  0.5996   LR 0.000538\n",
      "Epoch   6 training loss   19.025   accur  0.6207   LR 0.000375\n",
      "Epoch   7 training loss   18.198   accur  0.6353   LR 0.000212\n",
      "Epoch   8 training loss   17.577   accur  0.6468   LR 0.000050\n",
      "Epoch   9 training loss   17.170   accur  0.6545   LR 0.000131\n",
      "Epoch  10 training loss   17.102   accur  0.6560   LR 0.000212\n",
      "Epoch  11 training loss   17.037   accur  0.6568   LR 0.000294\n",
      "Epoch  12 training loss   16.953   accur  0.6579   LR 0.000375\n",
      "Epoch  13 training loss   16.860   accur  0.6593   LR 0.000294\n",
      "Epoch  14 training loss   16.510   accur  0.6663   LR 0.000212\n",
      "Epoch  15 training loss   16.204   accur  0.6726   LR 0.000131\n",
      "Epoch  16 training loss   15.954   accur  0.6775   LR 0.000050\n",
      "Epoch  17 training loss   15.769   accur  0.6812   LR 0.000091\n",
      "Epoch  18 training loss   15.714   accur  0.6822   LR 0.000131\n",
      "Epoch  19 training loss   15.680   accur  0.6828   LR 0.000172\n",
      "Epoch  20 training loss   15.642   accur  0.6835   LR 0.000212\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(len(ctoi), char_embed_sz)\n",
    "gru = GRU(char_embed_sz, nhidden)\n",
    "gru2 = GRU(nhidden+char_embed_sz, nhidden//2) # cat x onto hidden h as input\n",
    "lin = Linear(nhidden//2, nclasses)\n",
    "\n",
    "parameters = [emb.E,\n",
    "              gru.Whz,gru.Whr,gru.Whh_,gru.Uxh_,gru.Uxz,gru.Uxr,\n",
    "              gru2.Whz,gru2.Whr,gru2.Whh_,gru2.Uxh_,gru2.Uxz,gru2.Uxr,\n",
    "#               linO.V,linO.by,\n",
    "              lin.V,lin.by]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              mode='triangular2',\n",
    "                                              step_size_up=4,\n",
    "                                              base_lr=0.00005, max_lr=0.0007,\n",
    "                                              cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    H  = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    H2 = torch.zeros(nhidden//2, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "        x = emb(X[:,t]) # char_embed_sz x nchunks\n",
    "        H = gru(H, x)\n",
    "#         H2 = gru2(H2, Ho)\n",
    "        H2 = gru2(H2, torch.cat([H,x]))\n",
    "        o = lin(H2)\n",
    "\n",
    "        loss += F.cross_entropy(o, y[:,t])\n",
    "\n",
    "        p = softmax(o)        \n",
    "        correct = torch.argmax(p, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach()\n",
    "            H2 = H2.detach()\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    epoch_training_loss /= bptt * nchunks\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.3f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, use_nucleus=True):   \n",
    "    n -= len(initial_chars)\n",
    "    output = initial_chars.copy()\n",
    "    with torch.no_grad():\n",
    "        # get h for initial char\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        h2 = torch.zeros(nhidden//2, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "        for t in range(len(initial_chars)-1):\n",
    "            ci = ctoi[initial_chars[t]]\n",
    "            x = emb(torch.tensor([ci])) # char_embed_sz x nchunks\n",
    "            h = gru(h, x)\n",
    "            h2 = gru2(h2, h)\n",
    "\n",
    "        ci = ctoi[initial_chars[-1]] # get last initial char (it's unprocessed)\n",
    "        \n",
    "        for i in range(n):\n",
    "            x = emb(torch.tensor([ci]))\n",
    "            h = gru(h, x)\n",
    "            h2 = gru2(h2, h)\n",
    "            o = lin(h2)\n",
    "            p = softmax(o).flatten()\n",
    "            if use_nucleus:\n",
    "                p = nucleus(p)\n",
    "            ci = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            output.append(vocab[ci])\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [256 x 532], m2: [512 x 1] at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/THC/generic/THCTensorMathBlas.cu:283",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6f089d733faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yes we can'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-6e42b8d7a223>\u001b[0m in \u001b[0;36msample\u001b[0;34m(initial_chars, n, use_nucleus)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# char_embed_sz x nchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minitial_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get last initial char (it's unprocessed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-af3fd6d66c38>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, h, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUxr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhz\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUxz\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhr\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUxr\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhh_\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUxh_\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [256 x 532], m2: [512 x 1] at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/THC/generic/THCTensorMathBlas.cu:283"
     ]
    }
   ],
   "source": [
    "print(''.join( sample(list('yes we can'), 300) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
