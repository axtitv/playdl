{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with an LSTM and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers, Sequential, losses\n",
    "import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "def words(text:str):\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n",
    "    char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words\n",
    "    Remove English stop words\n",
    "    \"\"\"\n",
    "    ctrl_chars = '\\x00-\\x1f'\n",
    "    regex = re.compile(r'[' + ctrl_chars + string.punctuation + '\\r\\t\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 0]\n",
    "    words = [w.lower() for w in words]\n",
    "    return words\n",
    "\n",
    "def compress_whitespace(s): # collapse things like \"\\n   \\t  \" with \" \"\n",
    "    return re.sub(r\"(\\s+)\", ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus\n",
    "\n",
    "Let's use [Alexander Hamilton's federalist papers 1-10](https://guides.loc.gov/federalist-papers/text-1-10#s-lg-box-wrapper-25493264) as our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FEDERALIST NO. 1 General Introduction For the Independent Journal. Author: Alexander Hamilton To the People of the State of New York: AFTER an unequivocal experience of the inefficiency of the subsisting federal government, you are called upon to deliberate on a new Constitution for the United State'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/federalist-papers.txt\")\n",
    "text = compress_whitespace(text)\n",
    "text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# The following fails on paperspace gradient platform\n",
    "#nlp = spacy.load(\"en_core_web_sm\") # When I use plain English() it doesn't seem to give POS info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19263,\n",
       " ['federalist',\n",
       "  'no',\n",
       "  '1',\n",
       "  'general',\n",
       "  'introduction',\n",
       "  'for',\n",
       "  'the',\n",
       "  'independent',\n",
       "  'journal',\n",
       "  'author'])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = words(text)\n",
    "len(tokens), tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "tokens = tokens[:10_000]   # total is about 19.2k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab and get X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2091"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = sorted(set(tokens))\n",
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '10',\n",
       " '11',\n",
       " '1685',\n",
       " '1706',\n",
       " '1774',\n",
       " '1st',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {w:i for i,w in enumerate(V)}\n",
    "def wtoi(w):\n",
    "    return index[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "Xy = [np.array((np.array(tokens[i-k:i],dtype=object),tokens[i])) for i in range(k,len(tokens)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([array(['federalist', 'no', '1', 'general', 'introduction', 'for', 'the',\n",
       "        'independent', 'journal', 'author', 'alexander', 'hamilton', 'to',\n",
       "        'the', 'people', 'of', 'the', 'state', 'of', 'new', 'york',\n",
       "        'after', 'an', 'unequivocal', 'experience', 'of', 'the',\n",
       "        'inefficiency', 'of', 'the', 'subsisting', 'federal', 'government',\n",
       "        'you', 'are', 'called', 'upon', 'to', 'deliberate', 'on', 'a',\n",
       "        'new', 'constitution', 'for', 'the', 'united', 'states', 'of',\n",
       "        'america', 'the'], dtype=object),\n",
       "        'subject'], dtype=object),\n",
       " array([array(['no', '1', 'general', 'introduction', 'for', 'the', 'independent',\n",
       "        'journal', 'author', 'alexander', 'hamilton', 'to', 'the',\n",
       "        'people', 'of', 'the', 'state', 'of', 'new', 'york', 'after', 'an',\n",
       "        'unequivocal', 'experience', 'of', 'the', 'inefficiency', 'of',\n",
       "        'the', 'subsisting', 'federal', 'government', 'you', 'are',\n",
       "        'called', 'upon', 'to', 'deliberate', 'on', 'a', 'new',\n",
       "        'constitution', 'for', 'the', 'united', 'states', 'of', 'america',\n",
       "        'the', 'subject'], dtype=object),\n",
       "        'speaks'], dtype=object),\n",
       " array([array(['1', 'general', 'introduction', 'for', 'the', 'independent',\n",
       "        'journal', 'author', 'alexander', 'hamilton', 'to', 'the',\n",
       "        'people', 'of', 'the', 'state', 'of', 'new', 'york', 'after', 'an',\n",
       "        'unequivocal', 'experience', 'of', 'the', 'inefficiency', 'of',\n",
       "        'the', 'subsisting', 'federal', 'government', 'you', 'are',\n",
       "        'called', 'upon', 'to', 'deliberate', 'on', 'a', 'new',\n",
       "        'constitution', 'for', 'the', 'united', 'states', 'of', 'america',\n",
       "        'the', 'subject', 'speaks'], dtype=object),\n",
       "        'its'], dtype=object),\n",
       " array([array(['general', 'introduction', 'for', 'the', 'independent', 'journal',\n",
       "        'author', 'alexander', 'hamilton', 'to', 'the', 'people', 'of',\n",
       "        'the', 'state', 'of', 'new', 'york', 'after', 'an', 'unequivocal',\n",
       "        'experience', 'of', 'the', 'inefficiency', 'of', 'the',\n",
       "        'subsisting', 'federal', 'government', 'you', 'are', 'called',\n",
       "        'upon', 'to', 'deliberate', 'on', 'a', 'new', 'constitution',\n",
       "        'for', 'the', 'united', 'states', 'of', 'america', 'the',\n",
       "        'subject', 'speaks', 'its'], dtype=object),\n",
       "        'own'], dtype=object),\n",
       " array([array(['introduction', 'for', 'the', 'independent', 'journal', 'author',\n",
       "        'alexander', 'hamilton', 'to', 'the', 'people', 'of', 'the',\n",
       "        'state', 'of', 'new', 'york', 'after', 'an', 'unequivocal',\n",
       "        'experience', 'of', 'the', 'inefficiency', 'of', 'the',\n",
       "        'subsisting', 'federal', 'government', 'you', 'are', 'called',\n",
       "        'upon', 'to', 'deliberate', 'on', 'a', 'new', 'constitution',\n",
       "        'for', 'the', 'united', 'states', 'of', 'america', 'the',\n",
       "        'subject', 'speaks', 'its', 'own'], dtype=object),\n",
       "        'importance'], dtype=object)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = np.array(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Xy[:,0], Xy[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['federalist', 'no', '1', 'general', 'introduction', 'for', 'the',\n",
       "        'independent', 'journal', 'author', 'alexander', 'hamilton',\n",
       "        'to', 'the', 'people', 'of', 'the', 'state', 'of', 'new', 'york',\n",
       "        'after', 'an', 'unequivocal', 'experience', 'of', 'the',\n",
       "        'inefficiency', 'of', 'the', 'subsisting', 'federal',\n",
       "        'government', 'you', 'are', 'called', 'upon', 'to', 'deliberate',\n",
       "        'on', 'a', 'new', 'constitution', 'for', 'the', 'united',\n",
       "        'states', 'of', 'america', 'the'],\n",
       "       ['no', '1', 'general', 'introduction', 'for', 'the',\n",
       "        'independent', 'journal', 'author', 'alexander', 'hamilton',\n",
       "        'to', 'the', 'people', 'of', 'the', 'state', 'of', 'new', 'york',\n",
       "        'after', 'an', 'unequivocal', 'experience', 'of', 'the',\n",
       "        'inefficiency', 'of', 'the', 'subsisting', 'federal',\n",
       "        'government', 'you', 'are', 'called', 'upon', 'to', 'deliberate',\n",
       "        'on', 'a', 'new', 'constitution', 'for', 'the', 'united',\n",
       "        'states', 'of', 'america', 'the', 'subject']], dtype=object)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack(X)\n",
    "X[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encode tokens in X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = np.vectorize(wtoi)\n",
    "X = encode(X)\n",
    "y = encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.unique(y)   # not every word in V will be in target classes (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9949, 50), (9949,))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 791, 1291,    0,  882, 1076,  831, 1874, 1015, 1106,  213,  111,\n",
       "        917, 1908, 1874, 1401, 1323, 1874, 1799, 1323, 1289, 2085,   94,\n",
       "        140, 1951,  742, 1323, 1874, 1027, 1323, 1874, 1817,  790,  899,\n",
       "       2086,  175,  284, 1975, 1908,  512, 1336,   15, 1289,  424,  831,\n",
       "       1874, 1960, 1800, 1323,  130, 1874])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(X):\n",
    "    X_onehot = np.zeros((len(X), k, len(V)), dtype=np.bool)\n",
    "    for i,record in enumerate(X):\n",
    "        onehot = np.zeros((k,len(V)), dtype=np.bool)\n",
    "        for j,wi in enumerate(record):\n",
    "            onehot[j,wi] = 1\n",
    "        X_onehot[i] = onehot\n",
    "    return X_onehot\n",
    "\n",
    "X_onehot = onehot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X to shape (num sequences, window width k, len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9949,), 2091, 2087)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, len(V), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9949, 2087)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot_train, X_onehot_valid, y_train, y_valid = train_test_split(X_onehot, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ca8f9cd32b4b589fabd6ebb25acedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', layout=Layout(flex='2'), max=10.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f880d56e384863af4ad2bf98f1cd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd2ff0ec47a4779938a6f24229122ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40fdf24d57d4c72bf3cf39dfc0462ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e0c0c21e8248d78b7bd4b801c53803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21c4899711249279f76fc4be83372ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6ef5e4303444a0b4236ab26561dad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc9196d976f4023926aad9e5b0aefe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b121045ce582464b98da53f7d8f8d6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0a806c0ed34052ab192c93592f8abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3df9f31aadc4422b002549262c5f542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=63.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# If you don't want to onehot, you can leave X as 2D num records x k and use this:\n",
    "#model.add(layers.Embedding(input_dim=len(V), output_dim=100, input_length=k))\n",
    "# else have to one hot X as num records x k x len(V)\n",
    "model.add(layers.LSTM(units=256, input_shape=(k,len(V))))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(len(targets), activation='softmax'))\n",
    "#model.add(layers.Lambda(lambda x: tf.cast(K.argmax(x, axis=-1),dtype=float)))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "history = model.fit(X_onehot_train, y_train,\n",
    "                    shuffle=True,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_onehot_valid, y_valid),\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0\n",
    "                    , callbacks=[tfa.callbacks.TQDMProgressBar(show_epoch_progress=True)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "* BatchNormalization seems to help training accuracy converge faster. If no embedding layer, batch norm makes massive diff\n",
    "* Having trouble getting validation accuracy beyond 7 or 8%.\n",
    "* Moved to no embedding layer and used dropout layer not dropout arg on LSTM. Dropout followed by batch norm made accur increase slowly but reverse order does no good. weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
