{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch in PyTorch to generate char sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randn(n1, n2, dtype=torch.float64, mean=0.0, std=0.01, requires_grad=True):\n",
    "    x = torch.randn(n1, n2, dtype=dtype)\n",
    "    x = x*std + mean # Convert x to have mean and std\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fastai human numbers data\n",
    "\n",
    "The data is from [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb). Looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import untar_data, URLs\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers, Sequential, losses\n",
    "import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus and numericalize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one \\ntwo \\nthree \\nfour \\nfive \\ns'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(path/'train.txt')\n",
    "text = text[:50_000] # TESTING!!!\n",
    "text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'[ \\n]+', ' . ', text) # use '.' as separator token\n",
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', '.', 'two', '.', 'three']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split(' ')\n",
    "tokens = tokens[:-1] # last token is blank '' so delete\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "v = set('.')\n",
    "vocab = ['.']\n",
    "for t in tokens:\n",
    "    if t not in v:\n",
    "        vocab.append(t)\n",
    "        v.add(t)\n",
    "#vocab = sorted(set(tokens))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 0, 3, 0, 4, 0, 5, 0]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = {w:i for i,w in enumerate(vocab)}\n",
    "tokens = [index[w] for w in tokens]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15353, 15353, 15354)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(tokens[0:-1])\n",
    "y = torch.tensor(tokens[1:])\n",
    "len(X), len(y), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 2, 0, 3]), tensor([0, 2, 0, 3, 0]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5], y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)\n",
    "ntrain = int(len(X)*.80)\n",
    "X_train, y_train = X[:ntrain], y[:ntrain]\n",
    "X_valid, y_valid = X[ntrain:], y[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'one': 1,\n",
       " 'two': 2,\n",
       " 'three': 3,\n",
       " 'four': 4,\n",
       " 'five': 5,\n",
       " 'six': 6,\n",
       " 'seven': 7,\n",
       " 'eight': 8,\n",
       " 'nine': 9,\n",
       " 'ten': 10,\n",
       " 'eleven': 11,\n",
       " 'twelve': 12,\n",
       " 'thirteen': 13,\n",
       " 'fourteen': 14,\n",
       " 'fifteen': 15,\n",
       " 'sixteen': 16,\n",
       " 'seventeen': 17,\n",
       " 'eighteen': 18,\n",
       " 'nineteen': 19,\n",
       " 'twenty': 20,\n",
       " 'thirty': 21,\n",
       " 'forty': 22,\n",
       " 'fifty': 23,\n",
       " 'sixty': 24,\n",
       " 'seventy': 25,\n",
       " 'eighty': 26,\n",
       " 'ninety': 27,\n",
       " 'hundred': 28,\n",
       " 'thousand': 29}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtoi = {w:i for i, w in enumerate(vocab)}\n",
    "wtoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(ci:int, vocab):\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ci] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(2, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h0, ci, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    h = h0\n",
    "    words = [vocab[ci]]\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            x = onehot(X_train[i], vocab)\n",
    "            h = W.mm(h) + U.mm(x)\n",
    "            h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "            o = V.mm(h).reshape(-1) # unnormalized log probabilities for next char\n",
    "#             print(o)\n",
    "            o = o / temperature\n",
    "            o = np.exp(o)\n",
    "            p = o / np.sum(o.numpy())\n",
    "#             p = F.softmax(o[0]).numpy() # normalized probabilities\n",
    "#             print(p)\n",
    "#             print(np.sum(p))\n",
    "            wi = np.random.choice(range(len(vocab)), p=p)\n",
    "            words.append(vocab[wi])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(h0, input):\n",
    "    h = h0\n",
    "    words = [vocab[ci]]\n",
    "    n = len(input)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            x = onehot(input[i], vocab)\n",
    "            h = W.mm(h) + U.mm(x)\n",
    "            h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "            o = V.mm(h).reshape(1,-1) # unnormalized log probabilities for next char\n",
    "            p = F.softmax(o[0]).numpy() # normalized probabilities\n",
    "            words.append(vocab[ci])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input):\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64)\n",
    "    seq_outputs = torch.empty(len(input),len(vocab))\n",
    "    for i in range(0,len(input)):\n",
    "        x = onehot(input[i], vocab)\n",
    "        h = W.mm(h) + U.mm(x)\n",
    "        h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "#         print(h)\n",
    "        o = V.mm(h)\n",
    "        seq_outputs[i] = o.reshape(-1)\n",
    "    return seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 loss  1.3989 accuracy 0.606     loss  1.2747 accuracy 0.697\n",
      "Epoch   1 loss  1.4621 accuracy 0.607     loss  1.0880 accuracy 0.738\n",
      "Epoch   2 loss  1.8122 accuracy 0.631     loss  1.3815 accuracy 0.757\n",
      "Epoch   3 loss  1.9845 accuracy 0.653     loss  1.3950 accuracy 0.758\n",
      "Epoch   4 loss  1.7803 accuracy 0.643     loss  1.3222 accuracy 0.768\n",
      "Epoch   5 loss  1.7045 accuracy 0.651     loss  1.3061 accuracy 0.765\n",
      "Epoch   6 loss  1.6898 accuracy 0.635     loss  1.2245 accuracy 0.767\n",
      "Epoch   7 loss  1.7622 accuracy 0.654     loss  1.2443 accuracy 0.770\n",
      "Epoch   8 loss  1.6787 accuracy 0.668     loss  1.1817 accuracy 0.771\n",
      "Epoch   9 loss  1.7953 accuracy 0.662     loss  1.1798 accuracy 0.772\n",
      "Epoch  10 loss  1.8556 accuracy 0.647     loss  1.3997 accuracy 0.770\n",
      "Epoch  11 loss  1.6937 accuracy 0.662     loss  1.2926 accuracy 0.769\n",
      "Epoch  12 loss  1.7508 accuracy 0.668     loss  1.4438 accuracy 0.769\n",
      "Epoch  13 loss  1.6104 accuracy 0.663     loss  1.3482 accuracy 0.772\n",
      "Epoch  14 loss  2.2777 accuracy 0.662     loss  1.7528 accuracy 0.772\n",
      "Epoch  15 loss  1.8041 accuracy 0.648     loss  1.6105 accuracy 0.768\n",
      "Epoch  16 loss  1.9403 accuracy 0.653     loss  1.6254 accuracy 0.770\n",
      "Epoch  17 loss  1.9923 accuracy 0.664     loss  1.6395 accuracy 0.772\n",
      "Epoch  18 loss  2.1100 accuracy 0.667     loss  1.7971 accuracy 0.770\n",
      "Epoch  19 loss  2.6398 accuracy 0.669     loss  1.8939 accuracy 0.778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-156bed341083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# do one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mseq_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         '''\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nhidden = 64\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(vocab) # predicting chars\n",
    "seqlen = 16\n",
    "\n",
    "W = randn(nhidden, nhidden)\n",
    "U = randn(nhidden, nfeatures)\n",
    "V = randn(nclasses, nhidden)\n",
    "\n",
    "n = (len(X_train) // seqlen) * seqlen # make it a multiple of seqlen\n",
    "X_train = X_train[:n]\n",
    "y_train = y_train[:n]\n",
    "X_valid = X_valid[:n]\n",
    "y_valid = y_valid[:n]\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=learning_rate, weight_decay=weight_decay)\n",
    "nepochs=20\n",
    "loss = 0\n",
    "for epoch in range(nepochs+1):\n",
    "    h = randn(nhidden, 1, requires_grad=False) # reset hidden state at start of epoch\n",
    "    outputs = torch.empty(n,len(vocab))\n",
    "    for p in range(0,n,seqlen): # do one epoch\n",
    "        seq_outputs = forward(X_train[p:p+seqlen])\n",
    "        outputs[p:p+seqlen] = seq_outputs\n",
    "        '''\n",
    "        seq_outputs = torch.empty(seqlen,len(vocab))\n",
    "        for i in range(p,p+seqlen,1):    # do one subsequence of entire X_train\n",
    "            x = onehot(X_train[i], vocab)\n",
    "            h = W.mm(h) + U.mm(x)\n",
    "            h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "    #         print(h)\n",
    "            o = V.mm(h)\n",
    "            outputs[i] = seq_outputs[i-p] = o.reshape(-1)\n",
    "#             print(i, vocab[X_train[i]], '->', vocab[y_train[i]], \"vs\", vocab[np.argmax(F.softmax(o).detach().numpy())])\n",
    "#             loss = loss + F.cross_entropy(o, torch.tensor([y_train[i]]))\n",
    "#             print(i, X_train[i], loss.item())\n",
    "#         print(f\"SEQUENCE loss={loss.item():.4f} ------------\")\n",
    "        '''\n",
    "        h = h.detach() # truncated BPTT; tell pytorch to forget prev h computations for dx purposes\n",
    "        loss = F.cross_entropy(seq_outputs, y_train[p:p+seqlen])\n",
    "#         print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad and M.grad\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = F.cross_entropy(outputs, y_train)\n",
    "        y_prob = F.softmax(outputs, dim=1)\n",
    "        y_pred = torch.argmax(y_prob, dim=1)\n",
    "        metric_train = accuracy_score(y_pred, y_train)\n",
    "#         print(f\"Epoch {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f} OLD ----------\")\n",
    "\n",
    "        o = forward(X_train)\n",
    "        train_loss = F.cross_entropy(o, y_train)\n",
    "        y_prob = F.softmax(o, dim=1)\n",
    "        y_pred = torch.argmax(y_prob, dim=1)\n",
    "        metric_train = accuracy_score(y_pred, y_train)\n",
    "#         print(f\"Epoch {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}\")\n",
    "\n",
    "        o = forward(X_valid)\n",
    "        valid_loss = F.cross_entropy(o, y_valid)\n",
    "        y_prob = F.softmax(o, dim=1)\n",
    "        y_pred = torch.argmax(y_prob, dim=1)\n",
    "        metric_valid = accuracy_score(y_pred, y_valid)\n",
    "        print(f\"Epoch {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}     loss {valid_loss:7.4f} accuracy {metric_valid:4.3f}\")\n",
    "\n",
    "#     print(sample(h0=h, ci=np.random.randint(0,len(vocab)), n=40))\n",
    "#     print(sample(h0=h, ci=1, n=40))\n",
    "#     with torch.no_grad():\n",
    "#         loss = F.cross_entropy(model(train_data.tensors[0]), train_data.tensors[1])\n",
    "#     print(f\"loss={loss.item():.4f} ------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
