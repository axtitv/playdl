{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch in PyTorch to generate char sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randn(n1, n2, dtype=torch.float64, mean=0.0, std=0.01, requires_grad=True):\n",
    "    x = torch.randn(n1, n2, dtype=dtype)\n",
    "    x = x*std + mean # Convert x to have mean and std\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fastai human numbers data\n",
    "\n",
    "The data is from [fastai book chap 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb). Looks like:\n",
    "\n",
    "```\n",
    "one \n",
    "two \n",
    "three \n",
    "...\n",
    "two hundred seven \n",
    "two hundred eight \n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import untar_data, URLs\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers, Sequential, losses\n",
    "import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus and numericalize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one \\ntwo \\nthree \\nfour \\nfive \\ns'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(path/'train.txt')\n",
    "text = text[:5_000] # TESTING!!!\n",
    "text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'[ \\n]+', ' . ', text) # use '.' as separator token\n",
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', '.', 'two', '.', 'three']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split(' ')\n",
    "tokens = tokens[:-1] # last token is blank '' so delete\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique vocab but don't sort; keep order so 'one'=1 etc...\n",
    "v = set('.')\n",
    "vocab = ['.']\n",
    "for t in tokens:\n",
    "    if t not in v:\n",
    "        vocab.append(t)\n",
    "        v.add(t)\n",
    "#vocab = sorted(set(tokens))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 0, 3, 0, 4, 0, 5, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = {w:i for i,w in enumerate(vocab)}\n",
    "tokens = [index[w] for w in tokens]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 1581, 1582)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(tokens[0:-1])\n",
    "y = torch.tensor(tokens[1:])\n",
    "len(X), len(y), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 2, 0, 3]), tensor([0, 2, 0, 3, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5], y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)\n",
    "ntrain = int(len(X)*.80)\n",
    "X_train, y_train = X[:ntrain], y[:ntrain]\n",
    "X_valid, y_valid = X[ntrain:], y[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'one': 1,\n",
       " 'two': 2,\n",
       " 'three': 3,\n",
       " 'four': 4,\n",
       " 'five': 5,\n",
       " 'six': 6,\n",
       " 'seven': 7,\n",
       " 'eight': 8,\n",
       " 'nine': 9,\n",
       " 'ten': 10,\n",
       " 'eleven': 11,\n",
       " 'twelve': 12,\n",
       " 'thirteen': 13,\n",
       " 'fourteen': 14,\n",
       " 'fifteen': 15,\n",
       " 'sixteen': 16,\n",
       " 'seventeen': 17,\n",
       " 'eighteen': 18,\n",
       " 'nineteen': 19,\n",
       " 'twenty': 20,\n",
       " 'thirty': 21,\n",
       " 'forty': 22,\n",
       " 'fifty': 23,\n",
       " 'sixty': 24,\n",
       " 'seventy': 25,\n",
       " 'eighty': 26,\n",
       " 'ninety': 27,\n",
       " 'hundred': 28}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtoi = {w:i for i, w in enumerate(vocab)}\n",
    "wtoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(ci:int, vocab):\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ci] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(2, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h0, ci, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    h = h0\n",
    "    words = [vocab[ci]]\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            x = onehot(X_train[i], vocab)\n",
    "            h = W.mm(h) + U.mm(x)\n",
    "            h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "            o = V.mm(h).reshape(-1) # unnormalized log probabilities for next char\n",
    "#             print(o)\n",
    "            o = o / temperature\n",
    "            o = np.exp(o)\n",
    "            p = o / np.sum(o.numpy())\n",
    "#             p = F.softmax(o[0]).numpy() # normalized probabilities\n",
    "#             print(p)\n",
    "#             print(np.sum(p))\n",
    "            wi = np.random.choice(range(len(vocab)), p=p)\n",
    "            words.append(vocab[wi])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(h0, input):\n",
    "    h = h0\n",
    "    words = [vocab[ci]]\n",
    "    n = len(input)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            x = onehot(input[i], vocab)\n",
    "            h = W.mm(h) + U.mm(x)\n",
    "            h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "            o = V.mm(h).reshape(1,-1) # unnormalized log probabilities for next char\n",
    "            p = F.softmax(o[0]).numpy() # normalized probabilities\n",
    "            words.append(vocab[ci])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(h0, input):\n",
    "    h = h0\n",
    "    seq_outputs = torch.empty(len(input),len(vocab))\n",
    "    for i in range(0,len(input)):\n",
    "        x = onehot(input[i], vocab)\n",
    "        h = W.mm(h) + U.mm(x)\n",
    "        h = torch.relu(h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "#         print(h)\n",
    "        o = V.mm(h)\n",
    "        seq_outputs[i] = o.reshape(-1)\n",
    "    h = h.detach() # truncated BPTT; tell pytorch to forget prev h computations for dx purposes\n",
    "    return seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9810784798634202\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-408437cb5439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         y_pred = torch.argmax(y_prob, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         metric_valid = accuracy_score(y_pred, y_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: training {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}     validation loss {valid_loss:7.4f} accuracy {metric_valid:4.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_loss' is not defined"
     ]
    }
   ],
   "source": [
    "nhidden = 64\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(vocab) # predicting chars\n",
    "seqlen = 16\n",
    "\n",
    "W = randn(nhidden, nhidden)\n",
    "U = randn(nhidden, nfeatures)\n",
    "V = randn(nclasses, nhidden)\n",
    "\n",
    "n = (len(X_train) // seqlen) * seqlen # make it a multiple of seqlen\n",
    "X_train = X_train[:n]\n",
    "y_train = y_train[:n]\n",
    "X_valid = X_valid[:n]\n",
    "y_valid = y_valid[:n]\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=learning_rate, weight_decay=weight_decay)\n",
    "nepochs=20\n",
    "loss = 0\n",
    "for epoch in range(nepochs+1):\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "    losses = []\n",
    "    for p in range(0,n,seqlen): # do one epoch\n",
    "        seq_outputs = forward(h, X_train[p:p+seqlen])\n",
    "#         h = h.detach() # truncated BPTT; tell pytorch to forget prev h computations for dx purposes\n",
    "        loss = F.cross_entropy(seq_outputs, y_train[p:p+seqlen])\n",
    "#         print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad and M.grad\n",
    "        optimizer.step()\n",
    "\n",
    "    print(np.mean(losses))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "        o = forward(h, X_train)\n",
    "        train_loss = F.cross_entropy(o, y_train)\n",
    "        y_prob = F.softmax(o, dim=1)\n",
    "        y_pred = torch.argmax(y_prob, dim=1)\n",
    "        metric_train = accuracy_score(y_pred, y_train)\n",
    "#         print(f\"Epoch {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}\")\n",
    "\n",
    "#         h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "#         o = forward(h, X_valid)\n",
    "#         valid_loss = F.cross_entropy(o, y_valid)\n",
    "#         y_prob = F.softmax(o, dim=1)\n",
    "#         y_pred = torch.argmax(y_prob, dim=1)\n",
    "#         metric_valid = accuracy_score(y_pred, y_valid)\n",
    "        print(f\"Epoch: training {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}     validation loss {valid_loss:7.4f} accuracy {metric_valid:4.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nfeatures, nhidden, nclasses):\n",
    "        super(RNN, self).__init__()\n",
    "        self.W = randn(nhidden, nhidden, requires_grad=False)\n",
    "        self.U = randn(nhidden, nfeatures, requires_grad=False)\n",
    "        self.V = randn(nclasses, nhidden, requires_grad=False)\n",
    "        self.W = nn.Parameter(self.W)\n",
    "        self.U = nn.Parameter(self.U)\n",
    "        self.V = nn.Parameter(self.V)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "\n",
    "    def forward(self, input):\n",
    "        seq_outputs = torch.empty(len(input),len(vocab))\n",
    "        for i in range(0,len(input)):\n",
    "            x = onehot(input[i], vocab)\n",
    "            self.h = W.mm(self.h) + U.mm(x)\n",
    "            h = torch.relu(self.h)  # squish to (-1,+1); also better than sigmoid for vanishing gradient\n",
    "    #         print(h)\n",
    "            o = V.mm(self.h)\n",
    "            seq_outputs[i] = o.reshape(-1)\n",
    "        self.h = self.h.detach() # truncated BPTT; tell pytorch to forget prev h computations for dx purposes\n",
    "        return seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1264, 29])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhidden = 64\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(vocab) # predicting chars\n",
    "seqlen = 16\n",
    "rnn = RNN(nfeatures, nhidden, nclasses)\n",
    "rnn(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5160014538825313\n",
      "Epoch   0 loss 720030248219630131740672.0000 accuracy 0.500\n",
      "9.446561878688117e+23\n",
      "Epoch   1 loss 720030248219630131740672.0000 accuracy 0.500\n",
      "9.446561878688117e+23\n",
      "Epoch   2 loss 720030248219630131740672.0000 accuracy 0.500\n",
      "9.446561878688117e+23\n",
      "Epoch   3 loss 720030248219630131740672.0000 accuracy 0.500\n",
      "9.446561878688117e+23\n",
      "Epoch   4 loss 720030248219630131740672.0000 accuracy 0.500\n",
      "9.446561878688117e+23\n",
      "Epoch   5 loss 720030248219630131740672.0000 accuracy 0.500\n"
     ]
    }
   ],
   "source": [
    "model = RNN(nfeatures, nhidden, nclasses)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "nepochs=5\n",
    "for epoch in range(nepochs+1):\n",
    "#     h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "    losses = []\n",
    "    for p in range(0,n,seqlen): # do one epoch\n",
    "        seq_outputs = model(X_train[p:p+seqlen])\n",
    "        loss = F.cross_entropy(seq_outputs, y_train[p:p+seqlen])\n",
    "#         print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad and M.grad\n",
    "        optimizer.step()\n",
    "        model.reset()\n",
    "\n",
    "    print(np.mean(losses))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.reset()\n",
    "        o = model(X_train)\n",
    "        train_loss = F.cross_entropy(o, y_train)\n",
    "        y_prob = F.softmax(o, dim=1)\n",
    "        y_pred = torch.argmax(y_prob, dim=1)\n",
    "        metric_train = accuracy_score(y_pred, y_train)\n",
    "        print(f\"Epoch {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}\")\n",
    "\n",
    "#         h = torch.zeros(nhidden, 1, dtype=torch.float64) # reset hidden state at start of epoch\n",
    "#         o = forward(h, X_valid)\n",
    "#         valid_loss = F.cross_entropy(o, y_valid)\n",
    "#         y_prob = F.softmax(o, dim=1)\n",
    "#         y_pred = torch.argmax(y_prob, dim=1)\n",
    "#         metric_valid = accuracy_score(y_pred, y_valid)\n",
    "#         print(f\"Epoch: training {epoch:3d} loss {train_loss:7.4f} accuracy {metric_train:4.3f}     validation loss {valid_loss:7.4f} accuracy {metric_valid:4.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warnings\n",
    "\n",
    "Ugh. F.cross_entropy includes (log)softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
