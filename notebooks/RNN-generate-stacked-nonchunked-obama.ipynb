{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate obama speeches using stacked RNNs but not batched\n",
    "\n",
    "With truncated back propagation, add embedding layer instead of one-hot encoding going into RNN.\n",
    "\n",
    "Use one big long record and then every `bptt` update weights. No breaking into chunks so that `h` is just a vector.  Simple version of [chunked obama](RNN-generate-stacked-obama.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:20_000] # testing\n",
    "n = len(text)\n",
    "\n",
    "bptt = 8                  # only look back this many time steps for gradients\n",
    "nhidden = 100\n",
    "#batch_size = 32\n",
    "char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ctoi[c] for c in text[0:-1]]\n",
    "y = [ctoi[c] for c in text[1:]]\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19,999 char, vocab size 46, char_embed_sz 20, state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "nclasses = len(ctoi)\n",
    "print(f\"{n:,d} char, vocab size {len(ctoi)}, char_embed_sz {char_embed_sz}, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss  7493.58   accur  0.1816   LR 0.001000\n",
      "Epoch   2 training loss  7496.71   accur  0.1812   LR 0.001000\n",
      "Epoch   3 training loss  7487.25   accur  0.1814   LR 0.001000\n",
      "Epoch   4 training loss  7487.55   accur  0.1814   LR 0.001000\n",
      "Epoch   5 training loss  7487.27   accur  0.1814   LR 0.001000\n",
      "Epoch   6 training loss  7486.93   accur  0.1814   LR 0.001000\n",
      "Epoch   7 training loss  7486.80   accur  0.1814   LR 0.001000\n",
      "Epoch   8 training loss  7269.65   accur  0.2033   LR 0.001000\n",
      "Epoch   9 training loss  6236.05   accur  0.2924   LR 0.001000\n",
      "Epoch  10 training loss  5907.63   accur  0.3203   LR 0.001000\n",
      "Epoch  11 training loss  5713.53   accur  0.3396   LR 0.001000\n",
      "Epoch  12 training loss  5568.66   accur  0.3531   LR 0.001000\n",
      "Epoch  13 training loss  5437.24   accur  0.3653   LR 0.001000\n",
      "Epoch  14 training loss  5361.12   accur  0.3750   LR 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5c1de7d87e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mU2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(char_embed_sz, len(ctoi),     device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "bx = torch.zeros(nhidden,       1,             device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "bo = torch.zeros(nclasses,       1,             device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "W2 = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U2 = torch.randn(nhidden,       nclasses,      device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V2 = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "bx2 = torch.zeros(nhidden,       1,             device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "bo2 = torch.zeros(nclasses,       1,             device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "sd = 0.1   # weight stddev init for tanh (default is N(0,1) for torch.randn())\n",
    "sd = 0.01   # weight stddev init for tanh (default is N(0,1) for torch.randn())\n",
    "# sd = 1.0\n",
    "with torch.no_grad():\n",
    "    E *= sd\n",
    "    U *= sd\n",
    "    V *= sd\n",
    "    U2 *= sd\n",
    "    V2 *= sd\n",
    "    \n",
    "# parameters = [E,W,U,B,V,W2,U2,V2]\n",
    "parameters = [E,W,U,V,bx,bo,W2,U2,V2,bx2,bo2]\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "\n",
    "history = []\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    h2 = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)\n",
    "    loss = 0\n",
    "    for t in range(n):\n",
    "        embedding_step_t = E[:,X[t]]\n",
    "        embedding_step_t = embedding_step_t.reshape(char_embed_sz,1)\n",
    "        h = W @ h + U @ embedding_step_t + bx\n",
    "        h = torch.tanh(h)\n",
    "        o = V @ h + bo\n",
    "\n",
    "        h2 = W2 @ h2 + U2 @ o + bx2\n",
    "        h2 = torch.tanh(h2)\n",
    "        o = V2 @ h2 + bo2\n",
    "\n",
    "        o = o.reshape(1,nclasses)\n",
    "        loss += F.cross_entropy(o, torch.tensor([y[t]]))\n",
    "\n",
    "        p = softmax(o)\n",
    "        correct = torch.argmax(p[0])==y[t]\n",
    "        epoch_training_accur += correct\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "#             torch.nn.utils.clip_grad_value_(parameters, 1)\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            # no longer consider previous computations\n",
    "            h = h.detach()\n",
    "            h2 = h2.detach()\n",
    "\n",
    "    epoch_training_accur /= n\n",
    "    epoch_training_loss /= bptt # average loss computed\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
