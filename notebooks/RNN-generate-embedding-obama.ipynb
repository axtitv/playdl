{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate obama speeches using truncated back propagation and add embedding layer instead of one-hot encoding going into RNN\n",
    "\n",
    "Lessons:\n",
    "\n",
    "* w/o nonlinearity on embedding layer (I don't think people use nonlinearity for this). yep, adding embedding of chars before RNN helps. make len(vocab)->small embedding like 20 squeezes into more meaningful embedding than one of size len(vocab). After 20 epochs, was only 47% accurate before with:\n",
    "```\n",
    "lr = 0.001\n",
    "obama 100k text\n",
    "nchunks = 100\n",
    "nhidden = 512\n",
    "bptt = 8\n",
    "char embed size is 20\n",
    "```\n",
    "and is now 57% accurate! Got to 59% at 30 epochs.\n",
    "* Rather than one-hotting entire 2D input matrix, much smaller on GPU with embeddings.\n",
    "* With 1M char, 30 epochs same hyperparams gives 61% accur\n",
    "* Bumping to 2M char seems to help\n",
    "* With 1M char, making char embed size same as vocab len is converging slightly more slowly.  char embed size of 10 also less good (from 20). char embed size 30 seems about same.\n",
    "* Back to default args above. Increase nhidden to 600 from 512. seems much slower per epoch and not converging as fast. Trying 400: seems about same as 512.\n",
    "* Setting stddev to 0.01 for randn init seems to help. At epoch 6, (lr=0.001) we get 59% vs 56% accuracy (400 nhidden). 64% accurate at 30 epochs.\n",
    "* bptt from 8 to 16 is slower to converge but catches up.\n",
    "* nchunks 50 from 100 about same\n",
    "* nchunks 200 from 100 slower to converge even when bumping lr\n",
    "* 100 training records, chunk length 10000, vocab size 70, char_embed_sz 20, state is 400-vector; lr=0.001 dropping by .8 every 3 got me to 65% accurate.\n",
    "* with 100 epochs, got to 67% accurate with `lr_scheduler.StepLR(optimizer, step_size=10, gamma=.9)`:\n",
    "```\n",
    "...\n",
    "Epoch  99 training loss 10488.16   accur  0.6730   LR 0.000387\n",
    "Epoch 100 training loss 10490.52   accur  0.6727   LR 0.000349\n",
    "```\n",
    "* Same LR plan and with 2M text:\n",
    "```\n",
    "Epoch  99 training loss 21393.74   accur  0.6671   LR 0.000387\n",
    "Epoch 100 training loss 21388.15   accur  0.6674   LR 0.000349\n",
    "```\n",
    "vocab size seems to be increasing with increased text so should probably increase other hyperparameters\n",
    "* with all 4M text doesn't help so must need more complex model\n",
    "```\n",
    "Epoch  99 training loss 45675.70   accur  0.6647   LR 0.000387\n",
    "Epoch 100 training loss 45670.77   accur  0.6649   LR 0.000349\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "#     print(range(n), y_true)\n",
    "    p = y_prob[range(n),y_true]\n",
    "    return torch.mean(-torch.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks\n",
    "\n",
    "The stochastic part of SGD is critical for training models. The idea is simply to use a small subset of the data when computing gradients to update the model parameters. Generally we take a small batch size of say 32 records, run that through the model, and then compute a loss. From that loss we compute the gradient and then update the model parameters and move onto the next batch.  Once all batches are complete, we have completed an epoch.  We should shuffle the batches and keep going.\n",
    "\n",
    "We can also be stochastic by updating the gradient in the middle of long sequences, rather than waiting until after a complete batch of long sequences.  If the sequences are really long, waiting till the end of a batch reduces the stochastic nature. Instead I'm going to try breaking up the entire input into a small number of very long sequences. In this way the RNN can keep the hidden state going for the complete sequence. Of course the only problem is that we cannot compute back propagation that far, so at some sequence length I can update the gradient and wipe it out then continue. I think this is easier than modifying the data set stride so that a standard training loop for an RNN keeps the same hidden state across long sequences even if we have broken into chunks.\n",
    "\n",
    "Let's say that we have a large text and we break it up into six chunks: A,B,C,D,E,F. then, six is our batch size and we will process each long sequence exactly once per epic. However to get stochastic nature, we will update the gradient after only a small sequence of characters.  We pick the chunk size and then the batch sizes computed instead of having to specify both. I think the chunk size is more important: how much can you store in a single hidden state vector.\n",
    "\n",
    "Come to think of it, all we need to specify is the number of chunks we want to break the text into.  There won't be any batch size because we have a single batch with `nchunks`  long records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = text[0:2_000_000] # testing\n",
    "n = len(text)\n",
    "\n",
    "bptt = 8                 # only look back this many time steps for gradients\n",
    "nhidden = 400\n",
    "char_embed_sz = 20        # there are 50+ chars, squeeze down into fewer dimensions for embedding prior to input into RNN \n",
    "nchunks = 100             # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "chunk_size = n // nchunks # the sequences will be very long\n",
    "n = nchunks * chunk_size  # reset size so it's an even multiple of chunk size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]], device=device)\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]],   device=device)\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training records, chunk length 42241, vocab size 94, char_embed_sz 20, state is 400-vector\n"
     ]
    }
   ],
   "source": [
    "nclasses = len(ctoi)\n",
    "print(f\"{nchunks:,d} training records, chunk length {chunk_size}, vocab size {len(ctoi)}, char_embed_sz {char_embed_sz}, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 42240]), 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 training loss 72959.78   accur  0.4947   LR 0.001000\n",
      "Epoch   2 training loss 55274.63   accur  0.6054   LR 0.001000\n",
      "Epoch   3 training loss 52731.51   accur  0.6205   LR 0.001000\n",
      "Epoch   4 training loss 51595.81   accur  0.6273   LR 0.001000\n",
      "Epoch   5 training loss 50930.33   accur  0.6313   LR 0.001000\n",
      "Epoch   6 training loss 50492.30   accur  0.6340   LR 0.001000\n",
      "Epoch   7 training loss 50185.79   accur  0.6359   LR 0.001000\n",
      "Epoch   8 training loss 49960.48   accur  0.6372   LR 0.001000\n",
      "Epoch   9 training loss 49776.64   accur  0.6383   LR 0.001000\n",
      "Epoch  10 training loss 49632.73   accur  0.6390   LR 0.000900\n",
      "Epoch  11 training loss 49226.25   accur  0.6416   LR 0.000900\n",
      "Epoch  12 training loss 49101.52   accur  0.6424   LR 0.000900\n",
      "Epoch  13 training loss 49004.48   accur  0.6429   LR 0.000900\n",
      "Epoch  14 training loss 48921.54   accur  0.6434   LR 0.000900\n",
      "Epoch  15 training loss 48856.44   accur  0.6437   LR 0.000900\n",
      "Epoch  16 training loss 48802.13   accur  0.6441   LR 0.000900\n",
      "Epoch  17 training loss 48754.04   accur  0.6443   LR 0.000900\n",
      "Epoch  18 training loss 48716.43   accur  0.6446   LR 0.000900\n",
      "Epoch  19 training loss 48683.66   accur  0.6447   LR 0.000900\n",
      "Epoch  20 training loss 48643.81   accur  0.6450   LR 0.000810\n",
      "Epoch  21 training loss 48338.60   accur  0.6470   LR 0.000810\n",
      "Epoch  22 training loss 48286.57   accur  0.6473   LR 0.000810\n",
      "Epoch  23 training loss 48237.42   accur  0.6477   LR 0.000810\n",
      "Epoch  24 training loss 48217.21   accur  0.6477   LR 0.000810\n",
      "Epoch  25 training loss 48182.81   accur  0.6478   LR 0.000810\n",
      "Epoch  26 training loss 48170.70   accur  0.6480   LR 0.000810\n",
      "Epoch  27 training loss 48140.22   accur  0.6483   LR 0.000810\n",
      "Epoch  28 training loss 48123.37   accur  0.6482   LR 0.000810\n",
      "Epoch  29 training loss 48093.50   accur  0.6484   LR 0.000810\n",
      "Epoch  30 training loss 48082.60   accur  0.6485   LR 0.000729\n",
      "Epoch  31 training loss 47800.92   accur  0.6505   LR 0.000729\n",
      "Epoch  32 training loss 47762.92   accur  0.6505   LR 0.000729\n",
      "Epoch  33 training loss 47726.46   accur  0.6508   LR 0.000729\n",
      "Epoch  34 training loss 47703.13   accur  0.6510   LR 0.000729\n",
      "Epoch  35 training loss 47682.75   accur  0.6510   LR 0.000729\n",
      "Epoch  36 training loss 47666.06   accur  0.6514   LR 0.000729\n",
      "Epoch  37 training loss 47644.43   accur  0.6513   LR 0.000729\n",
      "Epoch  38 training loss 47639.44   accur  0.6515   LR 0.000729\n",
      "Epoch  39 training loss 47639.51   accur  0.6515   LR 0.000729\n",
      "Epoch  40 training loss 47624.23   accur  0.6514   LR 0.000656\n",
      "Epoch  41 training loss 47373.39   accur  0.6532   LR 0.000656\n",
      "Epoch  42 training loss 47336.03   accur  0.6535   LR 0.000656\n",
      "Epoch  43 training loss 47312.43   accur  0.6537   LR 0.000656\n",
      "Epoch  44 training loss 47297.01   accur  0.6538   LR 0.000656\n",
      "Epoch  45 training loss 47293.66   accur  0.6538   LR 0.000656\n",
      "Epoch  46 training loss 47272.05   accur  0.6538   LR 0.000656\n",
      "Epoch  47 training loss 47258.45   accur  0.6542   LR 0.000656\n",
      "Epoch  48 training loss 47252.61   accur  0.6540   LR 0.000656\n",
      "Epoch  49 training loss 47233.32   accur  0.6540   LR 0.000656\n",
      "Epoch  50 training loss 47220.66   accur  0.6543   LR 0.000590\n",
      "Epoch  51 training loss 47003.96   accur  0.6557   LR 0.000590\n",
      "Epoch  52 training loss 46959.24   accur  0.6559   LR 0.000590\n",
      "Epoch  53 training loss 46940.12   accur  0.6561   LR 0.000590\n",
      "Epoch  54 training loss 46910.47   accur  0.6564   LR 0.000590\n",
      "Epoch  55 training loss 46904.80   accur  0.6564   LR 0.000590\n",
      "Epoch  56 training loss 46903.03   accur  0.6565   LR 0.000590\n",
      "Epoch  57 training loss 46891.04   accur  0.6565   LR 0.000590\n",
      "Epoch  58 training loss 46873.79   accur  0.6567   LR 0.000590\n",
      "Epoch  59 training loss 46871.36   accur  0.6567   LR 0.000590\n",
      "Epoch  60 training loss 46854.79   accur  0.6567   LR 0.000531\n",
      "Epoch  61 training loss 46639.72   accur  0.6581   LR 0.000531\n",
      "Epoch  62 training loss 46613.92   accur  0.6584   LR 0.000531\n",
      "Epoch  63 training loss 46598.31   accur  0.6584   LR 0.000531\n",
      "Epoch  64 training loss 46568.72   accur  0.6586   LR 0.000531\n",
      "Epoch  65 training loss 46564.82   accur  0.6586   LR 0.000531\n",
      "Epoch  66 training loss 46556.22   accur  0.6588   LR 0.000531\n",
      "Epoch  67 training loss 46538.37   accur  0.6590   LR 0.000531\n",
      "Epoch  68 training loss 46531.64   accur  0.6589   LR 0.000531\n",
      "Epoch  69 training loss 46516.11   accur  0.6589   LR 0.000531\n",
      "Epoch  70 training loss 46512.07   accur  0.6590   LR 0.000478\n",
      "Epoch  71 training loss 46330.96   accur  0.6603   LR 0.000478\n",
      "Epoch  72 training loss 46301.54   accur  0.6605   LR 0.000478\n",
      "Epoch  73 training loss 46279.14   accur  0.6606   LR 0.000478\n",
      "Epoch  74 training loss 46262.60   accur  0.6609   LR 0.000478\n",
      "Epoch  75 training loss 46255.27   accur  0.6608   LR 0.000478\n",
      "Epoch  76 training loss 46240.92   accur  0.6610   LR 0.000478\n",
      "Epoch  77 training loss 46232.40   accur  0.6609   LR 0.000478\n",
      "Epoch  78 training loss 46225.84   accur  0.6608   LR 0.000478\n",
      "Epoch  79 training loss 46212.74   accur  0.6612   LR 0.000478\n",
      "Epoch  80 training loss 46210.50   accur  0.6612   LR 0.000430\n",
      "Epoch  81 training loss 46037.45   accur  0.6623   LR 0.000430\n",
      "Epoch  82 training loss 46007.16   accur  0.6626   LR 0.000430\n",
      "Epoch  83 training loss 45982.52   accur  0.6628   LR 0.000430\n",
      "Epoch  84 training loss 45975.70   accur  0.6627   LR 0.000430\n",
      "Epoch  85 training loss 45968.31   accur  0.6627   LR 0.000430\n",
      "Epoch  86 training loss 45964.93   accur  0.6627   LR 0.000430\n",
      "Epoch  87 training loss 45946.11   accur  0.6629   LR 0.000430\n",
      "Epoch  88 training loss 45945.02   accur  0.6628   LR 0.000430\n",
      "Epoch  89 training loss 45935.30   accur  0.6629   LR 0.000430\n",
      "Epoch  90 training loss 45934.56   accur  0.6629   LR 0.000387\n",
      "Epoch  91 training loss 45761.27   accur  0.6642   LR 0.000387\n",
      "Epoch  92 training loss 45735.33   accur  0.6644   LR 0.000387\n",
      "Epoch  93 training loss 45728.26   accur  0.6645   LR 0.000387\n",
      "Epoch  94 training loss 45721.65   accur  0.6645   LR 0.000387\n",
      "Epoch  95 training loss 45701.69   accur  0.6645   LR 0.000387\n",
      "Epoch  96 training loss 45691.67   accur  0.6647   LR 0.000387\n",
      "Epoch  97 training loss 45694.28   accur  0.6646   LR 0.000387\n",
      "Epoch  98 training loss 45682.35   accur  0.6647   LR 0.000387\n",
      "Epoch  99 training loss 45675.70   accur  0.6647   LR 0.000387\n",
      "Epoch 100 training loss 45670.77   accur  0.6649   LR 0.000349\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "E = torch.randn(char_embed_sz, len(ctoi),     device=device, dtype=torch.float64, requires_grad=True) # embedding\n",
    "W = torch.eye(nhidden,         nhidden,       device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,       char_embed_sz, device=device, dtype=torch.float64, requires_grad=True) # input converter\n",
    "V = torch.randn(nclasses,      nhidden,       device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "with torch.no_grad():\n",
    "    E *= 0.01  # make stddev 0.01, seems to help\n",
    "    W *= 0.01\n",
    "    U *= 0.01\n",
    "    V *= 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam([E,W,U,V], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=.9)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                               mode='triangular2',\n",
    "#                                               step_size_up=5,\n",
    "#                                               base_lr=0.0001, max_lr=0.005,\n",
    "#                                               cycle_momentum=False)\n",
    "\n",
    "history = []\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "#     shuffled_idx = torch.randperm(nchunks) # shuffle each epoch (don't need actually)\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "#         print(f\"t={t}\")\n",
    "        chars_step_t = X[:,t] # char_embed_sz x nchunks\n",
    "        # column E[i] is the embedding for char index i. same as multiple E.mm(onehot(i))\n",
    "        embedding_step_t = E[:,chars_step_t] # char_embed_sz x nchunks\n",
    "#         print(embedding_step_t.shape, E.shape, H.shape, W.shape, U.shape)\n",
    "        H = W.mm(H) + U.mm(embedding_step_t)\n",
    "        H = torch.tanh(H)\n",
    "        o = V.mm(H)\n",
    "        o = o.T # make it nchunks x nclasses\n",
    "        o = softmax(o)\n",
    "        correct = torch.argmax(o, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "#         print(f\"loss {loss:7.4f}\")\n",
    "        loss += cross_entropy(o, y[:,t])\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                c = chars[j]\n",
    "                ci = ctoi[c]\n",
    "                embedding_step_j = E[:,ci].reshape(char_embed_sz,1) # col is embedding for c; must be column\n",
    "#                 print(embedding_step_j.shape, E.shape, h.shape, W.shape, U.shape)#, V.shape)\n",
    "                h = W@h + U@embedding_step_j\n",
    "                h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p) # this doesn't work (just repeats 'and' a million times)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the job -- and i will sustain the grounding ahead and outlated and strategic recent impedettly destiny.  so let's help you who threatened. and more resisting for funding among even threatens a duporaud lines of producaton, and, “what it's good about the work of longer extent.\\n\\nwe know that can leave\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join( sample(list('the job'), 300) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
