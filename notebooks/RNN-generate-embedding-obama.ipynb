{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate obama speeches using truncated back propagation and add embedding layer instead of one-hot encoding going into RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "#from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, mode='r') as f:\n",
    "        s = f.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_transform(x, mean=0.0, std=0.01):\n",
    "    \"Convert x to have mean and std\"\n",
    "    return x*std + mean\n",
    "\n",
    "def randn(n1, n2,          \n",
    "          mean=0.0, std=0.01, requires_grad=False,\n",
    "          device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "          dtype=torch.float64):\n",
    "    x = torch.randn(n1, n2, device=device, dtype=dtype)\n",
    "    x = normal_transform(x, mean=mean, std=std)\n",
    "    x.requires_grad=requires_grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, yrange=(0.0, 5.00), figsize=(3.5,3)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.ylabel(\"Sentiment log loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    loss = history[:,0]\n",
    "    valid_loss = history[:,1]\n",
    "    plt.plot(loss, label='train_loss')\n",
    "    plt.plot(valid_loss, label='val_loss')\n",
    "    # plt.xlim(0, 200)\n",
    "    plt.ylim(*yrange)\n",
    "    plt.legend()#loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvocab(strings):\n",
    "    letters = [list(l) for l in strings]\n",
    "    vocab = set([c for cl in letters for c in cl])\n",
    "    vocab = sorted(list(vocab))\n",
    "    ctoi = {c:i for i, c in enumerate(vocab)}\n",
    "    return vocab, ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    expy = torch.exp(y)\n",
    "    if len(y.shape)==1: # 1D case can't use axis arg\n",
    "        return expy / torch.sum(expy)\n",
    "    return expy / torch.sum(expy, axis=1).reshape(-1,1)\n",
    "\n",
    "def cross_entropy(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    y_pred is n x k for n samples and k output classes and y_true is n x 1\n",
    "    and is often softmax of final layer.\n",
    "    y_pred values must be probability that output is a specific class.\n",
    "    Binary case: When we have y_pred close to 1 and y_true is 1,\n",
    "    loss is -1*log(1)==0. If y_pred close to 0 and y_true is 1, loss is\n",
    "    -1*log(small value) = big value.\n",
    "    y_true values must be positive integers in [0,k-1].\n",
    "    \"\"\"\n",
    "    n = y_prob.shape[0]\n",
    "    # Get value at y_true[j] for each sample with fancy indexing\n",
    "#     print(range(n), y_true)\n",
    "    p = y_prob[range(n),y_true]\n",
    "    return torch.mean(-torch.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), device=device, dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v\n",
    "\n",
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len\n",
    "\n",
    "def onehot_matrix(X, ctoi):\n",
    "    r, c = X.shape\n",
    "    X_onehot = torch.zeros(r, c, len(ctoi), device=device, dtype=torch.float64)\n",
    "    for i,x in enumerate(X):\n",
    "        for j,c in enumerate(x):\n",
    "            X_onehot[i,j,c] = 1\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split into chunks\n",
    "\n",
    "The stochastic part of SGD is critical for training models. The idea is simply to use a small subset of the data when computing gradients to update the model parameters. Generally we take a small batch size of say 32 records, run that through the model, and then compute a loss. From that loss we compute the gradient and then update the model parameters and move onto the next batch.  Once all batches are complete, we have completed an epoch.  We should shuffle the batches and keep going.\n",
    "\n",
    "We can also be stochastic by updating the gradient in the middle of long sequences, rather than waiting until after a complete batch of long sequences.  If the sequences are really long, waiting till the end of a batch reduces the stochastic nature. Instead I'm going to try breaking up the entire input into a small number of very long sequences. In this way the RNN can keep the hidden state going for the complete sequence. Of course the only problem is that we cannot compute back propagation that far, so at some sequence length I can update the gradient and wipe it out then continue. I think this is easier than modifying the data set stride so that a standard training loop for an RNN keeps the same hidden state across long sequences even if we have broken into chunks.\n",
    "\n",
    "Let's say that we have a large text and we break it up into six chunks: A,B,C,D,E,F. then, six is our batch size and we will process each long sequence exactly once per epic. However to get stochastic nature, we will update the gradient after only a small sequence of characters.  We pick the chunk size and then the batch sizes computed instead of having to specify both. I think the chunk size is more important: how much can you store in a single hidden state vector.\n",
    "\n",
    "Come to think of it, all we need to specify is the number of chunks we want to break the text into.  There won't be any batch size because we have a single batch with `nchunks`  long records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224143"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = get_text(\"data/obama-speeches.txt\").lower() # generated from obama-sentences.py\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:100_000] # testing\n",
    "\n",
    "n = len(text)\n",
    "nchunks = 100 # break up the input into a number of chunks (doesn't have to be small like batch size)\n",
    "chunk_size = n // nchunks # the sequences will be very long\n",
    "n = nchunks * chunk_size  # reset size so it's an even multiple of chunk size\n",
    "text = text[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [text[p:p+chunk_size] for p in range(0, n, chunk_size)]\n",
    "X = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long) # int8 doesn't work as indices\n",
    "y = torch.empty(nchunks, chunk_size-1, device=device, dtype=torch.long)\n",
    "for i,chunk in enumerate(chunks):\n",
    "    X[i,:] = torch.tensor([ctoi[c] for c in chunk[0:-1]], device=device)\n",
    "    y[i,:] = torch.tensor([ctoi[c] for c in chunk[1:]],   device=device)\n",
    "    \n",
    "# X, y are now chunked and numericalized into big 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training records, chunk length 1000, 52 features (chars), state is 512-vector\n"
     ]
    }
   ],
   "source": [
    "nhidden = 512\n",
    "char_embed_sz = 20 # there are 50+ chars, squeeze down into 20 dimensions for embedding prior to input into RNN \n",
    "nfeatures = len(ctoi)\n",
    "nclasses = nfeatures\n",
    "print(f\"{nchunks:,d} training records, chunk length {chunk_size}, {nfeatures} features (chars), state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we can fit this on the GPU in its entirety\n",
    "X_onehot = onehot_matrix(X, ctoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 999, 52]), 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_onehot.shape, nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\n",
      "Epoch   1 training loss  6565.61   accur  0.2304   LR 0.001000\n",
      "Epoch   2 training loss  2820.07   accur  0.3206   LR 0.001000\n",
      "Epoch   3 training loss  2484.21   accur  0.3545   LR 0.001000\n",
      "Epoch   4 training loss  2310.79   accur  0.3782   LR 0.001000\n",
      "Epoch   5 training loss  2207.48   accur  0.3925   LR 0.001000\n",
      "Epoch   6 training loss  2132.10   accur  0.4047   LR 0.001000\n",
      "Epoch   7 training loss  2077.74   accur  0.4134   LR 0.001000\n",
      "Epoch   8 training loss  2033.97   accur  0.4220   LR 0.001000\n",
      "Epoch   9 training loss  1996.37   accur  0.4276   LR 0.001000\n",
      "Epoch  10 training loss  1963.90   accur  0.4347   LR 0.001000\n",
      "Epoch  11 training loss  1937.15   accur  0.4392   LR 0.001000\n",
      "Epoch  12 training loss  1910.25   accur  0.4459   LR 0.001000\n",
      "Epoch  13 training loss  1888.03   accur  0.4502   LR 0.001000\n",
      "Epoch  14 training loss  1865.96   accur  0.4548   LR 0.001000\n",
      "Epoch  15 training loss  1847.47   accur  0.4577   LR 0.001000\n",
      "Epoch  16 training loss  1827.55   accur  0.4637   LR 0.001000\n",
      "Epoch  17 training loss  1812.08   accur  0.4663   LR 0.001000\n",
      "Epoch  18 training loss  1793.66   accur  0.4703   LR 0.001000\n",
      "Epoch  19 training loss  1780.98   accur  0.4732   LR 0.001000\n",
      "Epoch  20 training loss  1769.83   accur  0.4756   LR 0.001000\n"
     ]
    }
   ],
   "source": [
    "#%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   device=device, dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, device=device, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   device=device, dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                               mode='triangular2',\n",
    "#                                               step_size_up=5,\n",
    "#                                               base_lr=0.0001, max_lr=0.005,\n",
    "#                                               cycle_momentum=False)\n",
    "\n",
    "print(\"BEGIN\")\n",
    "history = []\n",
    "epochs = 20\n",
    "bptt = 8 # only look back this many time steps for gradients\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "#     shuffled_idx = torch.randperm(nchunks) # shuffle each epoch (don't need actually)\n",
    "    H = torch.zeros(nhidden, nchunks, device=device, dtype=torch.float64, requires_grad=False)\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    loss = 0\n",
    "    for t in range(chunk_size-1):  # char t in chunk predicts t+1 so one less\n",
    "#         print(f\"t={t}\")\n",
    "        x_step_t = X_onehot[:,t].T # make it len(vocab) x nchunks\n",
    "#         print(x_step_t.shape, H.shape, W.shape, U.shape)\n",
    "        H = W.mm(H) + U.mm(x_step_t)\n",
    "        H = torch.tanh(H)\n",
    "        o = V.mm(H)\n",
    "        o = o.T # make it nchunks x nclasses\n",
    "        o = softmax(o)\n",
    "        correct = torch.argmax(o, dim=1)==y[:,t]\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "#         print(f\"loss {loss:7.4f}\")\n",
    "        loss += cross_entropy(o, y[:,t])\n",
    "        \n",
    "        if t % bptt == 0 and t > 0:\n",
    "#             print(f\"gradient at {t:4d}, loss {loss.item():7.4f}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.detach().item()\n",
    "            loss = 0\n",
    "            H = H.detach() # no longer consider previous computations\n",
    "\n",
    "    epoch_training_accur /=  nchunks * (chunk_size-1)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:8.2f}   accur {epoch_training_accur:7.4f}   LR {scheduler.get_last_lr()[0]:7.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(initial_chars, n, temperature=0.1):\n",
    "    \"Derived from Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\"\n",
    "    chars = initial_chars\n",
    "    n -= len(initial_chars)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, device=device, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(chars)):  # for each char in a name\n",
    "                h = W@h + U@onehot(chars[j])\n",
    "                h = torch.tanh(h)\n",
    "            o = V@h\n",
    "            o = o.reshape(nclasses)\n",
    "            p = softmax(o)\n",
    "#             wi = torch.argmax(p) # this doesn't work (just repeats 'and' a million times)\n",
    "            wi = np.random.choice(range(len(vocab)), p=p.cpu()) # don't always pick most likely; pick per distribution\n",
    "            chars.append(vocab[wi])\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the jobs trogram.\\n\\nthrer way? ddentalers. it have than tave care quest that findifeled blet the somes. i do the unddiation los wressribut lessues of amrema's whan bekenestial sen mides mund have camermans recommy securesxand thate thanal we makes. greard.  when to fursherchiest of we trades.  wen we\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join( sample(list('the job'), 300) ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
